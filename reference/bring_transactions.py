# -*- coding: utf-8 -*-
"""master_fifo_project_CAF.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1S2eBZzYU0qgZQXjiXp_gJeDTKGCAPG60

#Libraries

##PIP Installs
"""

!pip install etherscan-python  # Install the correct package

!pip install eth-abi

!pip install web3

!pip install openpyxl

"""##Libraries"""

import os
import re
import math
import time
import json
import glob
import yaml
import pprint
import random
import logging
import warnings
import requests
import unicodedata
import numpy as np
import pandas as pd

from time import sleep
from pathlib import Path
from datetime import datetime, timedelta, timezone
from typing import Tuple, Dict, List, Set
from decimal import Decimal, getcontext, ROUND_DOWN, ROUND_FLOOR, DivisionUndefined
from collections import defaultdict
from collections.abc import Mapping, Sequence
from pandas import json_normalize
from pandas.tseries.offsets import MonthEnd
from tqdm.auto import tqdm
from tqdm.notebook import tqdm as tqdm_notebook
from IPython.display import display

from requests.exceptions import HTTPError, Timeout, RequestException
from openpyxl import load_workbook
from openpyxl import load_workbook
from eth_utils import to_checksum_address
from etherscan import Etherscan
from eth_abi import decode
from eth_abi.abi import decode as decode_abi
from hexbytes import HexBytes
from tenacity import retry, stop_after_attempt, wait_exponential, wait_random, retry_if_exception_type
from web3 import Web3, HTTPProvider
from web3.exceptions import Web3RPCError, ABIFunctionNotFound, Web3ValueError
from web3._utils.events import get_event_data, event_abi_to_log_topic
from functools import lru_cache

from google.colab import drive, files
drive.mount('/content/drive')

"""#Functions"""

import requests
from functools import lru_cache
from web3 import Web3
import json
from datetime import datetime, timezone
from decimal import Decimal
from pathlib import Path
import pandas as pd

ETHERSCAN_API_KEY = "P13CVTCP43NWU9GX5D9VBA2QMUTJDDS941"

# Chainlink ABI + contract
AGGREGATOR_V3_ABI = [{
    "inputs": [],
    "name": "latestRoundData",
    "outputs": [
        {"internalType": "uint80", "name": "roundId", "type": "uint80"},
        {"internalType": "int256", "name": "answer", "type": "int256"},
        {"internalType": "uint256", "name": "startedAt", "type": "uint256"},
        {"internalType": "uint256", "name": "updatedAt", "type": "uint256"},
        {"internalType": "uint80", "name": "answeredInRound", "type": "uint80"},
    ],
    "stateMutability": "view",
    "type": "function",
}]
INFURA_API_KEY = "02321aab179b4085b84cda11f9bffb8a"
w3 = Web3(Web3.HTTPProvider(f"https://mainnet.infura.io/v3/{INFURA_API_KEY}"))

FEED_ADDRESS = w3.to_checksum_address("0x5f4ec3df9cbd43714fe2740f5e3616155c5b8419")
aggregator = w3.eth.contract(address=FEED_ADDRESS, abi=AGGREGATOR_V3_ABI)
# … your existing setup (API keys, Web3, ABI, etc.) …

@lru_cache(maxsize=None)
def get_eth_usd_at_block(block_number: int) -> tuple[Decimal, datetime]:
    """Cache price + timestamp per block number."""
    round_id, answer, *_ = (
        aggregator.functions
                  .latestRoundData()
                  .call(block_identifier=block_number)
    )
    price = Decimal(answer) / Decimal(1e8)
    blk = w3.eth.get_block(block_number)
    return price, datetime.fromtimestamp(blk.timestamp, tz=timezone.utc)

@lru_cache(maxsize=None)
def get_eod_price_for_date(year: int, month: int, day: int) -> tuple[Decimal, datetime]:
    """
    Cache one EOD lookup per date.
    Binary‐search block at 23:59:59 UTC of that day and grab its price.
    """
    eod_dt = datetime(year, month, day, 23, 59, 59, tzinfo=timezone.utc)
    target_ts = int(eod_dt.timestamp())

    # find the latest block before or at that timestamp
    low, high = 0, w3.eth.block_number
    while low <= high:
        mid = (low + high) // 2
        ts = w3.eth.get_block(mid).timestamp
        if ts <= target_ts:
            low = mid + 1
        else:
            high = mid - 1
    block_eod = high

    return get_eth_usd_at_block(block_eod)

from concurrent.futures import ThreadPoolExecutor, as_completed
from decimal import Decimal
from datetime import datetime

def get_eth_usd_for_tx(tx_hash: str) -> tuple[str, Decimal, datetime] | None:
    """
    Returns (tx_hash, price_usd, block_datetime_utc) tuple, or None on error.
    """
    try:
        tx = w3.eth.get_transaction(tx_hash)
        price, dt = get_eth_usd_at_block(tx.blockNumber)
        return tx_hash, price, dt
    except Exception as e:
        print(f"❌ Error processing {tx_hash}: {e}")
        return None


def eth_usd_df_with_eod(tx_hashes, w3, aggregator) -> pd.DataFrame:
    rows = []
    for tx_hash in tqdm(tx_hashes, desc="Fetching ETH/USD prices"):
        try:
            price_tx, dt_tx = get_eth_usd_for_tx(tx_hash)
            if price_tx is None:
                print(f"[SKIP] No Chainlink price for tx {tx_hash} (block before feed start?)")
                continue

            price_eod, dt_eod = get_eod_price_for_date(dt_tx.year, dt_tx.month, dt_tx.day)
            rows.append({
                'hash':                     tx_hash,
                'TX_ETH_USD_price':         price_tx,
                'TX_transaction_datetime':  dt_tx,
                'EOD_ETH_USD_price':        price_eod,
                'EOD_transaction_datetime': dt_eod
            })
        except Exception as e:
            print(f"[ERROR] Skipping {tx_hash}: {e}")
            continue

    return pd.DataFrame(rows)

def get_implementation_address(w3, proxy):
    proxy = w3.to_checksum_address(proxy)

    # Try checking the implementation address using the EIP-1967 pattern
    impl_slot = int("0x360894A13BA1A3210667C828492DB98DCA3E2076CC3735A920A3CA505D382BBC", 16)
    try:
        raw_impl = w3.eth.get_storage_at(proxy, impl_slot)
        impl = w3.to_checksum_address(raw_impl[-20:].hex())
        if impl != "0x0000000000000000000000000000000000000000":
            return impl
    except Exception as e:
        print(f"EIP-1967 check failed: {e}")

    # Check for EIP-1967 beacon pattern
    beacon_slot = int("0xa3f0ad74e5423aebfd80d3ef4346578335a9a72aeaee59ff6cb3582b35133d50", 16)
    try:
        raw_beacon = w3.eth.get_storage_at(proxy, beacon_slot)
        beacon = w3.to_checksum_address(raw_beacon[-20:].hex())
        if beacon != "0x0000000000000000000000000000000000000000":
            beacon_abi = [{
                "inputs": [],
                "name": "implementation",
                "outputs": [{"internalType": "address", "name": "", "type": "address"}],
                "stateMutability": "view",
                "type": "function"
            }]
            beacon_ct = w3.eth.contract(address=beacon, abi=beacon_abi)
            return beacon_ct.functions.implementation().call()
    except Exception as e:
        print(f"EIP-1967 beacon check failed: {e}")

    # Check for EIP-1167 minimal proxy pattern
    try:
        code = w3.eth.get_code(proxy).hex()
        if code.startswith("0x3d602d80600a3d3981f3") and code.endswith("5af43d82803e903d91602b57fd5bf3"):
            impl_bytes = code[10*2:10*2 + 40]
            return w3.to_checksum_address("0x" + impl_bytes)
    except Exception as e:
        print(f"EIP-1167 minimal proxy check failed: {e}")

    # Additional Fallback: Manually inspect for known proxy patterns
    try:
        # Example: Check for a custom implementation-fetching function (if known)
        contract = w3.eth.contract(address=proxy, abi=[{
            "inputs": [],
            "name": "getImplementation",
            "outputs": [{"internalType": "address", "name": "", "type": "address"}],
            "stateMutability": "view",
            "type": "function"
        }])
        impl_address = contract.functions.getImplementation().call()
        return Web3.to_checksum_address(impl_address)
    except Exception as e:
        print(f"Custom proxy pattern check failed: {e}")

    return None  # If nothing matched, return None

def fetch_abi_and_name(addr: str) -> tuple[str, dict]:
    """Returns (contract_name, abi_dict), or (address, None) if ABI not verified or EOA"""
    addr = addr.lower()
    try:
        code = w3.eth.get_code(Web3.to_checksum_address(addr))
        if code in (b'', b'0x'):
            print(f"→ {addr} is an EOA, skipping ABI fetch.")
            return addr, None

        resp = requests.get(
            "https://api.etherscan.io/api",
            params={
                "module": "contract",
                "action": "getsourcecode",
                "address": addr,
                "apikey": ETHERSCAN_API_KEY
            }
        ).json()

        result = resp.get("result", [{}])[0]
        abi_raw = result.get("ABI")
        if abi_raw in (None, "Contract source code not verified", ""):
            print(f"→ ABI not found or not verified for {addr}")
            return addr, None

        name = result.get("ContractName", addr)
        abi = json.loads(abi_raw)
        return name, abi

    except Exception as e:
        print(f"→ Error fetching ABI for {addr}: {e}")
        return addr, None


missing_abis = []

def save_abi(addr: str, abi: dict):
    if abi is None:
        print(f"→ ABI missing for {addr}, skipping save")
        missing_abis.append(addr)
        return
    path = ABI_DIR / f"{addr.lower()}.json"
    with open(path, "w") as f:
        json.dump(abi, f, indent=2)
    print()
    print(f"→ ABI for {addr} saved to {path}")

def fetch_token_symbol(contract_address: str, w3: Web3, cache: Dict[str, str]) -> str:
    try:
        addr = w3.toChecksumAddress(contract_address)
    except Exception:
        return ""
    if addr in cache:
        return cache[addr]
    try:
        token = w3.eth.contract(address=addr, abi=ERC20_METADATA_ABI)
        sym = token.functions.symbol().call() or ""
    except Exception:
        sym = ""
    cache[addr] = sym
    return sym

def normalize_symbol(sym: str) -> str:
    """Clean raw symbol to canonical ticker, never returning blank."""
    if not isinstance(sym, str) or not sym.strip():
        return "Unknown"

    # Remove unwanted phrases and URLs
    sym_cleaned = re.sub(r"(?:\$|ETH35\.com|Visit to claim bonus rewards).*", "", sym)

    # Normalize the symbol by removing special characters and ensuring it is in a consistent format
    # --- CHANGE --- Check if the cleaned symbol is empty before attempting to split
    if not sym_cleaned.strip():
        return "Unknown"

    # --- CHANGE --- Use try-except block to handle potential IndexError
    try:
        key = re.sub(r"[^\w]", "", unicodedata.normalize("NFKC", sym_cleaned).split()[0]).upper()
    except IndexError:
        # If there is an IndexError (i.e., split() results in an empty list), return "Unknown"
        return "Unknown"

    # Check if the normalized symbol is in the SYMBOL_MAP, and return the canonical version if it exists
    mapped = SYMBOL_MAP.get(key, f"Unknown ({key})")

    # If mapping gave you empty or whitespace, force "Unknown"
    return mapped if mapped and mapped.strip() else "Unknown"

"""#Set Variables"""

#Time Period
start = pd.Timestamp("2024-07-01", tz="UTC")
end   = pd.Timestamp("2025-01-01", tz="UTC")
current_period = end
name_period = "20241231"
from decimal import Decimal, getcontext, ROUND_HALF_EVEN
getcontext().prec = 50  # ensure enough precision to work with 18 dp


SCALE_CRYPTO = Decimal('0.000000000000000001')  # 18 decimal places
SCALE_USD    = Decimal('0.01')                  # 2 decimal places

from pathlib import Path
import os

BASE_URL = "https://api.etherscan.io/v2/api"


# Directory paths
ROOT_DIR = Path("/content/drive/MyDrive/Drip_Capital")
ABI_DIR = ROOT_DIR / "smart_contract_ABIs"
ACCOUNTING_DIR = ROOT_DIR / "accounting_records"
NFT_INVESTMENTS_DIR = ACCOUNTING_DIR / "investments/NFT"
CRYPTO_INVESTMENTS_DIR = ACCOUNTING_DIR / "investments/cryptocurrency"

# Convert current_period to string (e.g., "20250731")
current_period_str = current_period.strftime("%Y%m%d")

# Period-specific directories
PERIOD_DIR = ACCOUNTING_DIR / current_period_str
JOURNAL_ENTRIES_DIR = PERIOD_DIR / "general_journal_entries"

# Create necessary directories
for path in [PERIOD_DIR, JOURNAL_ENTRIES_DIR, NFT_INVESTMENTS_DIR, CRYPTO_INVESTMENTS_DIR]:
    os.makedirs(path, exist_ok=True)

# Print paths for verification
print("ABI Directory:", ABI_DIR)
print("Accounting Directory for Current Period:", PERIOD_DIR)
print("Journal Entries Directory:", JOURNAL_ENTRIES_DIR)
print("NFT Investments Directory:", NFT_INVESTMENTS_DIR)
print("Cryptocurrency Investments Directory:", CRYPTO_INVESTMENTS_DIR)

"""## phishy"""

mint_or_burn = "0x0000000000000000000000000000000000000000"
wrapped_ETH_address = "0xC02aaA39b223FE8D0A0e5C4F27eAD9083C756Cc2" #wrapped Etherium
CoW_protocol_ETH_flow = "0x40A50cf069e992AA4536211B23F286eF88752187"
usdt_contract_address = "0xdAC17F958D2ee523a2206206994597C13D831ec7" #USDT Token
usdc_contract_address = "0xa0b86991c6218b36c1d19d4a2e9eb0ce3606eb48" #USDC Token
coinbase_prime = "0xCD531Ae9EFCCE479654c4926dec5F6209531Ca7b"
coinbase_prime_2 = "0xceB69F6342eCE283b2F5c9088Ff249B5d0Ae66ea"
coinbase_10 = "0xA9D1e08C7793af67e9d92fe308d5697FB81d3E43"

#Lending Platform Smart Contracts for Fund's Transactions
smart_contract_Gondi = "0xf65B99CE6DC5F6c556172BCC0Ff27D3665a7d9A8" #MultiSourceLoan; ERC-20: GONDI_MULTI_SOURCE_LOAN
smart_contract_Gondi_V2 = "0x478f6F994C6fb3cf3e444a489b3AD9edB8cCaE16" #MultiSourceLoan; ERC-20: GONDI_MULTI_SOURCE_LOAN
smart_contract_P2PLendingNfts = "0x5F19431BC8A3eb21222771c6C867a63a119DeDA7" #P2PLendingNfts
smart_contract_blur_pool = "0x0000000000A39bb272e79075ade125fd351887Ac" #Blur; ERC1967Proxy

#other
meme_coin_contract_address = "0xb131f4A55907B10d1F0A50d8ab8FA09EC342cd74" #Memecoin
chainlink_USD_ETH_price_feed = "0x5f4eC3Df9cbd43714FE2740f5E3616155c5b8419"


#Phishing/Scam addresses

raw_suspects = [
    "0x2a120e7f2F1d8fFD173eD17Aa5089f11206B5177",
    "0xcB4b7a5114E02c144a915c05C59192a6c6f33d5A",
    "0xa8F41D54Fd002aa0D027d010CDC3FCF3fd8F40c7",
    "0x8421f2Ae7f7D6ec64698E6A142515609932cFAbC",
    "0x5d72fcee79efe6a493078b57b310f8a854bcc71b",
    "0x729f7430e3e715c84bca27821a5e554cad056a35",
    "0xb6b15d694b07411823fe04ecd27399f18c521574",
    "0x4fbb350052bca5417566f188eb2ebce5b19bc964",
    "0xd08fd4141932f47a644b77a7ef968f552fa4daa0",
    "0x47c639efbabb3af26f95efb571293479e6c1d9cd",
    "0x1b3e77a721b2714fe7f80874e499e7825da29d0d",
    "0x3e3e8c461e4024757d0f81a30e9bcad8b3520671",
    "0xab3e1e638b19a8dbecc47d6d6433dbea67a76cb2",
    "0x91554a9f1b6582c6743f9d876c822816fd9639b8",
    "0xbf3314734852ecd952fd862da853d68d0a83e530",
    "0xe56333c2aedfeb4fbd5b7a4dbedc1b0f99e15abd",
    "0xcb2757d719f43ceb84d53915f4fa114be2fa3792",
    "0x69d706cfa647f989ad7e3f2cf151fd9c41e4ddb3",
    "0xa423e0855176835633c0d38b7c3cdda939903c02",
    "0xc04327b22e2160d1746d9b664d434e831dc06591",
    "0xf70b6c73e6ce7b82436b6e2f1c02dd50487b7362",
    "0x679488415fd76b482acda5328d90290d387835aa",
    "0x94b1afdd235b0daad3f56cc5507df2a6272c8013",
    "0x3b2ad323e2218de2eb57228e64f0073b3529713f",
    "0xa7504f4258e238b957c20b34427642700020ebd9",
    "0xdE9E976C9C53C22A2a0C74F50d5D5c70B35ffa8f",
]

# “fake_phishing” list
fake_phishing = [
    "0x0842661E4d34364c9d9023De581146DdeCF1d2d9",
    "0xE12933c0413Ca50F149C0379C797e515A96935Da",
    "0xaC52eD1e812d968BD5AF7Edb33B73A3559d7DaA0",
    "0x2B496312bD67Ab4F3a8519cda865F9728E50d209",
    "0x1bcc835e7a0e7f0672012e775967d4269f0c6dbc",
    "0x2a120e7f2F1d8fFD173eD17Aa5089f11206B5177",
    "0xcB4b7a5114E02c144a915c05C59192a6c6f33d5A",
    "0xa8F41D54Fd002aa0D027d010CDC3FCF3fd8F40c7",
    "0x8421f2Ae7f7D6ec64698E6A142515609932cFAbC",
    "0xdF8E18c88A0419bCBb81B720425d132341873A3A",
    "0x57F2EcD5A7Dd825293F922456c7887c182CF6A75",
    "0x29F7bbBc757aBB299Dd18e70f0D0F08CC7d570E6",
    "0xeBBa46F99D4Cb2BDc2dd344885CD5A90EFB766c1",
    "0x9E69CE60d1C1f36AB77BC8aBf3B708E86f97a1a5",
    "0xb07b783b117d1cc7a8bee2bba7269ab4d9c00875",
    "0xFA90071f8fBBa47Fb657e7D0E4439aA80A8FdAb0",
    "0xD327AF2c29e04202db17fc0C91b5060551A69D4C",
    "0x77aBA3531b9A444C1E062c3DaF5E5FCdaF8c2Ca3",
    "0x8928e48ddbe739a46013b92ed37cb69243789030",
    "0xd081b96d116912e54475ac6c0a26a3d7a2ba4b98",
    "0xb90319c573bfc546bcad5f0cc08c976f84aa1675",
    "0xff8d4d679affd82db7beeabf4708c777962bf620",
    "0x84b8032b5f2130b0eeef9809f23432d72e94222b",
    "0x1fe156b25ffc7b69fcd6643c3f06597ba5b4f60e",
    "0x42da1d197d1227a5214bb34e61d7127deb0eb015",
    "0x99ad412a77f11f712151b8ea2e3e31174f9d31a1",
    "0x669507921b89c11536fe28fb2b955fbc23dbe86d",
    "0xd504bbd965a7517b91e76f4367c731a61db9d490",
    "0x5313e1a88643dc79be044cafe011ea22864d7213",
    "0xe1a02b742c02dbb50d6c91577e2fe834a86d7db7",
    "0x4e7284fc10c0ebf9a2a34fd056026e5092e27d69",
    "0xedb2ebbd587f6ebeebe8635b57b8c03cff732159",
    "0x217ab96793512c4d49903df604af3f84dafe33b8",
    "0x17ff065985ca9ca0b20f6d32aa598dace61e49fb",
    "0x83089881fe93fb5d54b4bf765defe69db18658a4",
    "0x88e374f40d0be6fb7bb5ea6f595447b98db537f8",
    "0x25cd7a8e675ec82aa5edd7c5888fc334ce391863",
    "0x6f8b6ea1e53546e713393dec2fa7e574e92187f6",
    "0x5098ad0ffb45fbc6bde74f20c789bf33ef300b8e",
    "0x01ce1491ca73676cea9d25e4c284afb70981d6a3",
    "0x6dd75b227243f50db2c85052395c1d24d6a760b2",
    "0x2b2044531f997c846d5be5435286b2dbe1b04121",
    "0x0732ec41e2913d4924b8ad4b2089d5022e027ea7",
    "0xa6cf52a11549fc635256049e7c0244c56d3ba93e",
    "0x253e54860b416357c6804319a33b35f9bfc3c48d",
    "0x3d1f95c154b7be55f2179803785f85fd88d6245e",
    "0x6bcdafb6d8cff42fe62098cffef844998cffd140",
    "0x82a8dcb6ca21118f5bef31a632ff2f295e041cef",
    "0x07463ee1ba9c1fbfd6755965f89eb034276ff505",
    "0x7faf8ea5b506843796c999423f10041bd74dc713",
    "0x5fb249dbad777690a057a7778ff1ef42d3c8ee29",
    "0xabb0ad3232a51e3050f0cd660ee24f0ae27f377e",
    "0x7999e39bbc4d1794eecb55d29320b46639e866e8",
    "0x1315c4ccb95443a4385d3e784b31488859dcb633",
    "0x26b5f2bde7110d3c9e871c4f5799ea6bba7548aa",
    "0x9243ec06bf64d8b0d9b6f74806282487971e9b8e",
    "0xa406ed0fbdf166a6bc1b4855db86255f7c14437a",
    "0x9d61a0763cbd832152e1bb9b362c70ae6d3ad6c2",
    "0x8ceb1bdb554d8501f3134b7383233e40812e7fc4",
    "0x97bf0e89c2541d29b88d7874d257e87dc18f8004",
    "0xf1a2c14c485b43aca4afe8d593ad244293392b3b",
    "0x67761bbe1b56b16d9f58608215b1fb7d1d4b1afc",
    "0x4cf141870094cb2b8347ff11bfcbfbb69b2e0c25",
    "0x77ca2bc773e654e92e65a776242470336b9e619f",
    "0x2dacb3392c3c94653ffc22bb8d6b3e475afab94f",
    "0x32d5e0cc73a6ca358a111a826c8e919bf701726d",
    "0x09e571dadeb2aa116872bd2c59a8246797645c80",
    "0xe52f621c4a4c833341b5537220bd819c2eb54c01",
    "0x4f68df00b182440102ba573dcbe9e249404897d5",
    "0x71908f11f1d684ca84faa5ceab3cdf10028e96e1",
    "0x9a090dd9891e0171bd1c205eb400c04e9fe0e3be",
    "0xf76a7a74fd9e04820caf535852a58c358c93c597",
    "0x3513cbdfcc78b428d33f8633e2d64600573450b7",
    "0x3c9b970f1dd9dc5356d81cb699a670b01c1669cb",
    "0x25a0d6092ee10fd34d56fb4aca324664aff7ad1f",
    "0x5d72fcee79efe6a493078b57b310f8a854bcc71b",
    "0x3b2ad323e2218de2eb57228e64f0073b3529713f",
    "0x3b2a866aa3fed5bfd9bab9d7bf8506c4ff70713f",
    "0x3b2ad323e2218de2eb57228e64f0073b3529713f",
    "0xdE9E976C9C53C22A2a0C74F50d5D5c70B35ffa8f",
    "0x0842661E4d34364c9d9023De581146DdeCF1d2d9",
    "0xca7582f355b8d6041d9f0fe65569d7c1b6a3955d",
    "0xa7504f4258e238b957c20b34427642700020ebd9",
    "0x3fC29836E84E471a053D2D9E80494A867D670EAD",
    "0xcB4b7a5114E02c144a915c05C59192a6c6f33d5A",
    "0xaC52eD1e812d968BD5AF7Edb33B73A3559d7DaA0",
    "0x31127c1A3011F141F29A35BD8f7dcBd1b0952Fd5",
    "0x5ff0d2de4cd862149c6672c99b7edf3b092667a3",
]

# normalize to lowercase
suspects = [addr.lower() for addr in (raw_suspects + fake_phishing)]

"""##Wallet ID Metadata"""

#Load excel file for wallet mapping

wallet_ID_mapping_file_df = pd.read_excel('/content/drive/MyDrive/Drip_Capital/drip_capital_wallet_ID_mapping.xlsx', engine='openpyxl')  # Explicitly specify the engine
wallet_ID_mapping_file_df = wallet_ID_mapping_file_df.fillna('') # Replace NaN with empty string
wallet_ID_mapping_file_df.tail()

wallet_addy = wallet_ID_mapping_file_df[wallet_ID_mapping_file_df["wallet_address"] == "0xA9898fa8E23cE51a779f30506285ea4636f0b033"]
wallet_addy

wallet_metadata      = {}        # address ➔ metadata
wallets_by_group     = {}        # group   ➔ {address ➔ metadata}

# build both dictionaries
for _, row in wallet_ID_mapping_file_df.iterrows():
    addr = Web3.to_checksum_address(row["wallet_address"].lower())
    meta = {
        "friendly_name"        : row["friendly_name"],
        "wallet_address"       : row["wallet_address"],
        "platform_variable_name": row["platform_variable_name"],
        "crypto_type"          : row["crypto_type"],
        "category"             : row["category"],
        "fund_id"              : row["fund_id"],
        "group"                : row["group"],
        "subgroup"             : row["subgroup"],
    }

    wallet_metadata[addr] = meta
    wallets_by_group.setdefault(meta["category"], {})[addr] = meta   # <-- group index


# -------- helpers ----------------------------------------------------------

def get_wallets_by_group(group_name: str) -> dict:
    """Return {address: metadata} for a given group (empty dict if none)."""
    return wallets_by_group.get(group_name, {})


def print_metadata(d: dict):
    for addr, meta in d.items():
        print(f"'{addr}': {{")
        for k, v in meta.items():
            print(f"    '{k}': '{v}',")
        print("},")


# -------- usage examples ---------------------------------------------------

fund_wallets = get_wallets_by_group("fund")
fund_wallets

fund_wallets

print(wallet_ID_mapping_file_df['category'].unique())
print()

print(wallet_ID_mapping_file_df['group'].unique())
print()
print(wallet_ID_mapping_file_df['subgroup'].unique())
print()
print(wallet_ID_mapping_file_df['fund_id'].unique())

# Slice into categories, using wallet_address instead of platform_variable_name
fund_wallet_ids      = [m["wallet_address"] for m in wallet_metadata.values() if m["category"] == "fund"]
gp_wallet_ids        = [m["wallet_address"] for m in wallet_metadata.values() if m["category"] == "gp"]
lp_wallet_ids        = [m["wallet_address"] for m in wallet_metadata.values() if m["category"] == "lp"]
holdings_wallet_ids  = [m["wallet_address"] for m in wallet_metadata.values() if m["group"] == "holdings"]
fund_i_wallet_ids    = [m["wallet_address"] for m in wallet_metadata.values() if m["group"] == "fund_i"]
fund_ii_wallet_ids   = [m["wallet_address"] for m in wallet_metadata.values() if m["group"] == "fund_ii"]
holdings_class_B_wallet_ids = [m["wallet_address"] for m in wallet_metadata.values() if m["subgroup"] == "holdings_class_B"]
holdings_class_A_wallet_ids = [m["wallet_address"] for m in wallet_metadata.values() if m["subgroup"] == "holdings_class_A"]
fund_i_class_B_wallet_ids = [m["wallet_address"] for m in wallet_metadata.values() if m["subgroup"] == "fund_i_class_B"]
fund_i_class_A_wallet_ids = [m["wallet_address"] for m in wallet_metadata.values() if m["subgroup"] == "fund_i_class_A"]
fund_ii_class_B_wallet_ids = [m["wallet_address"] for m in wallet_metadata.values() if m["subgroup"] == "fund_ii_class_B"]

# DEBUG: print counts & contents
print(f"‣ fund_wallet_ids      ({len(fund_wallet_ids)}):", fund_wallet_ids)
print(f"‣ gp_wallet_ids        ({len(gp_wallet_ids)}):", gp_wallet_ids)
print(f"‣ lp_wallet_ids        ({len(lp_wallet_ids)}):", lp_wallet_ids)
print(f"‣ holdings_wallet_ids  ({len(holdings_wallet_ids)}):", holdings_wallet_ids)
print(f"‣ fund_i_wallet_ids  ({len(fund_i_wallet_ids)}):", fund_i_wallet_ids)
print(f"‣ fund_ii_wallet_ids  ({len(fund_ii_wallet_ids)}):", fund_ii_wallet_ids)
print(f"‣ holdings_class_B_wallet_ids  ({len(holdings_class_B_wallet_ids)}):", holdings_class_B_wallet_ids)
print(f"‣ holdings_class_A_wallet_ids  ({len(holdings_class_A_wallet_ids)}):", holdings_class_A_wallet_ids)
print(f"‣ fund_i_class_B_wallet_ids  ({len(fund_i_class_B_wallet_ids)}):", fund_i_class_B_wallet_ids)
print(f"‣ fund_i_class_A_wallet_ids  ({len(fund_i_class_A_wallet_ids)}):", fund_i_class_A_wallet_ids)
print(f"‣ fund_ii_class_B_wallet_ids  ({len(fund_ii_class_B_wallet_ids)}):", fund_ii_class_B_wallet_ids)

# ─── Fetch list (omit LPs at first) ─────────────────────────────────
raw_wallets: List[str] = fund_wallet_ids + gp_wallet_ids

print(f"Will fetch txs for {len(raw_wallets)} wallets:", raw_wallets)

# ─── Build name maps ───────────────────────────────────────────────
# name_map for from/to mapping (excludes LPs)

name_map = {
    m["wallet_address"]: m["friendly_name"]
    for m in wallet_metadata.values()
}

# Check the map to verify
print(name_map)
# wallet_friendly_names if you need to label just your fetched set (no LPs at first)
wallet_friendly_names = {
    m["wallet_address"]: m["friendly_name"]
    for m in wallet_metadata.values() if m["wallet_address"] in raw_wallets
}

# ─── 5) Other derived maps ─────────────────────────────────────
wallet_crypto_types = {
    m["wallet_address"]: m["crypto_type"]
    for m in wallet_metadata.values()
}

decimal_map = {
    "ETH": 18, "WETH": 18, "USDC": 6, "USDT": 6,
    "WSTETH": 18, "BLUR POOL": 18,
}

# Generate the dictionary of wallet variable names with their corresponding wallet addresses using "=" for the output format
wallet_variable_address_mapping_eq = [
    f"{wallet_metadata[addr]['platform_variable_name']} = {wallet_metadata[addr]['wallet_address']}"
    for addr in wallet_metadata
]

# Join the results into a clean format
formatted_output = "\n".join(wallet_variable_address_mapping_eq)

# Display the result in a clean format
print(formatted_output)

gp_wallet_ids_checksum = {
    Web3.to_checksum_address(addr.lower()) for addr in gp_wallet_ids
}
fund_wallet_ids_checksum = {
    Web3.to_checksum_address(addr.lower()) for addr in fund_wallet_ids
}

fund_i_wallet_ids



#Possibly access this database for look ups
#https://www.4byte.directory/

"""##Map ERC-20 tokens"""

# ERC-20 ABI for symbol() only
ERC20_METADATA_ABI = [
    {
        "constant": True,
        "inputs": [],
        "name": "symbol",
        "outputs": [{"name": "", "type": "string"}],
        "type": "function",
    }
]

# Map of known irregular symbols to canonical tickers
SYMBOL_MAP: Dict[str, str] = {
    # ─── ETH variants ───────────────────────────────────────────────────────
    'ETH35.COM':     'ETH',
    'ETHG':          'ETH',
    'ETHF':          'ETH',
    'ETH':           'ETH',
    'ЕТН':           'ETH',   # Cyrillic Н
    'EТH':           'ETH',   # Cyrillic Т
    'ЕTH':           'ETH',   # Cyrillic Е
    'Ͼ':             'ETH',   # special char

    # ─── USDC variants ──────────────────────────────────────────────────────
    'CIRCLEUSD':     'USDC',
    'CIRCLE':        'USDC',
    'USD0':          'USDC',
    'USDC':          'USDC',
    'USDС':          'USDC',  # Cyrillic С
    'UЅDС':          'USDC',  # Cyrillic Ѕ,С

    # ─── USDT ────────────────────────────────────────────────────────────────
    'USDT':          'USDT',

    # ─── WETH variants ──────────────────────────────────────────────────────
    'WETH':          'WETH',
    'WЕТН':          'WETH',  # Cyrillic Т,Н
    'WEТH':          'WETH',  # Cyrillic Т
    'WWETH':         'WETH',
    'AETHWETH':      'WETH',
    'MWETHPPG5':     'WETH',

    # ─── staked ETH ─────────────────────────────────────────────────────────
    'WSTETH':        'WSTETH',

    # ─── BLUR variants ──────────────────────────────────────────────────────
    'BLUR':          'BLUR',
    'BLURPOOL':      'BLUR',
    'ВLUR':          'BLUR',  # Cyrillic В

    # ─── Unknown variants ───────────────────────────────────────────────────
    'ERC20':         'Unknown (ERC20)',
    'AIRDROPS':      'Unknown (AIRDROPS)',
    'NCELIGIBLE':    'Unknown (NCELIGIBLE)',
    '':              'Unknown',

    # ─── Additional Mappings ────────────────────────────────────────────────
    'AIC':           'Unknown',  # No known match, map to Unknown
    'ARCD':          'Unknown',  # No known match, map to Unknown
    'Airdrop(s) to be claimed at https://getdrops.org': 'Unknown',  # Airdrop info
    'Circle USD':    'USDC',  # Likely referring to USD-based token, map to USDC
    'DAI':           'DAI',  # DAI is a stablecoin
    'ETHf':          'ETH',  # Mapping as ETH, similar to other ETH variants
    'FRAX':          'FRAX',  # Known stablecoin
    'GRG':           'Unknown',  # No direct match, map to Unknown
    'HQG':           'Unknown',  # No direct match, map to Unknown
    'MEME':          'Unknown',  # Likely referring to a meme coin, map to Unknown
    'NC-Eligible (Verify: https://nodecoin.claims)': 'Unknown',  # Airdrop info
    'OETH':          'ETH',  # Mapping to ETH since it's staked ETH
    'PUMP':          'Unknown',  # No direct match, map to Unknown
    'SPX':           'Unknown',  # No direct match, map to Unknown
    'The Next Ethereum': 'ETH',  # Possible reference to ETH
    'USD0 [www.usual.finance]': 'USDC',  # Likely referring to USD token
    'USDC':          'USDC',  # Already mapped
    'USDT':          'USDT',  # Already mapped
    'USDС':          'USDC',  # Cyrillic С
    'UЅDC':          'USDC',  # Cyrillic Ѕ,С
    'Visit fraxnetwork.com to claim rewards': 'FRAX',  # Frax network token
    'WETH':          'WETH',  # Already mapped
    'WEТH':          'WETH',  # Cyrillic T
    'WЕТН':          'WETH',  # Cyrillic H
    'aEthWETH':      'WETH',  # Mapping to WETH
    'eETH':          'ETH',  # Mapping to ETH
    'mWETH-PPG:5':   'WETH',  # Mapping to WETH
    'mWETH-WPUNKS:40': 'WETH',  # Mapping to WETH
    'mjLP-GENERAL-WETH': 'WETH',  # Mapping to WETH
    'mpDAO':         'Unknown',  # No direct match, map to Unknown
    'msLP-GENERAL-WETH': 'WETH',  # Mapping to WETH
    'pUSDC':         'USDC',  # Mapping to USDC
    'pWETH':         'WETH',  # Mapping to WETH
    'pzETH':         'ETH',  # Mapping to ETH
    'rsETH':         'WSTETH',  # Mapping to staked ETH
    'rstETH':        'WSTETH',  # Mapping to staked ETH
    's1':            'Unknown',  # No direct match, map to Unknown
    'sFRAX':         'FRAX',  # Mapping to FRAX
    'sMEME':         'Unknown',  # No direct match, map to Unknown
    'sbWETH':        'WETH',  # Mapping to WETH
    'stETH':         'WSTETH',  # Mapping to staked ETH
    'variableDebtEthUSDC': 'USDC',  # Mapping to USDC
    'variableDebtEthUSDT': 'USDT',  # Mapping to USDT
    'wWETH':         'WETH',  # Mapping to WETH
    'weETHk':        'ETH',  # Mapping to ETH
    'weETHs':        'ETH',  # Mapping to ETH
    'wstETH':        'WSTETH',  # Mapping to staked ETH
    'Ͼ':             'ETH',  # Special char mapping to ETH
    'Вlur Роol':     'BLUR',  # Cyrillic B, mapping to BLUR
    'ЕRC20':         'Unknown (ERC20)',  # Mapping to Unknown
    'ЕRС20':         'Unknown (ERC20)',  # Cyrillic E, mapping to Unknown
    'ЕTH':           'ETH',  # Cyrillic E, mapping to ETH
    'ЕТН':           'ETH',  # Cyrillic H, mapping to ETH
    '$ ETH35.com - Visit to claim bonus rewards:': 'Unknown',
    'AIC$ ETH35.com - Visit to claim bonus rewards': 'Unknown',
}

# ─── Normalize Symbol Function ────────────────────────────────────────────────
def normalize_symbol(sym: str) -> str:
    """Clean raw symbol to canonical ticker, never returning blank."""
    if not isinstance(sym, str) or not sym.strip():
        return "Unknown"

    # Remove unwanted phrases and URLs
    sym_cleaned = re.sub(r"(?:\$|ETH35\.com|Visit to claim bonus rewards).*", "", sym)

    # Normalize the symbol by removing special characters and ensuring it is in a consistent format
    # --- CHANGE --- Check if the cleaned symbol is empty before attempting to split
    if not sym_cleaned.strip():
        return "Unknown"

    # Use try-except block to handle potential IndexError
    try:
        key = re.sub(r"[^\w]", "", unicodedata.normalize("NFKC", sym_cleaned).split()[0]).upper()
    except IndexError:
        # If there is an IndexError (i.e., split() results in an empty list), return "Unknown"
        return "Unknown"

    # Check if the normalized symbol is in the SYMBOL_MAP, and return the canonical version if it exists
    mapped = SYMBOL_MAP.get(key, f"Unknown ({key})")

    # If mapping gave you empty or whitespace, force "Unknown"
    return mapped if mapped and mapped.strip() else "Unknown"

"""## grab block number from date if you wish"""

BLOCK_CACHE_PATH    = "/content/block_range_cache.json"  # disk cache location (change if you like)
DEFAULT_CHAINID     = 1   # 1 = Ethereum mainnet
CHAIN_ID = 1
# ------------------------------------------------------------------
# 1)  Low-level call: timestamp → block number   (memoized)
# ------------------------------------------------------------------
@lru_cache(maxsize=None)
def block_number_by_time(
    ts: pd.Timestamp,
    closest: str = "before",
    chainid: int = DEFAULT_CHAINID,
) -> int:
    """
    Query Etherscan V2 for the block number nearest `ts`.
    Uses in-memory LRU cache + basic retry on rate-limit.
    """
    url = (
        f"{BASE_URL}"
        f"?chainid={chainid}"
        f"&module=block"
        f"&action=getblocknobytime"
        f"&timestamp={int(ts.timestamp())}"
        f"&closest={closest}"
        f"&apikey={ETHERSCAN_API_KEY}"
    )

    while True:
        js = requests.get(url, timeout=10).json()
        if js.get("status") == "1":
            return int(js["result"])
        if "rate limit" in str(js.get("result", "")).lower():
            time.sleep(0.6)  # back-off and retry
            continue
        raise ValueError(f"Etherscan error: {js}")

# ------------------------------------------------------------------
# 2)  High-level helper: (start, end) → (start_block, end_block)
#     • Persists to JSON so each range is fetched only once.
# ------------------------------------------------------------------
def get_block_range_for_period(
    start: pd.Timestamp,
    end: pd.Timestamp,
    chainid: int = DEFAULT_CHAINID,
    cache_path: str = BLOCK_CACHE_PATH,
) -> Tuple[int, int]:
    """
    Return (start_block, end_block) for the given date window.
    Cached on disk so later runs are instant.
    """
    # canonical cache key  e.g. "20241101_20241201_1"
    key = f"{start.strftime('%Y%m%d')}_{end.strftime('%Y%m%d')}_{chainid}"

    # load cache if present
    cache = {}
    if os.path.exists(cache_path):
        with open(cache_path, "r") as f:
            cache = json.load(f)

    # compute if missing
    if key not in cache:
        blk_start = block_number_by_time(start, "before", chainid)
        time.sleep(0.6)  # guard against burst
        blk_end   = block_number_by_time(end,   "before", chainid)
        cache[key] = {"start": blk_start, "end": blk_end}
        with open(cache_path, "w") as f:
            json.dump(cache, f, indent=2)

    rng = cache[key]
    return rng["start"], rng["end"]

# ------------------------------------------------------------------
# 3)  EXAMPLE  – produce constants you can pass to your fetchers
# ------------------------------------------------------------------

BLOCK_START, BLOCK_END = get_block_range_for_period(start, end, chainid=1)

print("Block range:", BLOCK_START, BLOCK_END)

"""# insane pull"""

import pandas as pd
from web3 import Web3
from web3.exceptions import BadFunctionCallOutput, ContractLogicError
from functools import lru_cache
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm

df = pd.read_csv(
    f'/content/drive/MyDrive/Drip_Capital/accounting_records/20241231/20241231_CLEAN_all_wallets_with_contract_names.csv',
    parse_dates=['transaction_datetime']
)
df = df[df["hash"] == "0xb4d18ef8d7f5b7aab2ffeadc91bb53bf7bf41d5e396598eeb51d0397b97b9bd1"].copy()

df

import pandas as pd
from web3 import Web3
from web3.exceptions import BadFunctionCallOutput, ContractLogicError
from functools import lru_cache
from concurrent.futures import ThreadPoolExecutor, as_completed

# === Setup ===
INFURA_URL = "https://mainnet.infura.io/v3/16f12641c1db46beb60e95cf4c88cbe1"
w3 = Web3(Web3.HTTPProvider(INFURA_URL))

# === Load hashes ===
df = pd.read_csv(
    '/content/drive/MyDrive/Drip_Capital/accounting_records/20241231/20241231_CLEAN_all_wallets_with_contract_names.csv',
    parse_dates=['transaction_datetime']
)

# Optional sanity check
#target_hash = "0x1e1c0abdf00e5829f5ae2113c87dce40bba6437557772f55b6000b44cbb44202"
#df = df[df["hash"].str.lower() == target_hash.lower()]

hashes = df["hash"].dropna().unique()
# === Helpers ===
def extract_eth_transfer(tx_hash):
    try:
        tx = w3.eth.get_transaction(tx_hash)
        if tx["value"] > 0:
            return pd.DataFrame([{
                "tx_hash": tx_hash,
                "logIndex": None,
                "contract_address": None,
                "event_name": "ETH Transfer",
                "from": tx["from"],
                "to": tx["to"],
                "value_int": tx["value"],  # Match process_tx_receipt
                "tokenSymbol": "ETH",
                "tokenDecimal": 18,
                "date": pd.to_datetime(w3.eth.get_block(tx["blockNumber"]).timestamp, unit="s"),
                "function": None  # Leave blank for consistency
            }])
        return pd.DataFrame(columns=[
            "tx_hash", "logIndex", "contract_address", "event_name",
            "from", "to", "value_int", "tokenSymbol", "tokenDecimal",
            "date", "function"
        ])
    except Exception as e:
        print(f"❌ extract_eth_transfer failed for {tx_hash}: {e}")
        return pd.DataFrame(columns=[
            "tx_hash", "logIndex", "contract_address", "event_name",
            "from", "to", "value_int", "tokenSymbol", "tokenDecimal",
            "date", "function"
        ])


def clean_topic_to_address(topic_hex):
    try:
        if pd.isna(topic_hex) or topic_hex in ["None", "", "0x", "0X"]:
            return None
        return Web3.to_checksum_address("0x" + topic_hex[-40:])
    except Exception:
        return None

def clean_data_to_int(data_hex):
    try:
        if pd.isna(data_hex) or data_hex in ["None", "", "0x", "0X"]:
            return None
        val = int(data_hex, 16)
        return val
    except (ValueError, OverflowError, TypeError) as e:
        print(f"❌ clean_data_to_int failed — data: {data_hex} — error: {e}")
        return None

TOPIC0_HASH_MAP = {
    "0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef": "Transfer",
    "0xe1fffcc4923d04b559f4d29a8bfc6cda04eb5b0d3c460751c2402c5c5cc9109c": "Deposit",
    "0x7fcf532c15f0a6db0bd6d0e038bea71d30d808c7d98cb3bf7268a95bf5081b65": "Withdraw"
}

def detect_event_name(topic0):
    if isinstance(topic0, str):
        cleaned = topic0.lower()
        if not cleaned.startswith("0x"):
            cleaned = "0x" + cleaned
        return TOPIC0_HASH_MAP.get(cleaned, "Other")
    return "Other"

# ========================================== Log Parsing Function ==================================================================
def process_tx_receipt(tx_hash):
    try:
        # Get receipt and block
        receipt = w3.eth.get_transaction_receipt(tx_hash)
        block = w3.eth.get_block(receipt.blockNumber)
        tx_date = pd.to_datetime(block.timestamp, unit="s")

        # Get transaction to extract function selector
        tx = w3.eth.get_transaction(tx_hash)
        function_selector = tx.input[:10] if tx.input and tx.input != "0x" else None
    except Exception as e:
        print(f"❌ Error retrieving tx or receipt for {tx_hash}: {e}")
        return pd.DataFrame()

    decoded_rows = []
    for log in receipt.logs:
        try:
            row = {
                "tx_hash": tx_hash,
                "logIndex": log.get("logIndex", None),
                "contract_address": log.get("address", None),
                "topic0": log["topics"][0].hex() if len(log.get("topics", [])) > 0 else None,
                "topic1": log["topics"][1].hex() if len(log.get("topics", [])) > 1 else None,
                "topic2": log["topics"][2].hex() if len(log.get("topics", [])) > 2 else None,
                "data": log.get("data", ""),
                "date": tx_date,
                "function": function_selector
            }

            if isinstance(row["data"], bytes):
                row["data"] = row["data"].hex()
            decoded_rows.append(row)
        except Exception:
            continue

    if not decoded_rows:
        return pd.DataFrame()

    logs_df = pd.DataFrame(decoded_rows)

    logs_df["event_name"] = logs_df["topic0"].apply(detect_event_name)
    logs_df["from"] = logs_df["topic1"].apply(clean_topic_to_address)
    logs_df["to"] = logs_df["topic2"].apply(clean_topic_to_address)

    TRANSFER_LIKE_EVENTS = {"Transfer", "Withdraw", "Deposit"}
    logs_df["value_int"] = logs_df.apply(
        lambda row: clean_data_to_int(row["data"]) if row["event_name"] in TRANSFER_LIKE_EVENTS else None,
        axis=1
    ).astype("object")

    # Add ETH transfer if exists
    eth_df = extract_eth_transfer(tx_hash)
    if not eth_df.empty:
        eth_df["function"] = function_selector
        logs_df = pd.concat([logs_df, eth_df], ignore_index=True)

    return logs_df[[
        "tx_hash", "logIndex", "contract_address", "event_name",
        "from", "to", "value_int", "date", "function"
    ]]





# Process all logs into list first
log_dfs = []

with ThreadPoolExecutor(max_workers=100) as executor:
    futures = {executor.submit(process_tx_receipt, h): h for h in hashes}

    for i, future in enumerate(as_completed(futures)):
        tx_hash = futures[future]
        try:
            df_logs = future.result()
            if not df_logs.empty:
                print(f"✅ [{i+1}/{len(futures)}] tx: {tx_hash} → {len(df_logs)} logs")
                log_dfs.append(df_logs)
            else:
                print(f"⚠️ [{i+1}/{len(futures)}] tx: {tx_hash} → empty")
        except Exception as e:
            print(f"❌ [{i+1}/{len(futures)}] tx: {tx_hash} failed with error: {e}")


# Try concat
try:
    all_logs = pd.concat(log_dfs, ignore_index=True)
    print("✅ Concat successful! Shape:", all_logs.shape)
except Exception as e:
    print("💥 Concat failed:", e)
    all_logs = None

all_logs_checka = all_logs[all_logs["tx_hash"] == "0xa2469c2ea4ce6b37a12340e26887f282f394491b7845d5f0feb03814da08390a"]
all_logs_checka

all_logs.tail(1)

import requests

@lru_cache(maxsize=None)
def decode_function_selector(selector):
    if not selector or not isinstance(selector, str) or not selector.startswith("0x"):
        return selector
    try:
        response = requests.get(
            f"https://www.4byte.directory/api/v1/signatures/?hex_signature={selector}",
            timeout=5
        )
        results = response.json().get("results", [])
        return results[0]["text_signature"] if results else selector
    except Exception as e:
        print(f"❌ Failed to decode {selector}: {e}")
        return selector
@lru_cache(maxsize=None)
def normalize_function_selector(x):
    if isinstance(x, bytes):
        return Web3.to_hex(x)[:10]
    if isinstance(x, str) and x.startswith("0x"):
        return x[:10]  # keep only 4-byte selector
    return None

all_logs["function"] = all_logs["function"].apply(normalize_function_selector)
all_logs["function"] = all_logs["function"].apply(decode_function_selector)

all_logs.tail(1)

df_logs1 = all_logs[all_logs["value_int"].notna() & (all_logs["value_int"] != 0)].copy()
df_logs1.sample(20)

ADDRESS_SYMBOL_OVERRIDES = {
    "0x0000000000a39bb272e79075ade125fd351887ac": "BLUR POOL",
    "0xb131f4a55907b10d1f0a50d8ab8fa09ec342cd74": "MEME",
    "0xa0b86991c6218b36c1d19d4a2e9eb0ce3606eb48": "USDC",
    "0x72e95b8931767c79ba4eee721354d6e99a61d004": "VARIABLEDEBTETHUSDC",
    "0xe020b01b6fbd83066aa2e8ee0ccd1eb8d9cc70bf": "ARCD",
    "0x4d5f47fa6a74757f35c14fd3a6ef8e3c9bc514e8": "AETHWETH",
    "0xc02aaa39b223fe8d0a0e5c4f27ead9083c756cc2": "WETH"
}

# Normalize address
df_logs1["contract_address"] = df_logs1["contract_address"].str.lower()

# Set tokenSymbol
df_logs1["tokenSymbol"] = df_logs1.apply(
    lambda row: (
        "ETH" if pd.isna(row["contract_address"]) or row["contract_address"] == ""
        else ADDRESS_SYMBOL_OVERRIDES.get(row["contract_address"], "MYSTERY")
    ),
    axis=1
)

# Set tokenDecimal: USDC = 6, all others = 18
df_logs1["tokenDecimal"] = df_logs1["tokenSymbol"].apply(
    lambda x: 6 if x in ["USDC", "VARIABLEDEBTETHUSDC"] else 18
)

df_logs1.head(3)

df_hash_checker = df_logs1[df_logs1["tx_hash"] == "0xa2469c2ea4ce6b37a12340e26887f282f394491b7845d5f0feb03814da08390a"]
df_hash_checker

"""# eth checka"""

import pandas as pd
from concurrent.futures import ThreadPoolExecutor, as_completed

# === Load Data ===
df = pd.read_csv(
    '/content/drive/MyDrive/Drip_Capital/accounting_records/20241231/20241231_CLEAN_all_wallets_with_contract_names.csv',
    parse_dates=['transaction_datetime']
)
#df = df[df["hash"] == "0x1e1c0abdf00e5829f5ae2113c87dce40bba6437557772f55b6000b44cbb44202"]
hashes = df["hash"].dropna().unique()

# === Threaded Execution ===
eth_transfer_results = []

with ThreadPoolExecutor(max_workers=10) as executor:
    futures = {executor.submit(extract_eth_transfer, tx_hash): tx_hash for tx_hash in hashes}

    for i, future in enumerate(as_completed(futures), 1):
        tx_hash = futures[future]
        try:
            result_df = future.result()
            if not result_df.empty:
                print(f"✅ [{i}/{len(futures)}] ETH transfer found: {tx_hash}")
                eth_transfer_results.append(result_df)
            else:
                print(f"⚠️ [{i}/{len(futures)}] No ETH transfer: {tx_hash}")
        except Exception as e:
            print(f"❌ [{i}/{len(futures)}] Error on {tx_hash}: {e}")
# === Combine all results into a single DataFrame ===
if eth_transfer_results:
    eth_transfers_df = pd.concat(eth_transfer_results, ignore_index=True)
    print(f"✅ Final ETH transfer DataFrame created: {len(eth_transfers_df)} rows")
else:
    eth_transfers_df = pd.DataFrame()
    print("⚠️ No ETH transfers found, created empty DataFrame.")

eth_transfers_df.columns

df_logs1.columns

"""## checka in the checka"""

eth_checka_hash = eth_transfers_df[eth_transfers_df["tx_hash"] == "0x1e1c0abdf00e5829f5ae2113c87dce40bba6437557772f55b6000b44cbb44202"]
eth_checka_hash

logs_checka_hash = all_logs[all_logs["tx_hash"] == "0x8cc79b3506718f10a4cc230574483041b13b770b157069ad37010325f21e749e"]
logs_checka_hash

eth_transfers_merge_ready = eth_transfers_df.copy()

# Normalize address casing for consistency
eth_transfers_merge_ready["from"] = eth_transfers_merge_ready["from"].str.lower()
eth_transfers_merge_ready["to"] = eth_transfers_merge_ready["to"].str.lower()
df_logs1["from"] = df_logs1["from"].str.lower()
df_logs1["to"] = df_logs1["to"].str.lower()

# Define the subset of columns that identify true duplicates
dupe_key_cols = ["tx_hash", "from", "to", "value_int", "tokenSymbol", "date"]

# Create a copy of df_logs1 with only these columns for comparison
existing_keys = df_logs1[dupe_key_cols].drop_duplicates()

# Merge to flag ETH rows that already exist in df_logs1
eth_transfers_merge_ready["_is_duplicate"] = eth_transfers_merge_ready[dupe_key_cols].apply(
    lambda row: tuple(row) in set([tuple(x) for x in existing_keys.values]),
    axis=1
)

# Filter out duplicates
eth_transfers_df_deduped = eth_transfers_merge_ready[~eth_transfers_merge_ready["_is_duplicate"]].drop(columns="_is_duplicate")

# Reorder and match columns before merging
eth_transfers_df_deduped = eth_transfers_df_deduped[df_logs1.columns]

# Merge
df_ultimate = pd.concat([df_logs1, eth_transfers_df_deduped], ignore_index=True)

# ✅ Print result
print(f"✅ Deduped and merged: {len(eth_transfers_merge_ready)} ETH rows → {len(eth_transfers_df_deduped)} added.")
print(f"✅ Final df_ultimate shape: {df_ultimate.shape}")

df_ultimate = df_ultimate.rename(columns={
    "tx_hash": "hash",
    "logIndex": "logIndex",  # unchanged
    "contract_address": "contractAddress",
    "event_name": "event_name",  # unchanged
    "from_address": "from",
    "to_address": "to",
    "value_int": "value",
    "tokenSymbol": "tokenSymbol",  # unchanged
    "tokenDecimal": "tokenDecimal",  # unchanged
    "date": "transaction_datetime"
})
df_ultimate.columns
 
"""## Rule 0 - Only our transactions"""

# --- Rule 0: Drop transactions not involving known wallets ---
df_rule0 = df_month1.copy()

# Ensure all comparison addresses are lowercase
raw_wallets_set = {addr.lower() for addr in raw_wallets}

# Normalize 'from' and 'to' to lowercase
df_rule0["from"] = df_rule0["from"].str.lower()
df_rule0["to"] = df_rule0["to"].str.lower()

# Keep rows where either sender or receiver is in our wallet list
mask_rule0 = (
    df_rule0["from"].isin(raw_wallets_set) |
    df_rule0["to"].isin(raw_wallets_set)
)

# Apply the rule
df_rule0 = df_rule0[mask_rule0].copy()
df0_applied = df_rule0.copy()  # ✅ this now reflects the actual filtered DataFrame

# Optional shape print or inspection
df0_applied.shape
df_rule0_applied = df0_applied.copy()
print(f"✅ Rule 0 applied: {len(df_month1) - len(df_rule0_applied)} rows dropped, {len(df_rule0_applied)} rows retained involving known wallets.")

"""## Rule 1 - Wrap *WETH*"""

WETH_CONTRACT = "0xc02aaa39b223fe8d0a0e5c4f27ead9083c756cc2"

# Step 1: Copy base DataFrame
df_rule1_applied = df_rule0_applied.copy()

# Step 2: Drop ETH Transfers going to WETH contract
drop_mask = (
    (df_rule1_applied["event_name"] == "ETH Transfer") &
    (df_rule1_applied["to"].str.lower() == WETH_CONTRACT)
)
dropped_count = drop_mask.sum()
df_rule1_applied = df_rule1_applied[~drop_mask]

# Step 3: WETH deposit split logic
mask_rule1 = (
    (df_rule1_applied["event_name"].str.lower() == "deposit") &
    (df_rule1_applied["to"].isna()) &
    (df_rule1_applied["contractAddress"].str.lower() == WETH_CONTRACT)
)

rows_to_duplicate = df_rule1_applied[mask_rule1].copy()

eth_rows = rows_to_duplicate.copy()
eth_rows["tokenSymbol"] = "ETH"
eth_rows["to"] = eth_rows["contractAddress"]
eth_rows["from"] = rows_to_duplicate["from"]
eth_rows["contractAddress"] = None

weth_rows = rows_to_duplicate.copy()
weth_rows["tokenSymbol"] = "WETH"
weth_rows["from"] = weth_rows["contractAddress"]
weth_rows["to"] = rows_to_duplicate["from"]

df_rule1_applied = pd.concat([df_rule1_applied[~mask_rule1], eth_rows, weth_rows], ignore_index=True)

# ✅ Print results
print(f"🗑️ Dropped {dropped_count} ETH Transfer → WETH_CONTRACT rows.")
print(f"✅ Rule 1 applied: {len(rows_to_duplicate)} WETH deposit rows duplicated as ETH + WETH splits.")

df_month1_checker = df_rule1_applied[df_rule1_applied["hash"] == "0x91dcca6b512233dbf0fe79c3c1ca1321a5197e9a3c98fbc7d9b27f94cd6ddc1f"]
df_month1_checker

"""## Rule 2 - Unwrap *WETH*"""

WETH_CONTRACT = "0xc02aaa39b223fe8d0a0e5c4f27ead9083c756cc2"

df_rule2_applied = df_rule1_applied.copy()

mask_rule2 = (
    (df_rule2_applied["event_name"].str.lower() == "withdraw") &
    (df_rule2_applied["to"].isna()) &
    (df_rule2_applied["contractAddress"].str.lower() == WETH_CONTRACT)
)

rows_to_duplicate = df_rule2_applied[mask_rule2].copy()
num_rows = len(rows_to_duplicate)

eth_rows = rows_to_duplicate.copy()
eth_rows["tokenSymbol"] = "ETH"
eth_rows["from"] = eth_rows["contractAddress"]
eth_rows["to"] = rows_to_duplicate["from"]
eth_rows["contractAddress"] = None

weth_rows = rows_to_duplicate.copy()
weth_rows["tokenSymbol"] = "WETH"
weth_rows["to"] = weth_rows["contractAddress"]
weth_rows["from"] = rows_to_duplicate["from"]

df_rule2_applied = pd.concat([df_rule2_applied[~mask_rule2], eth_rows, weth_rows], ignore_index=True)

print(f"Rule 2 duplicated {num_rows} rows into {num_rows * 2} new rows.")

hash_checker = df_rule2_applied[df_rule2_applied["hash"] == "0xd1282673ee38b3d924516f0000aa96793d0863d3d9c77d02ac42c2349f0dddfd"]
hash_checker

"""## Rule 3 - Normalize BLUR POOL name"""

df_rule3_applied = df_rule2_applied.copy()

mask_rule3 = df_rule3_applied["tokenSymbol"].str.upper() == "BLUR"
num_replaced = mask_rule3.sum()

df_rule3_applied.loc[mask_rule3, "tokenSymbol"] = "BLUR POOL"
print(f"Rule 3 replaced {num_replaced} tokenSymbol values with 'BLUR POOL'.")

"""## Rule 4 - Remove phishy things"""

df_rule4_applied = df_rule3_applied.copy()

# Normalize address case
df_rule4_applied["contractAddress"] = df_rule4_applied["contractAddress"].str.lower()
raw_suspects = [addr.lower() for addr in raw_suspects]
fake_phishing = [addr.lower() for addr in fake_phishing]

# Count rows before
before_count = len(df_rule4_applied)

# Remove rows with suspicious contractAddress
df_rule4_applied = df_rule4_applied[
    ~df_rule4_applied["contractAddress"].isin(raw_suspects + fake_phishing)
]

# Count rows after
after_count = len(df_rule4_applied)
print(f"Rule 4 removed {before_count - after_count} suspicious rows.")

"""## Rule 5 - Purchased *BLUR POOL*"""

BLUR_POOL_CONTRACT = "0x0000000000a39bb272e79075ade125fd351887ac"
MINT_ADDRESS = "0x0000000000000000000000000000000000000000"

df_rule5_applied = df_rule4_applied.copy()

# Build mask
mask_rule5 = (
    (df_rule5_applied["from"].str.lower() == MINT_ADDRESS.lower()) &
    (df_rule5_applied["contractAddress"].str.lower() == BLUR_POOL_CONTRACT.lower())
)

# Get matching rows
rows_to_duplicate = df_rule5_applied[mask_rule5].copy()

# Create flipped ETH rows
eth_rows = rows_to_duplicate.copy()
eth_rows["tokenSymbol"] = "ETH"
eth_rows["from"] = rows_to_duplicate["to"]
eth_rows["to"] = rows_to_duplicate["from"]
eth_rows["contractAddress"] = None  # Native ETH

# Append
df_rule5_applied = pd.concat([df_rule5_applied, eth_rows], ignore_index=True)

# ✅ Print what happened
print(f"✅ Rule 5 applied: {len(rows_to_duplicate)} mint rows duplicated as ETH transfers.")

"""## Rule 6 - Sold *BLUR POOL*"""

BLUR_POOL_CONTRACT = "0x0000000000a39bb272e79075ade125fd351887ac"
BURN_ADDRESS = "0x0000000000000000000000000000000000000000"

df_rule6_applied = df_rule4_applied.copy()

# Build mask
mask_rule6 = (
    (df_rule6_applied["to"].str.lower() == BURN_ADDRESS.lower()) &
    (df_rule6_applied["contractAddress"].str.lower() == BLUR_POOL_CONTRACT.lower()) &
    (df_rule6_applied["function"] == "OwnerTransferV7b711143(uint256)")
)

# Get matching rows
rows_to_duplicate = df_rule6_applied[mask_rule6].copy()

# Create flipped ETH rows
eth_rows = rows_to_duplicate.copy()
eth_rows["tokenSymbol"] = "ETH"
eth_rows["from"] = rows_to_duplicate["to"]
eth_rows["to"] = rows_to_duplicate["from"]
eth_rows["contractAddress"] = None  # Native ETH

# Append
df_rule6_applied = pd.concat([df_rule6_applied, eth_rows], ignore_index=True)

# ✅ Print what happened
print(f"✅ Rule 6 applied: {len(rows_to_duplicate)} burn rows duplicated as ETH receipts.")

rows_to_duplicate["function"].unique()

df_for_fifo = df_rule6_applied.copy()
df_for_fifo.shape

hash_checkaa = df_for_fifo[df_for_fifo["hash"] == "0x84f25b9a9f6d1fef4363156c1fc269aaea46a2721a80aa166acff4c8e6d57432"]
hash_checkaa

"""# Add USD Prices"""

from typing import Union
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed

def fetch_all_eth_usd(tx_hashes: Union[str, list[str]], max_workers: int = 10) -> pd.DataFrame:
    # Convert single hash to list
    if isinstance(tx_hashes, str):
        tx_hashes = [tx_hashes]

    results = []

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {executor.submit(get_eth_usd_for_tx, h): h for h in tx_hashes}

        for future in tqdm(as_completed(futures), total=len(futures)):
            result = future.result()
            if result:
                tx_hash, price, dt = result
                results.append({
                    "hash": tx_hash,
                    "TX_ETH_USD_price": price,
                    "block_datetime": dt
                })

    return pd.DataFrame(results)

tx_hashes = df_for_fifo['hash'].dropna().unique().tolist()
df_with_prices = fetch_all_eth_usd(tx_hashes, max_workers=10)

tx_with_price = fetch_all_eth_usd("0x1bf0d0cc928978ac155e618f7f85e17ccf9fcb61a3c7071142d4e4c33da78854")
tx_with_price

df_with_prices

#merge df_with_prices with df_for_group on "hash"
df_for_fifo = df_for_fifo.merge(df_with_prices, left_on='hash', right_on='hash', how='left')

df_for_fifo.tail(5)

hash_checker7 = df_for_fifo[df_for_fifo["hash"] == "0xd28c8c2712e822c4c957d7094d0116734ba0ee3bdee61252a717a8ad247a3452"]
hash_checker7
