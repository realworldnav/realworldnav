# -*- coding: utf-8 -*-
"""master_fifo_project_CAF.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1S2eBZzYU0qgZQXjiXp_gJeDTKGCAPG60

#Libraries

##PIP Installs
"""

!pip install etherscan-python  # Install the correct package

!pip install eth-abi

!pip install web3

!pip install openpyxl

"""##Libraries"""

import os
import re
import math
import time
import json
import glob
import yaml
import pprint
import random
import logging
import warnings
import requests
import unicodedata
import numpy as np
import pandas as pd

from time import sleep
from pathlib import Path
from datetime import datetime, timedelta, timezone
from typing import Tuple, Dict, List, Set
from decimal import Decimal, getcontext, ROUND_DOWN, ROUND_FLOOR, DivisionUndefined
from collections import defaultdict
from collections.abc import Mapping, Sequence
from pandas import json_normalize
from pandas.tseries.offsets import MonthEnd
from tqdm.auto import tqdm
from tqdm.notebook import tqdm as tqdm_notebook
from IPython.display import display

from requests.exceptions import HTTPError, Timeout, RequestException
from openpyxl import load_workbook
from openpyxl import load_workbook
from eth_utils import to_checksum_address
from etherscan import Etherscan
from eth_abi import decode
from eth_abi.abi import decode as decode_abi
from hexbytes import HexBytes
from tenacity import retry, stop_after_attempt, wait_exponential, wait_random, retry_if_exception_type
from web3 import Web3, HTTPProvider
from web3.exceptions import Web3RPCError, ABIFunctionNotFound, Web3ValueError
from web3._utils.events import get_event_data, event_abi_to_log_topic
from functools import lru_cache

from google.colab import drive, files
drive.mount('/content/drive')

"""#Functions"""

import requests
from functools import lru_cache
from web3 import Web3
import json
from datetime import datetime, timezone
from decimal import Decimal
from pathlib import Path
import pandas as pd

ETHERSCAN_API_KEY = "P13CVTCP43NWU9GX5D9VBA2QMUTJDDS941"

# Chainlink ABI + contract
AGGREGATOR_V3_ABI = [{
    "inputs": [],
    "name": "latestRoundData",
    "outputs": [
        {"internalType": "uint80", "name": "roundId", "type": "uint80"},
        {"internalType": "int256", "name": "answer", "type": "int256"},
        {"internalType": "uint256", "name": "startedAt", "type": "uint256"},
        {"internalType": "uint256", "name": "updatedAt", "type": "uint256"},
        {"internalType": "uint80", "name": "answeredInRound", "type": "uint80"},
    ],
    "stateMutability": "view",
    "type": "function",
}]
INFURA_API_KEY = "02321aab179b4085b84cda11f9bffb8a"
w3 = Web3(Web3.HTTPProvider(f"https://mainnet.infura.io/v3/{INFURA_API_KEY}"))

FEED_ADDRESS = w3.to_checksum_address("0x5f4ec3df9cbd43714fe2740f5e3616155c5b8419")
aggregator = w3.eth.contract(address=FEED_ADDRESS, abi=AGGREGATOR_V3_ABI)
# … your existing setup (API keys, Web3, ABI, etc.) …

@lru_cache(maxsize=None)
def get_eth_usd_at_block(block_number: int) -> tuple[Decimal, datetime]:
    """Cache price + timestamp per block number."""
    round_id, answer, *_ = (
        aggregator.functions
                  .latestRoundData()
                  .call(block_identifier=block_number)
    )
    price = Decimal(answer) / Decimal(1e8)
    blk = w3.eth.get_block(block_number)
    return price, datetime.fromtimestamp(blk.timestamp, tz=timezone.utc)

@lru_cache(maxsize=None)
def get_eod_price_for_date(year: int, month: int, day: int) -> tuple[Decimal, datetime]:
    """
    Cache one EOD lookup per date.
    Binary‐search block at 23:59:59 UTC of that day and grab its price.
    """
    eod_dt = datetime(year, month, day, 23, 59, 59, tzinfo=timezone.utc)
    target_ts = int(eod_dt.timestamp())

    # find the latest block before or at that timestamp
    low, high = 0, w3.eth.block_number
    while low <= high:
        mid = (low + high) // 2
        ts = w3.eth.get_block(mid).timestamp
        if ts <= target_ts:
            low = mid + 1
        else:
            high = mid - 1
    block_eod = high

    return get_eth_usd_at_block(block_eod)

from concurrent.futures import ThreadPoolExecutor, as_completed
from decimal import Decimal
from datetime import datetime

def get_eth_usd_for_tx(tx_hash: str) -> tuple[str, Decimal, datetime] | None:
    """
    Returns (tx_hash, price_usd, block_datetime_utc) tuple, or None on error.
    """
    try:
        tx = w3.eth.get_transaction(tx_hash)
        price, dt = get_eth_usd_at_block(tx.blockNumber)
        return tx_hash, price, dt
    except Exception as e:
        print(f"❌ Error processing {tx_hash}: {e}")
        return None


def eth_usd_df_with_eod(tx_hashes, w3, aggregator) -> pd.DataFrame:
    rows = []
    for tx_hash in tqdm(tx_hashes, desc="Fetching ETH/USD prices"):
        try:
            price_tx, dt_tx = get_eth_usd_for_tx(tx_hash)
            if price_tx is None:
                print(f"[SKIP] No Chainlink price for tx {tx_hash} (block before feed start?)")
                continue

            price_eod, dt_eod = get_eod_price_for_date(dt_tx.year, dt_tx.month, dt_tx.day)
            rows.append({
                'hash':                     tx_hash,
                'TX_ETH_USD_price':         price_tx,
                'TX_transaction_datetime':  dt_tx,
                'EOD_ETH_USD_price':        price_eod,
                'EOD_transaction_datetime': dt_eod
            })
        except Exception as e:
            print(f"[ERROR] Skipping {tx_hash}: {e}")
            continue

    return pd.DataFrame(rows)

def get_implementation_address(w3, proxy):
    proxy = w3.to_checksum_address(proxy)

    # Try checking the implementation address using the EIP-1967 pattern
    impl_slot = int("0x360894A13BA1A3210667C828492DB98DCA3E2076CC3735A920A3CA505D382BBC", 16)
    try:
        raw_impl = w3.eth.get_storage_at(proxy, impl_slot)
        impl = w3.to_checksum_address(raw_impl[-20:].hex())
        if impl != "0x0000000000000000000000000000000000000000":
            return impl
    except Exception as e:
        print(f"EIP-1967 check failed: {e}")

    # Check for EIP-1967 beacon pattern
    beacon_slot = int("0xa3f0ad74e5423aebfd80d3ef4346578335a9a72aeaee59ff6cb3582b35133d50", 16)
    try:
        raw_beacon = w3.eth.get_storage_at(proxy, beacon_slot)
        beacon = w3.to_checksum_address(raw_beacon[-20:].hex())
        if beacon != "0x0000000000000000000000000000000000000000":
            beacon_abi = [{
                "inputs": [],
                "name": "implementation",
                "outputs": [{"internalType": "address", "name": "", "type": "address"}],
                "stateMutability": "view",
                "type": "function"
            }]
            beacon_ct = w3.eth.contract(address=beacon, abi=beacon_abi)
            return beacon_ct.functions.implementation().call()
    except Exception as e:
        print(f"EIP-1967 beacon check failed: {e}")

    # Check for EIP-1167 minimal proxy pattern
    try:
        code = w3.eth.get_code(proxy).hex()
        if code.startswith("0x3d602d80600a3d3981f3") and code.endswith("5af43d82803e903d91602b57fd5bf3"):
            impl_bytes = code[10*2:10*2 + 40]
            return w3.to_checksum_address("0x" + impl_bytes)
    except Exception as e:
        print(f"EIP-1167 minimal proxy check failed: {e}")

    # Additional Fallback: Manually inspect for known proxy patterns
    try:
        # Example: Check for a custom implementation-fetching function (if known)
        contract = w3.eth.contract(address=proxy, abi=[{
            "inputs": [],
            "name": "getImplementation",
            "outputs": [{"internalType": "address", "name": "", "type": "address"}],
            "stateMutability": "view",
            "type": "function"
        }])
        impl_address = contract.functions.getImplementation().call()
        return Web3.to_checksum_address(impl_address)
    except Exception as e:
        print(f"Custom proxy pattern check failed: {e}")

    return None  # If nothing matched, return None

def fetch_abi_and_name(addr: str) -> tuple[str, dict]:
    """Returns (contract_name, abi_dict), or (address, None) if ABI not verified or EOA"""
    addr = addr.lower()
    try:
        code = w3.eth.get_code(Web3.to_checksum_address(addr))
        if code in (b'', b'0x'):
            print(f"→ {addr} is an EOA, skipping ABI fetch.")
            return addr, None

        resp = requests.get(
            "https://api.etherscan.io/api",
            params={
                "module": "contract",
                "action": "getsourcecode",
                "address": addr,
                "apikey": ETHERSCAN_API_KEY
            }
        ).json()

        result = resp.get("result", [{}])[0]
        abi_raw = result.get("ABI")
        if abi_raw in (None, "Contract source code not verified", ""):
            print(f"→ ABI not found or not verified for {addr}")
            return addr, None

        name = result.get("ContractName", addr)
        abi = json.loads(abi_raw)
        return name, abi

    except Exception as e:
        print(f"→ Error fetching ABI for {addr}: {e}")
        return addr, None


missing_abis = []

def save_abi(addr: str, abi: dict):
    if abi is None:
        print(f"→ ABI missing for {addr}, skipping save")
        missing_abis.append(addr)
        return
    path = ABI_DIR / f"{addr.lower()}.json"
    with open(path, "w") as f:
        json.dump(abi, f, indent=2)
    print()
    print(f"→ ABI for {addr} saved to {path}")

def fetch_token_symbol(contract_address: str, w3: Web3, cache: Dict[str, str]) -> str:
    try:
        addr = w3.toChecksumAddress(contract_address)
    except Exception:
        return ""
    if addr in cache:
        return cache[addr]
    try:
        token = w3.eth.contract(address=addr, abi=ERC20_METADATA_ABI)
        sym = token.functions.symbol().call() or ""
    except Exception:
        sym = ""
    cache[addr] = sym
    return sym

def normalize_symbol(sym: str) -> str:
    """Clean raw symbol to canonical ticker, never returning blank."""
    if not isinstance(sym, str) or not sym.strip():
        return "Unknown"

    # Remove unwanted phrases and URLs
    sym_cleaned = re.sub(r"(?:\$|ETH35\.com|Visit to claim bonus rewards).*", "", sym)

    # Normalize the symbol by removing special characters and ensuring it is in a consistent format
    # --- CHANGE --- Check if the cleaned symbol is empty before attempting to split
    if not sym_cleaned.strip():
        return "Unknown"

    # --- CHANGE --- Use try-except block to handle potential IndexError
    try:
        key = re.sub(r"[^\w]", "", unicodedata.normalize("NFKC", sym_cleaned).split()[0]).upper()
    except IndexError:
        # If there is an IndexError (i.e., split() results in an empty list), return "Unknown"
        return "Unknown"

    # Check if the normalized symbol is in the SYMBOL_MAP, and return the canonical version if it exists
    mapped = SYMBOL_MAP.get(key, f"Unknown ({key})")

    # If mapping gave you empty or whitespace, force "Unknown"
    return mapped if mapped and mapped.strip() else "Unknown"

"""#Set Variables"""

#Time Period
start = pd.Timestamp("2024-07-01", tz="UTC")
end   = pd.Timestamp("2025-01-01", tz="UTC")
current_period = end
name_period = "20241231"
from decimal import Decimal, getcontext, ROUND_HALF_EVEN
getcontext().prec = 50  # ensure enough precision to work with 18 dp


SCALE_CRYPTO = Decimal('0.000000000000000001')  # 18 decimal places
SCALE_USD    = Decimal('0.01')                  # 2 decimal places

from pathlib import Path
import os

BASE_URL = "https://api.etherscan.io/v2/api"


# Directory paths
ROOT_DIR = Path("/content/drive/MyDrive/Drip_Capital")
ABI_DIR = ROOT_DIR / "smart_contract_ABIs"
ACCOUNTING_DIR = ROOT_DIR / "accounting_records"
NFT_INVESTMENTS_DIR = ACCOUNTING_DIR / "investments/NFT"
CRYPTO_INVESTMENTS_DIR = ACCOUNTING_DIR / "investments/cryptocurrency"

# Convert current_period to string (e.g., "20250731")
current_period_str = current_period.strftime("%Y%m%d")

# Period-specific directories
PERIOD_DIR = ACCOUNTING_DIR / current_period_str
JOURNAL_ENTRIES_DIR = PERIOD_DIR / "general_journal_entries"

# Create necessary directories
for path in [PERIOD_DIR, JOURNAL_ENTRIES_DIR, NFT_INVESTMENTS_DIR, CRYPTO_INVESTMENTS_DIR]:
    os.makedirs(path, exist_ok=True)

# Print paths for verification
print("ABI Directory:", ABI_DIR)
print("Accounting Directory for Current Period:", PERIOD_DIR)
print("Journal Entries Directory:", JOURNAL_ENTRIES_DIR)
print("NFT Investments Directory:", NFT_INVESTMENTS_DIR)
print("Cryptocurrency Investments Directory:", CRYPTO_INVESTMENTS_DIR)

"""## phishy"""

mint_or_burn = "0x0000000000000000000000000000000000000000"
wrapped_ETH_address = "0xC02aaA39b223FE8D0A0e5C4F27eAD9083C756Cc2" #wrapped Etherium
CoW_protocol_ETH_flow = "0x40A50cf069e992AA4536211B23F286eF88752187"
usdt_contract_address = "0xdAC17F958D2ee523a2206206994597C13D831ec7" #USDT Token
usdc_contract_address = "0xa0b86991c6218b36c1d19d4a2e9eb0ce3606eb48" #USDC Token
coinbase_prime = "0xCD531Ae9EFCCE479654c4926dec5F6209531Ca7b"
coinbase_prime_2 = "0xceB69F6342eCE283b2F5c9088Ff249B5d0Ae66ea"
coinbase_10 = "0xA9D1e08C7793af67e9d92fe308d5697FB81d3E43"

#Lending Platform Smart Contracts for Fund's Transactions
smart_contract_Gondi = "0xf65B99CE6DC5F6c556172BCC0Ff27D3665a7d9A8" #MultiSourceLoan; ERC-20: GONDI_MULTI_SOURCE_LOAN
smart_contract_Gondi_V2 = "0x478f6F994C6fb3cf3e444a489b3AD9edB8cCaE16" #MultiSourceLoan; ERC-20: GONDI_MULTI_SOURCE_LOAN
smart_contract_P2PLendingNfts = "0x5F19431BC8A3eb21222771c6C867a63a119DeDA7" #P2PLendingNfts
smart_contract_blur_pool = "0x0000000000A39bb272e79075ade125fd351887Ac" #Blur; ERC1967Proxy

#other
meme_coin_contract_address = "0xb131f4A55907B10d1F0A50d8ab8FA09EC342cd74" #Memecoin
chainlink_USD_ETH_price_feed = "0x5f4eC3Df9cbd43714FE2740f5E3616155c5b8419"


#Phishing/Scam addresses

raw_suspects = [
    "0x2a120e7f2F1d8fFD173eD17Aa5089f11206B5177",
    "0xcB4b7a5114E02c144a915c05C59192a6c6f33d5A",
    "0xa8F41D54Fd002aa0D027d010CDC3FCF3fd8F40c7",
    "0x8421f2Ae7f7D6ec64698E6A142515609932cFAbC",
    "0x5d72fcee79efe6a493078b57b310f8a854bcc71b",
    "0x729f7430e3e715c84bca27821a5e554cad056a35",
    "0xb6b15d694b07411823fe04ecd27399f18c521574",
    "0x4fbb350052bca5417566f188eb2ebce5b19bc964",
    "0xd08fd4141932f47a644b77a7ef968f552fa4daa0",
    "0x47c639efbabb3af26f95efb571293479e6c1d9cd",
    "0x1b3e77a721b2714fe7f80874e499e7825da29d0d",
    "0x3e3e8c461e4024757d0f81a30e9bcad8b3520671",
    "0xab3e1e638b19a8dbecc47d6d6433dbea67a76cb2",
    "0x91554a9f1b6582c6743f9d876c822816fd9639b8",
    "0xbf3314734852ecd952fd862da853d68d0a83e530",
    "0xe56333c2aedfeb4fbd5b7a4dbedc1b0f99e15abd",
    "0xcb2757d719f43ceb84d53915f4fa114be2fa3792",
    "0x69d706cfa647f989ad7e3f2cf151fd9c41e4ddb3",
    "0xa423e0855176835633c0d38b7c3cdda939903c02",
    "0xc04327b22e2160d1746d9b664d434e831dc06591",
    "0xf70b6c73e6ce7b82436b6e2f1c02dd50487b7362",
    "0x679488415fd76b482acda5328d90290d387835aa",
    "0x94b1afdd235b0daad3f56cc5507df2a6272c8013",
    "0x3b2ad323e2218de2eb57228e64f0073b3529713f",
    "0xa7504f4258e238b957c20b34427642700020ebd9",
    "0xdE9E976C9C53C22A2a0C74F50d5D5c70B35ffa8f",
]

# “fake_phishing” list
fake_phishing = [
    "0x0842661E4d34364c9d9023De581146DdeCF1d2d9",
    "0xE12933c0413Ca50F149C0379C797e515A96935Da",
    "0xaC52eD1e812d968BD5AF7Edb33B73A3559d7DaA0",
    "0x2B496312bD67Ab4F3a8519cda865F9728E50d209",
    "0x1bcc835e7a0e7f0672012e775967d4269f0c6dbc",
    "0x2a120e7f2F1d8fFD173eD17Aa5089f11206B5177",
    "0xcB4b7a5114E02c144a915c05C59192a6c6f33d5A",
    "0xa8F41D54Fd002aa0D027d010CDC3FCF3fd8F40c7",
    "0x8421f2Ae7f7D6ec64698E6A142515609932cFAbC",
    "0xdF8E18c88A0419bCBb81B720425d132341873A3A",
    "0x57F2EcD5A7Dd825293F922456c7887c182CF6A75",
    "0x29F7bbBc757aBB299Dd18e70f0D0F08CC7d570E6",
    "0xeBBa46F99D4Cb2BDc2dd344885CD5A90EFB766c1",
    "0x9E69CE60d1C1f36AB77BC8aBf3B708E86f97a1a5",
    "0xb07b783b117d1cc7a8bee2bba7269ab4d9c00875",
    "0xFA90071f8fBBa47Fb657e7D0E4439aA80A8FdAb0",
    "0xD327AF2c29e04202db17fc0C91b5060551A69D4C",
    "0x77aBA3531b9A444C1E062c3DaF5E5FCdaF8c2Ca3",
    "0x8928e48ddbe739a46013b92ed37cb69243789030",
    "0xd081b96d116912e54475ac6c0a26a3d7a2ba4b98",
    "0xb90319c573bfc546bcad5f0cc08c976f84aa1675",
    "0xff8d4d679affd82db7beeabf4708c777962bf620",
    "0x84b8032b5f2130b0eeef9809f23432d72e94222b",
    "0x1fe156b25ffc7b69fcd6643c3f06597ba5b4f60e",
    "0x42da1d197d1227a5214bb34e61d7127deb0eb015",
    "0x99ad412a77f11f712151b8ea2e3e31174f9d31a1",
    "0x669507921b89c11536fe28fb2b955fbc23dbe86d",
    "0xd504bbd965a7517b91e76f4367c731a61db9d490",
    "0x5313e1a88643dc79be044cafe011ea22864d7213",
    "0xe1a02b742c02dbb50d6c91577e2fe834a86d7db7",
    "0x4e7284fc10c0ebf9a2a34fd056026e5092e27d69",
    "0xedb2ebbd587f6ebeebe8635b57b8c03cff732159",
    "0x217ab96793512c4d49903df604af3f84dafe33b8",
    "0x17ff065985ca9ca0b20f6d32aa598dace61e49fb",
    "0x83089881fe93fb5d54b4bf765defe69db18658a4",
    "0x88e374f40d0be6fb7bb5ea6f595447b98db537f8",
    "0x25cd7a8e675ec82aa5edd7c5888fc334ce391863",
    "0x6f8b6ea1e53546e713393dec2fa7e574e92187f6",
    "0x5098ad0ffb45fbc6bde74f20c789bf33ef300b8e",
    "0x01ce1491ca73676cea9d25e4c284afb70981d6a3",
    "0x6dd75b227243f50db2c85052395c1d24d6a760b2",
    "0x2b2044531f997c846d5be5435286b2dbe1b04121",
    "0x0732ec41e2913d4924b8ad4b2089d5022e027ea7",
    "0xa6cf52a11549fc635256049e7c0244c56d3ba93e",
    "0x253e54860b416357c6804319a33b35f9bfc3c48d",
    "0x3d1f95c154b7be55f2179803785f85fd88d6245e",
    "0x6bcdafb6d8cff42fe62098cffef844998cffd140",
    "0x82a8dcb6ca21118f5bef31a632ff2f295e041cef",
    "0x07463ee1ba9c1fbfd6755965f89eb034276ff505",
    "0x7faf8ea5b506843796c999423f10041bd74dc713",
    "0x5fb249dbad777690a057a7778ff1ef42d3c8ee29",
    "0xabb0ad3232a51e3050f0cd660ee24f0ae27f377e",
    "0x7999e39bbc4d1794eecb55d29320b46639e866e8",
    "0x1315c4ccb95443a4385d3e784b31488859dcb633",
    "0x26b5f2bde7110d3c9e871c4f5799ea6bba7548aa",
    "0x9243ec06bf64d8b0d9b6f74806282487971e9b8e",
    "0xa406ed0fbdf166a6bc1b4855db86255f7c14437a",
    "0x9d61a0763cbd832152e1bb9b362c70ae6d3ad6c2",
    "0x8ceb1bdb554d8501f3134b7383233e40812e7fc4",
    "0x97bf0e89c2541d29b88d7874d257e87dc18f8004",
    "0xf1a2c14c485b43aca4afe8d593ad244293392b3b",
    "0x67761bbe1b56b16d9f58608215b1fb7d1d4b1afc",
    "0x4cf141870094cb2b8347ff11bfcbfbb69b2e0c25",
    "0x77ca2bc773e654e92e65a776242470336b9e619f",
    "0x2dacb3392c3c94653ffc22bb8d6b3e475afab94f",
    "0x32d5e0cc73a6ca358a111a826c8e919bf701726d",
    "0x09e571dadeb2aa116872bd2c59a8246797645c80",
    "0xe52f621c4a4c833341b5537220bd819c2eb54c01",
    "0x4f68df00b182440102ba573dcbe9e249404897d5",
    "0x71908f11f1d684ca84faa5ceab3cdf10028e96e1",
    "0x9a090dd9891e0171bd1c205eb400c04e9fe0e3be",
    "0xf76a7a74fd9e04820caf535852a58c358c93c597",
    "0x3513cbdfcc78b428d33f8633e2d64600573450b7",
    "0x3c9b970f1dd9dc5356d81cb699a670b01c1669cb",
    "0x25a0d6092ee10fd34d56fb4aca324664aff7ad1f",
    "0x5d72fcee79efe6a493078b57b310f8a854bcc71b",
    "0x3b2ad323e2218de2eb57228e64f0073b3529713f",
    "0x3b2a866aa3fed5bfd9bab9d7bf8506c4ff70713f",
    "0x3b2ad323e2218de2eb57228e64f0073b3529713f",
    "0xdE9E976C9C53C22A2a0C74F50d5D5c70B35ffa8f",
    "0x0842661E4d34364c9d9023De581146DdeCF1d2d9",
    "0xca7582f355b8d6041d9f0fe65569d7c1b6a3955d",
    "0xa7504f4258e238b957c20b34427642700020ebd9",
    "0x3fC29836E84E471a053D2D9E80494A867D670EAD",
    "0xcB4b7a5114E02c144a915c05C59192a6c6f33d5A",
    "0xaC52eD1e812d968BD5AF7Edb33B73A3559d7DaA0",
    "0x31127c1A3011F141F29A35BD8f7dcBd1b0952Fd5",
    "0x5ff0d2de4cd862149c6672c99b7edf3b092667a3",
]

# normalize to lowercase
suspects = [addr.lower() for addr in (raw_suspects + fake_phishing)]

"""##Wallet ID Metadata"""

#Load excel file for wallet mapping

wallet_ID_mapping_file_df = pd.read_excel('/content/drive/MyDrive/Drip_Capital/drip_capital_wallet_ID_mapping.xlsx', engine='openpyxl')  # Explicitly specify the engine
wallet_ID_mapping_file_df = wallet_ID_mapping_file_df.fillna('') # Replace NaN with empty string
wallet_ID_mapping_file_df.tail()

wallet_addy = wallet_ID_mapping_file_df[wallet_ID_mapping_file_df["wallet_address"] == "0xA9898fa8E23cE51a779f30506285ea4636f0b033"]
wallet_addy

wallet_metadata      = {}        # address ➔ metadata
wallets_by_group     = {}        # group   ➔ {address ➔ metadata}

# build both dictionaries
for _, row in wallet_ID_mapping_file_df.iterrows():
    addr = Web3.to_checksum_address(row["wallet_address"].lower())
    meta = {
        "friendly_name"        : row["friendly_name"],
        "wallet_address"       : row["wallet_address"],
        "platform_variable_name": row["platform_variable_name"],
        "crypto_type"          : row["crypto_type"],
        "category"             : row["category"],
        "fund_id"              : row["fund_id"],
        "group"                : row["group"],
        "subgroup"             : row["subgroup"],
    }

    wallet_metadata[addr] = meta
    wallets_by_group.setdefault(meta["category"], {})[addr] = meta   # <-- group index


# -------- helpers ----------------------------------------------------------

def get_wallets_by_group(group_name: str) -> dict:
    """Return {address: metadata} for a given group (empty dict if none)."""
    return wallets_by_group.get(group_name, {})


def print_metadata(d: dict):
    for addr, meta in d.items():
        print(f"'{addr}': {{")
        for k, v in meta.items():
            print(f"    '{k}': '{v}',")
        print("},")


# -------- usage examples ---------------------------------------------------

fund_wallets = get_wallets_by_group("fund")
fund_wallets

fund_wallets

print(wallet_ID_mapping_file_df['category'].unique())
print()

print(wallet_ID_mapping_file_df['group'].unique())
print()
print(wallet_ID_mapping_file_df['subgroup'].unique())
print()
print(wallet_ID_mapping_file_df['fund_id'].unique())

# Slice into categories, using wallet_address instead of platform_variable_name
fund_wallet_ids      = [m["wallet_address"] for m in wallet_metadata.values() if m["category"] == "fund"]
gp_wallet_ids        = [m["wallet_address"] for m in wallet_metadata.values() if m["category"] == "gp"]
lp_wallet_ids        = [m["wallet_address"] for m in wallet_metadata.values() if m["category"] == "lp"]
holdings_wallet_ids  = [m["wallet_address"] for m in wallet_metadata.values() if m["group"] == "holdings"]
fund_i_wallet_ids    = [m["wallet_address"] for m in wallet_metadata.values() if m["group"] == "fund_i"]
fund_ii_wallet_ids   = [m["wallet_address"] for m in wallet_metadata.values() if m["group"] == "fund_ii"]
holdings_class_B_wallet_ids = [m["wallet_address"] for m in wallet_metadata.values() if m["subgroup"] == "holdings_class_B"]
holdings_class_A_wallet_ids = [m["wallet_address"] for m in wallet_metadata.values() if m["subgroup"] == "holdings_class_A"]
fund_i_class_B_wallet_ids = [m["wallet_address"] for m in wallet_metadata.values() if m["subgroup"] == "fund_i_class_B"]
fund_i_class_A_wallet_ids = [m["wallet_address"] for m in wallet_metadata.values() if m["subgroup"] == "fund_i_class_A"]
fund_ii_class_B_wallet_ids = [m["wallet_address"] for m in wallet_metadata.values() if m["subgroup"] == "fund_ii_class_B"]

# DEBUG: print counts & contents
print(f"‣ fund_wallet_ids      ({len(fund_wallet_ids)}):", fund_wallet_ids)
print(f"‣ gp_wallet_ids        ({len(gp_wallet_ids)}):", gp_wallet_ids)
print(f"‣ lp_wallet_ids        ({len(lp_wallet_ids)}):", lp_wallet_ids)
print(f"‣ holdings_wallet_ids  ({len(holdings_wallet_ids)}):", holdings_wallet_ids)
print(f"‣ fund_i_wallet_ids  ({len(fund_i_wallet_ids)}):", fund_i_wallet_ids)
print(f"‣ fund_ii_wallet_ids  ({len(fund_ii_wallet_ids)}):", fund_ii_wallet_ids)
print(f"‣ holdings_class_B_wallet_ids  ({len(holdings_class_B_wallet_ids)}):", holdings_class_B_wallet_ids)
print(f"‣ holdings_class_A_wallet_ids  ({len(holdings_class_A_wallet_ids)}):", holdings_class_A_wallet_ids)
print(f"‣ fund_i_class_B_wallet_ids  ({len(fund_i_class_B_wallet_ids)}):", fund_i_class_B_wallet_ids)
print(f"‣ fund_i_class_A_wallet_ids  ({len(fund_i_class_A_wallet_ids)}):", fund_i_class_A_wallet_ids)
print(f"‣ fund_ii_class_B_wallet_ids  ({len(fund_ii_class_B_wallet_ids)}):", fund_ii_class_B_wallet_ids)

# ─── Fetch list (omit LPs at first) ─────────────────────────────────
raw_wallets: List[str] = fund_wallet_ids + gp_wallet_ids

print(f"Will fetch txs for {len(raw_wallets)} wallets:", raw_wallets)

# ─── Build name maps ───────────────────────────────────────────────
# name_map for from/to mapping (excludes LPs)

name_map = {
    m["wallet_address"]: m["friendly_name"]
    for m in wallet_metadata.values()
}

# Check the map to verify
print(name_map)
# wallet_friendly_names if you need to label just your fetched set (no LPs at first)
wallet_friendly_names = {
    m["wallet_address"]: m["friendly_name"]
    for m in wallet_metadata.values() if m["wallet_address"] in raw_wallets
}

# ─── 5) Other derived maps ─────────────────────────────────────
wallet_crypto_types = {
    m["wallet_address"]: m["crypto_type"]
    for m in wallet_metadata.values()
}

decimal_map = {
    "ETH": 18, "WETH": 18, "USDC": 6, "USDT": 6,
    "WSTETH": 18, "BLUR POOL": 18,
}

# Generate the dictionary of wallet variable names with their corresponding wallet addresses using "=" for the output format
wallet_variable_address_mapping_eq = [
    f"{wallet_metadata[addr]['platform_variable_name']} = {wallet_metadata[addr]['wallet_address']}"
    for addr in wallet_metadata
]

# Join the results into a clean format
formatted_output = "\n".join(wallet_variable_address_mapping_eq)

# Display the result in a clean format
print(formatted_output)

gp_wallet_ids_checksum = {
    Web3.to_checksum_address(addr.lower()) for addr in gp_wallet_ids
}
fund_wallet_ids_checksum = {
    Web3.to_checksum_address(addr.lower()) for addr in fund_wallet_ids
}

fund_i_wallet_ids



#Possibly access this database for look ups
#https://www.4byte.directory/

"""##Map ERC-20 tokens"""

# ERC-20 ABI for symbol() only
ERC20_METADATA_ABI = [
    {
        "constant": True,
        "inputs": [],
        "name": "symbol",
        "outputs": [{"name": "", "type": "string"}],
        "type": "function",
    }
]

# Map of known irregular symbols to canonical tickers
SYMBOL_MAP: Dict[str, str] = {
    # ─── ETH variants ───────────────────────────────────────────────────────
    'ETH35.COM':     'ETH',
    'ETHG':          'ETH',
    'ETHF':          'ETH',
    'ETH':           'ETH',
    'ЕТН':           'ETH',   # Cyrillic Н
    'EТH':           'ETH',   # Cyrillic Т
    'ЕTH':           'ETH',   # Cyrillic Е
    'Ͼ':             'ETH',   # special char

    # ─── USDC variants ──────────────────────────────────────────────────────
    'CIRCLEUSD':     'USDC',
    'CIRCLE':        'USDC',
    'USD0':          'USDC',
    'USDC':          'USDC',
    'USDС':          'USDC',  # Cyrillic С
    'UЅDС':          'USDC',  # Cyrillic Ѕ,С

    # ─── USDT ────────────────────────────────────────────────────────────────
    'USDT':          'USDT',

    # ─── WETH variants ──────────────────────────────────────────────────────
    'WETH':          'WETH',
    'WЕТН':          'WETH',  # Cyrillic Т,Н
    'WEТH':          'WETH',  # Cyrillic Т
    'WWETH':         'WETH',
    'AETHWETH':      'WETH',
    'MWETHPPG5':     'WETH',

    # ─── staked ETH ─────────────────────────────────────────────────────────
    'WSTETH':        'WSTETH',

    # ─── BLUR variants ──────────────────────────────────────────────────────
    'BLUR':          'BLUR',
    'BLURPOOL':      'BLUR',
    'ВLUR':          'BLUR',  # Cyrillic В

    # ─── Unknown variants ───────────────────────────────────────────────────
    'ERC20':         'Unknown (ERC20)',
    'AIRDROPS':      'Unknown (AIRDROPS)',
    'NCELIGIBLE':    'Unknown (NCELIGIBLE)',
    '':              'Unknown',

    # ─── Additional Mappings ────────────────────────────────────────────────
    'AIC':           'Unknown',  # No known match, map to Unknown
    'ARCD':          'Unknown',  # No known match, map to Unknown
    'Airdrop(s) to be claimed at https://getdrops.org': 'Unknown',  # Airdrop info
    'Circle USD':    'USDC',  # Likely referring to USD-based token, map to USDC
    'DAI':           'DAI',  # DAI is a stablecoin
    'ETHf':          'ETH',  # Mapping as ETH, similar to other ETH variants
    'FRAX':          'FRAX',  # Known stablecoin
    'GRG':           'Unknown',  # No direct match, map to Unknown
    'HQG':           'Unknown',  # No direct match, map to Unknown
    'MEME':          'Unknown',  # Likely referring to a meme coin, map to Unknown
    'NC-Eligible (Verify: https://nodecoin.claims)': 'Unknown',  # Airdrop info
    'OETH':          'ETH',  # Mapping to ETH since it's staked ETH
    'PUMP':          'Unknown',  # No direct match, map to Unknown
    'SPX':           'Unknown',  # No direct match, map to Unknown
    'The Next Ethereum': 'ETH',  # Possible reference to ETH
    'USD0 [www.usual.finance]': 'USDC',  # Likely referring to USD token
    'USDC':          'USDC',  # Already mapped
    'USDT':          'USDT',  # Already mapped
    'USDС':          'USDC',  # Cyrillic С
    'UЅDC':          'USDC',  # Cyrillic Ѕ,С
    'Visit fraxnetwork.com to claim rewards': 'FRAX',  # Frax network token
    'WETH':          'WETH',  # Already mapped
    'WEТH':          'WETH',  # Cyrillic T
    'WЕТН':          'WETH',  # Cyrillic H
    'aEthWETH':      'WETH',  # Mapping to WETH
    'eETH':          'ETH',  # Mapping to ETH
    'mWETH-PPG:5':   'WETH',  # Mapping to WETH
    'mWETH-WPUNKS:40': 'WETH',  # Mapping to WETH
    'mjLP-GENERAL-WETH': 'WETH',  # Mapping to WETH
    'mpDAO':         'Unknown',  # No direct match, map to Unknown
    'msLP-GENERAL-WETH': 'WETH',  # Mapping to WETH
    'pUSDC':         'USDC',  # Mapping to USDC
    'pWETH':         'WETH',  # Mapping to WETH
    'pzETH':         'ETH',  # Mapping to ETH
    'rsETH':         'WSTETH',  # Mapping to staked ETH
    'rstETH':        'WSTETH',  # Mapping to staked ETH
    's1':            'Unknown',  # No direct match, map to Unknown
    'sFRAX':         'FRAX',  # Mapping to FRAX
    'sMEME':         'Unknown',  # No direct match, map to Unknown
    'sbWETH':        'WETH',  # Mapping to WETH
    'stETH':         'WSTETH',  # Mapping to staked ETH
    'variableDebtEthUSDC': 'USDC',  # Mapping to USDC
    'variableDebtEthUSDT': 'USDT',  # Mapping to USDT
    'wWETH':         'WETH',  # Mapping to WETH
    'weETHk':        'ETH',  # Mapping to ETH
    'weETHs':        'ETH',  # Mapping to ETH
    'wstETH':        'WSTETH',  # Mapping to staked ETH
    'Ͼ':             'ETH',  # Special char mapping to ETH
    'Вlur Роol':     'BLUR',  # Cyrillic B, mapping to BLUR
    'ЕRC20':         'Unknown (ERC20)',  # Mapping to Unknown
    'ЕRС20':         'Unknown (ERC20)',  # Cyrillic E, mapping to Unknown
    'ЕTH':           'ETH',  # Cyrillic E, mapping to ETH
    'ЕТН':           'ETH',  # Cyrillic H, mapping to ETH
    '$ ETH35.com - Visit to claim bonus rewards:': 'Unknown',
    'AIC$ ETH35.com - Visit to claim bonus rewards': 'Unknown',
}

# ─── Normalize Symbol Function ────────────────────────────────────────────────
def normalize_symbol(sym: str) -> str:
    """Clean raw symbol to canonical ticker, never returning blank."""
    if not isinstance(sym, str) or not sym.strip():
        return "Unknown"

    # Remove unwanted phrases and URLs
    sym_cleaned = re.sub(r"(?:\$|ETH35\.com|Visit to claim bonus rewards).*", "", sym)

    # Normalize the symbol by removing special characters and ensuring it is in a consistent format
    # --- CHANGE --- Check if the cleaned symbol is empty before attempting to split
    if not sym_cleaned.strip():
        return "Unknown"

    # Use try-except block to handle potential IndexError
    try:
        key = re.sub(r"[^\w]", "", unicodedata.normalize("NFKC", sym_cleaned).split()[0]).upper()
    except IndexError:
        # If there is an IndexError (i.e., split() results in an empty list), return "Unknown"
        return "Unknown"

    # Check if the normalized symbol is in the SYMBOL_MAP, and return the canonical version if it exists
    mapped = SYMBOL_MAP.get(key, f"Unknown ({key})")

    # If mapping gave you empty or whitespace, force "Unknown"
    return mapped if mapped and mapped.strip() else "Unknown"

"""## grab block number from date if you wish"""

BLOCK_CACHE_PATH    = "/content/block_range_cache.json"  # disk cache location (change if you like)
DEFAULT_CHAINID     = 1   # 1 = Ethereum mainnet
CHAIN_ID = 1
# ------------------------------------------------------------------
# 1)  Low-level call: timestamp → block number   (memoized)
# ------------------------------------------------------------------
@lru_cache(maxsize=None)
def block_number_by_time(
    ts: pd.Timestamp,
    closest: str = "before",
    chainid: int = DEFAULT_CHAINID,
) -> int:
    """
    Query Etherscan V2 for the block number nearest `ts`.
    Uses in-memory LRU cache + basic retry on rate-limit.
    """
    url = (
        f"{BASE_URL}"
        f"?chainid={chainid}"
        f"&module=block"
        f"&action=getblocknobytime"
        f"&timestamp={int(ts.timestamp())}"
        f"&closest={closest}"
        f"&apikey={ETHERSCAN_API_KEY}"
    )

    while True:
        js = requests.get(url, timeout=10).json()
        if js.get("status") == "1":
            return int(js["result"])
        if "rate limit" in str(js.get("result", "")).lower():
            time.sleep(0.6)  # back-off and retry
            continue
        raise ValueError(f"Etherscan error: {js}")

# ------------------------------------------------------------------
# 2)  High-level helper: (start, end) → (start_block, end_block)
#     • Persists to JSON so each range is fetched only once.
# ------------------------------------------------------------------
def get_block_range_for_period(
    start: pd.Timestamp,
    end: pd.Timestamp,
    chainid: int = DEFAULT_CHAINID,
    cache_path: str = BLOCK_CACHE_PATH,
) -> Tuple[int, int]:
    """
    Return (start_block, end_block) for the given date window.
    Cached on disk so later runs are instant.
    """
    # canonical cache key  e.g. "20241101_20241201_1"
    key = f"{start.strftime('%Y%m%d')}_{end.strftime('%Y%m%d')}_{chainid}"

    # load cache if present
    cache = {}
    if os.path.exists(cache_path):
        with open(cache_path, "r") as f:
            cache = json.load(f)

    # compute if missing
    if key not in cache:
        blk_start = block_number_by_time(start, "before", chainid)
        time.sleep(0.6)  # guard against burst
        blk_end   = block_number_by_time(end,   "before", chainid)
        cache[key] = {"start": blk_start, "end": blk_end}
        with open(cache_path, "w") as f:
            json.dump(cache, f, indent=2)

    rng = cache[key]
    return rng["start"], rng["end"]

# ------------------------------------------------------------------
# 3)  EXAMPLE  – produce constants you can pass to your fetchers
# ------------------------------------------------------------------

BLOCK_START, BLOCK_END = get_block_range_for_period(start, end, chainid=1)

print("Block range:", BLOCK_START, BLOCK_END)

"""# insane pull"""

import pandas as pd
from web3 import Web3
from web3.exceptions import BadFunctionCallOutput, ContractLogicError
from functools import lru_cache
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm

df = pd.read_csv(
    f'/content/drive/MyDrive/Drip_Capital/accounting_records/20241231/20241231_CLEAN_all_wallets_with_contract_names.csv',
    parse_dates=['transaction_datetime']
)
df = df[df["hash"] == "0xb4d18ef8d7f5b7aab2ffeadc91bb53bf7bf41d5e396598eeb51d0397b97b9bd1"].copy()

df

import pandas as pd
from web3 import Web3
from web3.exceptions import BadFunctionCallOutput, ContractLogicError
from functools import lru_cache
from concurrent.futures import ThreadPoolExecutor, as_completed

# === Setup ===
INFURA_URL = "https://mainnet.infura.io/v3/16f12641c1db46beb60e95cf4c88cbe1"
w3 = Web3(Web3.HTTPProvider(INFURA_URL))

# === Load hashes ===
df = pd.read_csv(
    '/content/drive/MyDrive/Drip_Capital/accounting_records/20241231/20241231_CLEAN_all_wallets_with_contract_names.csv',
    parse_dates=['transaction_datetime']
)

# Optional sanity check
#target_hash = "0x1e1c0abdf00e5829f5ae2113c87dce40bba6437557772f55b6000b44cbb44202"
#df = df[df["hash"].str.lower() == target_hash.lower()]

hashes = df["hash"].dropna().unique()
# === Helpers ===
def extract_eth_transfer(tx_hash):
    try:
        tx = w3.eth.get_transaction(tx_hash)
        if tx["value"] > 0:
            return pd.DataFrame([{
                "tx_hash": tx_hash,
                "logIndex": None,
                "contract_address": None,
                "event_name": "ETH Transfer",
                "from": tx["from"],
                "to": tx["to"],
                "value_int": tx["value"],  # Match process_tx_receipt
                "tokenSymbol": "ETH",
                "tokenDecimal": 18,
                "date": pd.to_datetime(w3.eth.get_block(tx["blockNumber"]).timestamp, unit="s"),
                "function": None  # Leave blank for consistency
            }])
        return pd.DataFrame(columns=[
            "tx_hash", "logIndex", "contract_address", "event_name",
            "from", "to", "value_int", "tokenSymbol", "tokenDecimal",
            "date", "function"
        ])
    except Exception as e:
        print(f"❌ extract_eth_transfer failed for {tx_hash}: {e}")
        return pd.DataFrame(columns=[
            "tx_hash", "logIndex", "contract_address", "event_name",
            "from", "to", "value_int", "tokenSymbol", "tokenDecimal",
            "date", "function"
        ])


def clean_topic_to_address(topic_hex):
    try:
        if pd.isna(topic_hex) or topic_hex in ["None", "", "0x", "0X"]:
            return None
        return Web3.to_checksum_address("0x" + topic_hex[-40:])
    except Exception:
        return None

def clean_data_to_int(data_hex):
    try:
        if pd.isna(data_hex) or data_hex in ["None", "", "0x", "0X"]:
            return None
        val = int(data_hex, 16)
        return val
    except (ValueError, OverflowError, TypeError) as e:
        print(f"❌ clean_data_to_int failed — data: {data_hex} — error: {e}")
        return None

TOPIC0_HASH_MAP = {
    "0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef": "Transfer",
    "0xe1fffcc4923d04b559f4d29a8bfc6cda04eb5b0d3c460751c2402c5c5cc9109c": "Deposit",
    "0x7fcf532c15f0a6db0bd6d0e038bea71d30d808c7d98cb3bf7268a95bf5081b65": "Withdraw"
}

def detect_event_name(topic0):
    if isinstance(topic0, str):
        cleaned = topic0.lower()
        if not cleaned.startswith("0x"):
            cleaned = "0x" + cleaned
        return TOPIC0_HASH_MAP.get(cleaned, "Other")
    return "Other"

# ========================================== Log Parsing Function ==================================================================
def process_tx_receipt(tx_hash):
    try:
        # Get receipt and block
        receipt = w3.eth.get_transaction_receipt(tx_hash)
        block = w3.eth.get_block(receipt.blockNumber)
        tx_date = pd.to_datetime(block.timestamp, unit="s")

        # Get transaction to extract function selector
        tx = w3.eth.get_transaction(tx_hash)
        function_selector = tx.input[:10] if tx.input and tx.input != "0x" else None
    except Exception as e:
        print(f"❌ Error retrieving tx or receipt for {tx_hash}: {e}")
        return pd.DataFrame()

    decoded_rows = []
    for log in receipt.logs:
        try:
            row = {
                "tx_hash": tx_hash,
                "logIndex": log.get("logIndex", None),
                "contract_address": log.get("address", None),
                "topic0": log["topics"][0].hex() if len(log.get("topics", [])) > 0 else None,
                "topic1": log["topics"][1].hex() if len(log.get("topics", [])) > 1 else None,
                "topic2": log["topics"][2].hex() if len(log.get("topics", [])) > 2 else None,
                "data": log.get("data", ""),
                "date": tx_date,
                "function": function_selector
            }

            if isinstance(row["data"], bytes):
                row["data"] = row["data"].hex()
            decoded_rows.append(row)
        except Exception:
            continue

    if not decoded_rows:
        return pd.DataFrame()

    logs_df = pd.DataFrame(decoded_rows)

    logs_df["event_name"] = logs_df["topic0"].apply(detect_event_name)
    logs_df["from"] = logs_df["topic1"].apply(clean_topic_to_address)
    logs_df["to"] = logs_df["topic2"].apply(clean_topic_to_address)

    TRANSFER_LIKE_EVENTS = {"Transfer", "Withdraw", "Deposit"}
    logs_df["value_int"] = logs_df.apply(
        lambda row: clean_data_to_int(row["data"]) if row["event_name"] in TRANSFER_LIKE_EVENTS else None,
        axis=1
    ).astype("object")

    # Add ETH transfer if exists
    eth_df = extract_eth_transfer(tx_hash)
    if not eth_df.empty:
        eth_df["function"] = function_selector
        logs_df = pd.concat([logs_df, eth_df], ignore_index=True)

    return logs_df[[
        "tx_hash", "logIndex", "contract_address", "event_name",
        "from", "to", "value_int", "date", "function"
    ]]





# Process all logs into list first
log_dfs = []

with ThreadPoolExecutor(max_workers=100) as executor:
    futures = {executor.submit(process_tx_receipt, h): h for h in hashes}

    for i, future in enumerate(as_completed(futures)):
        tx_hash = futures[future]
        try:
            df_logs = future.result()
            if not df_logs.empty:
                print(f"✅ [{i+1}/{len(futures)}] tx: {tx_hash} → {len(df_logs)} logs")
                log_dfs.append(df_logs)
            else:
                print(f"⚠️ [{i+1}/{len(futures)}] tx: {tx_hash} → empty")
        except Exception as e:
            print(f"❌ [{i+1}/{len(futures)}] tx: {tx_hash} failed with error: {e}")


# Try concat
try:
    all_logs = pd.concat(log_dfs, ignore_index=True)
    print("✅ Concat successful! Shape:", all_logs.shape)
except Exception as e:
    print("💥 Concat failed:", e)
    all_logs = None

all_logs_checka = all_logs[all_logs["tx_hash"] == "0xa2469c2ea4ce6b37a12340e26887f282f394491b7845d5f0feb03814da08390a"]
all_logs_checka

all_logs.tail(1)

import requests

@lru_cache(maxsize=None)
def decode_function_selector(selector):
    if not selector or not isinstance(selector, str) or not selector.startswith("0x"):
        return selector
    try:
        response = requests.get(
            f"https://www.4byte.directory/api/v1/signatures/?hex_signature={selector}",
            timeout=5
        )
        results = response.json().get("results", [])
        return results[0]["text_signature"] if results else selector
    except Exception as e:
        print(f"❌ Failed to decode {selector}: {e}")
        return selector
@lru_cache(maxsize=None)
def normalize_function_selector(x):
    if isinstance(x, bytes):
        return Web3.to_hex(x)[:10]
    if isinstance(x, str) and x.startswith("0x"):
        return x[:10]  # keep only 4-byte selector
    return None

all_logs["function"] = all_logs["function"].apply(normalize_function_selector)
all_logs["function"] = all_logs["function"].apply(decode_function_selector)

all_logs.tail(1)

df_logs1 = all_logs[all_logs["value_int"].notna() & (all_logs["value_int"] != 0)].copy()
df_logs1.sample(20)

ADDRESS_SYMBOL_OVERRIDES = {
    "0x0000000000a39bb272e79075ade125fd351887ac": "BLUR POOL",
    "0xb131f4a55907b10d1f0a50d8ab8fa09ec342cd74": "MEME",
    "0xa0b86991c6218b36c1d19d4a2e9eb0ce3606eb48": "USDC",
    "0x72e95b8931767c79ba4eee721354d6e99a61d004": "VARIABLEDEBTETHUSDC",
    "0xe020b01b6fbd83066aa2e8ee0ccd1eb8d9cc70bf": "ARCD",
    "0x4d5f47fa6a74757f35c14fd3a6ef8e3c9bc514e8": "AETHWETH",
    "0xc02aaa39b223fe8d0a0e5c4f27ead9083c756cc2": "WETH"
}

# Normalize address
df_logs1["contract_address"] = df_logs1["contract_address"].str.lower()

# Set tokenSymbol
df_logs1["tokenSymbol"] = df_logs1.apply(
    lambda row: (
        "ETH" if pd.isna(row["contract_address"]) or row["contract_address"] == ""
        else ADDRESS_SYMBOL_OVERRIDES.get(row["contract_address"], "MYSTERY")
    ),
    axis=1
)

# Set tokenDecimal: USDC = 6, all others = 18
df_logs1["tokenDecimal"] = df_logs1["tokenSymbol"].apply(
    lambda x: 6 if x in ["USDC", "VARIABLEDEBTETHUSDC"] else 18
)

df_logs1.head(3)

df_hash_checker = df_logs1[df_logs1["tx_hash"] == "0xa2469c2ea4ce6b37a12340e26887f282f394491b7845d5f0feb03814da08390a"]
df_hash_checker

"""# eth checka"""

import pandas as pd
from concurrent.futures import ThreadPoolExecutor, as_completed

# === Load Data ===
df = pd.read_csv(
    '/content/drive/MyDrive/Drip_Capital/accounting_records/20241231/20241231_CLEAN_all_wallets_with_contract_names.csv',
    parse_dates=['transaction_datetime']
)
#df = df[df["hash"] == "0x1e1c0abdf00e5829f5ae2113c87dce40bba6437557772f55b6000b44cbb44202"]
hashes = df["hash"].dropna().unique()

# === Threaded Execution ===
eth_transfer_results = []

with ThreadPoolExecutor(max_workers=10) as executor:
    futures = {executor.submit(extract_eth_transfer, tx_hash): tx_hash for tx_hash in hashes}

    for i, future in enumerate(as_completed(futures), 1):
        tx_hash = futures[future]
        try:
            result_df = future.result()
            if not result_df.empty:
                print(f"✅ [{i}/{len(futures)}] ETH transfer found: {tx_hash}")
                eth_transfer_results.append(result_df)
            else:
                print(f"⚠️ [{i}/{len(futures)}] No ETH transfer: {tx_hash}")
        except Exception as e:
            print(f"❌ [{i}/{len(futures)}] Error on {tx_hash}: {e}")
# === Combine all results into a single DataFrame ===
if eth_transfer_results:
    eth_transfers_df = pd.concat(eth_transfer_results, ignore_index=True)
    print(f"✅ Final ETH transfer DataFrame created: {len(eth_transfers_df)} rows")
else:
    eth_transfers_df = pd.DataFrame()
    print("⚠️ No ETH transfers found, created empty DataFrame.")

eth_transfers_df.columns

df_logs1.columns

"""## checka in the checka"""

eth_checka_hash = eth_transfers_df[eth_transfers_df["tx_hash"] == "0x1e1c0abdf00e5829f5ae2113c87dce40bba6437557772f55b6000b44cbb44202"]
eth_checka_hash

logs_checka_hash = all_logs[all_logs["tx_hash"] == "0x8cc79b3506718f10a4cc230574483041b13b770b157069ad37010325f21e749e"]
logs_checka_hash

eth_transfers_merge_ready = eth_transfers_df.copy()

# Normalize address casing for consistency
eth_transfers_merge_ready["from"] = eth_transfers_merge_ready["from"].str.lower()
eth_transfers_merge_ready["to"] = eth_transfers_merge_ready["to"].str.lower()
df_logs1["from"] = df_logs1["from"].str.lower()
df_logs1["to"] = df_logs1["to"].str.lower()

# Define the subset of columns that identify true duplicates
dupe_key_cols = ["tx_hash", "from", "to", "value_int", "tokenSymbol", "date"]

# Create a copy of df_logs1 with only these columns for comparison
existing_keys = df_logs1[dupe_key_cols].drop_duplicates()

# Merge to flag ETH rows that already exist in df_logs1
eth_transfers_merge_ready["_is_duplicate"] = eth_transfers_merge_ready[dupe_key_cols].apply(
    lambda row: tuple(row) in set([tuple(x) for x in existing_keys.values]),
    axis=1
)

# Filter out duplicates
eth_transfers_df_deduped = eth_transfers_merge_ready[~eth_transfers_merge_ready["_is_duplicate"]].drop(columns="_is_duplicate")

# Reorder and match columns before merging
eth_transfers_df_deduped = eth_transfers_df_deduped[df_logs1.columns]

# Merge
df_ultimate = pd.concat([df_logs1, eth_transfers_df_deduped], ignore_index=True)

# ✅ Print result
print(f"✅ Deduped and merged: {len(eth_transfers_merge_ready)} ETH rows → {len(eth_transfers_df_deduped)} added.")
print(f"✅ Final df_ultimate shape: {df_ultimate.shape}")

df_ultimate = df_ultimate.rename(columns={
    "tx_hash": "hash",
    "logIndex": "logIndex",  # unchanged
    "contract_address": "contractAddress",
    "event_name": "event_name",  # unchanged
    "from_address": "from",
    "to_address": "to",
    "value_int": "value",
    "tokenSymbol": "tokenSymbol",  # unchanged
    "tokenDecimal": "tokenDecimal",  # unchanged
    "date": "transaction_datetime"
})
df_ultimate.columns

"""# Rules

## load data from blockchain
"""

# ─── 0) Read in & parse your timestamp column as UTC ───────────────────────
df = df_ultimate.copy()

# 1) Ensure your column is UTC-aware
if df['transaction_datetime'].dt.tz is None:
    df['transaction_datetime'] = df['transaction_datetime'].dt.tz_localize('UTC')
else:
    df['transaction_datetime'] = df['transaction_datetime'].dt.tz_convert('UTC')

# 2) Build UNIX-seconds column
df['timeStamp'] = (
    df['transaction_datetime'].astype('int64') // 10**9
).astype('Int64')

# 3) Normalize to midnight UTC for date col
df['date'] = df['transaction_datetime'].dt.normalize()

# 4) Sort and inspect
df = df.sort_values('timeStamp')
print(df[['transaction_datetime', 'timeStamp']].dtypes)
print(df[['transaction_datetime', 'date', 'timeStamp']].tail())

# ─── 5) Lowercase helpers (guard against NaN) ─────────────────────────────
df1 = df.assign(
    _from = df['from'].fillna('').str.lower(),
    _to   = df['to'  ].fillna('').str.lower()
)

# ─── 6) GP/fund filtering ─────────────────────────────────────────────────
fund_ids = set(w.lower() for w in fund_wallet_ids)
gp_ids   = set(w.lower() for w in gp_wallet_ids)

mask_from_gp_to_notfund = df1['_from'].isin(gp_ids) & ~df1['_to'].isin(fund_ids)
mask_to_gp_from_notfund = df1['_to'].isin(gp_ids) & ~df1['_from'].isin(fund_ids)
mask_gp_nonfund         = mask_from_gp_to_notfund | mask_to_gp_from_notfund

df2 = df1.loc[~mask_gp_nonfund].copy()

# ─── 7) Remove phishing contracts ─────────────────────────────────────────
df2 = df2[~df2['contractAddress'].fillna('').str.lower().isin(fake_phishing)]

# ─── 8) Apply start/end time filter (UTC-aware) ───────────────────────────
df_month_old = df2[(df2['transaction_datetime'] >= start) & (df2['transaction_datetime'] <= end)]

# ─── 9) Done ──────────────────────────────────────────────────────────────
df_month_old.shape

df_month1 = df_month_old.copy()

df_month1["function"].unique()

"""### hash checker"""

columns_to_keep = [
    "hash", "from", "to", "value", "tokenSymbol", "transaction_datetime", "event_name", "contractAddress", "function"
]

hash_checker = df_month1.loc[
    df_month1["hash"] == "0xc70a3262637f165461c55a79d27c12bc44d336545203863987be915826dbc4ae",
    columns_to_keep
].copy()

hash_checker

target_address = "0x3b2a51fec517bbc7feaf68acfdb068b57870713f"

mask = (
    df_month1["from"].str.lower() == target_address
) & (
    df_month1["to"].isna()
)

df_month1.loc[mask, ["hash", "from", "to", "value", "tokenSymbol","event_name", "transaction_datetime"]]

"""## Rule 0 - Only our transactions"""

# --- Rule 0: Drop transactions not involving known wallets ---
df_rule0 = df_month1.copy()

# Ensure all comparison addresses are lowercase
raw_wallets_set = {addr.lower() for addr in raw_wallets}

# Normalize 'from' and 'to' to lowercase
df_rule0["from"] = df_rule0["from"].str.lower()
df_rule0["to"] = df_rule0["to"].str.lower()

# Keep rows where either sender or receiver is in our wallet list
mask_rule0 = (
    df_rule0["from"].isin(raw_wallets_set) |
    df_rule0["to"].isin(raw_wallets_set)
)

# Apply the rule
df_rule0 = df_rule0[mask_rule0].copy()
df0_applied = df_rule0.copy()  # ✅ this now reflects the actual filtered DataFrame

# Optional shape print or inspection
df0_applied.shape
df_rule0_applied = df0_applied.copy()
print(f"✅ Rule 0 applied: {len(df_month1) - len(df_rule0_applied)} rows dropped, {len(df_rule0_applied)} rows retained involving known wallets.")

"""## Rule 1 - Wrap *WETH*"""

WETH_CONTRACT = "0xc02aaa39b223fe8d0a0e5c4f27ead9083c756cc2"

# Step 1: Copy base DataFrame
df_rule1_applied = df_rule0_applied.copy()

# Step 2: Drop ETH Transfers going to WETH contract
drop_mask = (
    (df_rule1_applied["event_name"] == "ETH Transfer") &
    (df_rule1_applied["to"].str.lower() == WETH_CONTRACT)
)
dropped_count = drop_mask.sum()
df_rule1_applied = df_rule1_applied[~drop_mask]

# Step 3: WETH deposit split logic
mask_rule1 = (
    (df_rule1_applied["event_name"].str.lower() == "deposit") &
    (df_rule1_applied["to"].isna()) &
    (df_rule1_applied["contractAddress"].str.lower() == WETH_CONTRACT)
)

rows_to_duplicate = df_rule1_applied[mask_rule1].copy()

eth_rows = rows_to_duplicate.copy()
eth_rows["tokenSymbol"] = "ETH"
eth_rows["to"] = eth_rows["contractAddress"]
eth_rows["from"] = rows_to_duplicate["from"]
eth_rows["contractAddress"] = None

weth_rows = rows_to_duplicate.copy()
weth_rows["tokenSymbol"] = "WETH"
weth_rows["from"] = weth_rows["contractAddress"]
weth_rows["to"] = rows_to_duplicate["from"]

df_rule1_applied = pd.concat([df_rule1_applied[~mask_rule1], eth_rows, weth_rows], ignore_index=True)

# ✅ Print results
print(f"🗑️ Dropped {dropped_count} ETH Transfer → WETH_CONTRACT rows.")
print(f"✅ Rule 1 applied: {len(rows_to_duplicate)} WETH deposit rows duplicated as ETH + WETH splits.")

df_month1_checker = df_rule1_applied[df_rule1_applied["hash"] == "0x91dcca6b512233dbf0fe79c3c1ca1321a5197e9a3c98fbc7d9b27f94cd6ddc1f"]
df_month1_checker

"""## Rule 2 - Unwrap *WETH*"""

WETH_CONTRACT = "0xc02aaa39b223fe8d0a0e5c4f27ead9083c756cc2"

df_rule2_applied = df_rule1_applied.copy()

mask_rule2 = (
    (df_rule2_applied["event_name"].str.lower() == "withdraw") &
    (df_rule2_applied["to"].isna()) &
    (df_rule2_applied["contractAddress"].str.lower() == WETH_CONTRACT)
)

rows_to_duplicate = df_rule2_applied[mask_rule2].copy()
num_rows = len(rows_to_duplicate)

eth_rows = rows_to_duplicate.copy()
eth_rows["tokenSymbol"] = "ETH"
eth_rows["from"] = eth_rows["contractAddress"]
eth_rows["to"] = rows_to_duplicate["from"]
eth_rows["contractAddress"] = None

weth_rows = rows_to_duplicate.copy()
weth_rows["tokenSymbol"] = "WETH"
weth_rows["to"] = weth_rows["contractAddress"]
weth_rows["from"] = rows_to_duplicate["from"]

df_rule2_applied = pd.concat([df_rule2_applied[~mask_rule2], eth_rows, weth_rows], ignore_index=True)

print(f"Rule 2 duplicated {num_rows} rows into {num_rows * 2} new rows.")

hash_checker = df_rule2_applied[df_rule2_applied["hash"] == "0xd1282673ee38b3d924516f0000aa96793d0863d3d9c77d02ac42c2349f0dddfd"]
hash_checker

"""## Rule 3 - Normalize BLUR POOL name"""

df_rule3_applied = df_rule2_applied.copy()

mask_rule3 = df_rule3_applied["tokenSymbol"].str.upper() == "BLUR"
num_replaced = mask_rule3.sum()

df_rule3_applied.loc[mask_rule3, "tokenSymbol"] = "BLUR POOL"
print(f"Rule 3 replaced {num_replaced} tokenSymbol values with 'BLUR POOL'.")

"""## Rule 4 - Remove phishy things"""

df_rule4_applied = df_rule3_applied.copy()

# Normalize address case
df_rule4_applied["contractAddress"] = df_rule4_applied["contractAddress"].str.lower()
raw_suspects = [addr.lower() for addr in raw_suspects]
fake_phishing = [addr.lower() for addr in fake_phishing]

# Count rows before
before_count = len(df_rule4_applied)

# Remove rows with suspicious contractAddress
df_rule4_applied = df_rule4_applied[
    ~df_rule4_applied["contractAddress"].isin(raw_suspects + fake_phishing)
]

# Count rows after
after_count = len(df_rule4_applied)
print(f"Rule 4 removed {before_count - after_count} suspicious rows.")

"""## Rule 5 - Purchased *BLUR POOL*"""

BLUR_POOL_CONTRACT = "0x0000000000a39bb272e79075ade125fd351887ac"
MINT_ADDRESS = "0x0000000000000000000000000000000000000000"

df_rule5_applied = df_rule4_applied.copy()

# Build mask
mask_rule5 = (
    (df_rule5_applied["from"].str.lower() == MINT_ADDRESS.lower()) &
    (df_rule5_applied["contractAddress"].str.lower() == BLUR_POOL_CONTRACT.lower())
)

# Get matching rows
rows_to_duplicate = df_rule5_applied[mask_rule5].copy()

# Create flipped ETH rows
eth_rows = rows_to_duplicate.copy()
eth_rows["tokenSymbol"] = "ETH"
eth_rows["from"] = rows_to_duplicate["to"]
eth_rows["to"] = rows_to_duplicate["from"]
eth_rows["contractAddress"] = None  # Native ETH

# Append
df_rule5_applied = pd.concat([df_rule5_applied, eth_rows], ignore_index=True)

# ✅ Print what happened
print(f"✅ Rule 5 applied: {len(rows_to_duplicate)} mint rows duplicated as ETH transfers.")

"""## Rule 6 - Sold *BLUR POOL*"""

BLUR_POOL_CONTRACT = "0x0000000000a39bb272e79075ade125fd351887ac"
BURN_ADDRESS = "0x0000000000000000000000000000000000000000"

df_rule6_applied = df_rule4_applied.copy()

# Build mask
mask_rule6 = (
    (df_rule6_applied["to"].str.lower() == BURN_ADDRESS.lower()) &
    (df_rule6_applied["contractAddress"].str.lower() == BLUR_POOL_CONTRACT.lower()) &
    (df_rule6_applied["function"] == "OwnerTransferV7b711143(uint256)")
)

# Get matching rows
rows_to_duplicate = df_rule6_applied[mask_rule6].copy()

# Create flipped ETH rows
eth_rows = rows_to_duplicate.copy()
eth_rows["tokenSymbol"] = "ETH"
eth_rows["from"] = rows_to_duplicate["to"]
eth_rows["to"] = rows_to_duplicate["from"]
eth_rows["contractAddress"] = None  # Native ETH

# Append
df_rule6_applied = pd.concat([df_rule6_applied, eth_rows], ignore_index=True)

# ✅ Print what happened
print(f"✅ Rule 6 applied: {len(rows_to_duplicate)} burn rows duplicated as ETH receipts.")

rows_to_duplicate["function"].unique()

df_for_fifo = df_rule6_applied.copy()
df_for_fifo.shape

hash_checkaa = df_for_fifo[df_for_fifo["hash"] == "0x84f25b9a9f6d1fef4363156c1fc269aaea46a2721a80aa166acff4c8e6d57432"]
hash_checkaa

"""# Add USD Prices"""

from typing import Union
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed

def fetch_all_eth_usd(tx_hashes: Union[str, list[str]], max_workers: int = 10) -> pd.DataFrame:
    # Convert single hash to list
    if isinstance(tx_hashes, str):
        tx_hashes = [tx_hashes]

    results = []

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {executor.submit(get_eth_usd_for_tx, h): h for h in tx_hashes}

        for future in tqdm(as_completed(futures), total=len(futures)):
            result = future.result()
            if result:
                tx_hash, price, dt = result
                results.append({
                    "hash": tx_hash,
                    "TX_ETH_USD_price": price,
                    "block_datetime": dt
                })

    return pd.DataFrame(results)

tx_hashes = df_for_fifo['hash'].dropna().unique().tolist()
df_with_prices = fetch_all_eth_usd(tx_hashes, max_workers=10)

tx_with_price = fetch_all_eth_usd("0x1bf0d0cc928978ac155e618f7f85e17ccf9fcb61a3c7071142d4e4c33da78854")
tx_with_price

df_with_prices

#merge df_with_prices with df_for_group on "hash"
df_for_fifo = df_for_fifo.merge(df_with_prices, left_on='hash', right_on='hash', how='left')

df_for_fifo.tail(5)

hash_checker7 = df_for_fifo[df_for_fifo["hash"] == "0xd28c8c2712e822c4c957d7094d0116734ba0ee3bdee61252a717a8ad247a3452"]
hash_checker7

"""# Save DF as CSV"""

df_for_fifo.to_csv(f'/content/drive/MyDrive/Drip_Capital/accounting_records/investments/cryptocurrency/dfs/{start}_to_{end}_df_for_fifo.csv', index=False)

hash_checker = df_for_fifo[df_for_fifo["hash"] =="0xf8c61469a549de1b69b2cd8b4c0b884825e1694db7dc8764130040529887f39d"]
hash_checker

"""# Concat DF to whatcha need it to be 🙌"""

df_new = pd.read_csv(f'/content/drive/MyDrive/Drip_Capital/accounting_records/investments/cryptocurrency/dfs/{start}_to_{end}_df_for_fifo.csv', parse_dates=['transaction_datetime'],)# let pandas parse it up front
df_old = pd.read_csv("/content/drive/MyDrive/Drip_Capital/accounting_records/investments/cryptocurrency/dfs/2024-07-01 00:00:00+00:00_to_2024-12-01 00:00:00+00:00_df_for_fifo_rounded_correctlyTEST.csv")

df_new.columns

df_old.columns

# ─── Align Columns Between df_old and df_new ──────────────────────────────

# Fill any missing columns in df_new that are present in df_old
for col in df_old.columns:
    if col not in df_new.columns:
        df_new[col] = pd.NA  # or use np.nan if you prefer

# Reorder df_new columns to match df_old
df_new = df_new[df_old.columns]

# Concatenate them
df_combined = pd.concat([df_old, df_new], ignore_index=True)

# Normalize datetime columns
date_cols = ['transaction_datetime', 'TX_transaction_datetime', 'EOD_transaction_datetime']
for col in date_cols:
    if col in df_combined.columns:
        df_combined[col] = pd.to_datetime(df_combined[col], errors='coerce')

df_combined["transaction_datetime"].tail(10)

print(df_new.shape)
print(df_old.shape)
print(df_combined.shape)

df_combined.columns

"""# Work with said CSV

## load and rip
"""

df_for_group1 = df_combined
# 1) Ensure your column is UTC-aware
if df_for_group1['transaction_datetime'].dt.tz is None:
    # was naive → localize to UTC
    df_for_group1['transaction_datetime'] = df_for_group1['transaction_datetime'].dt.tz_localize('UTC')
else:
    # already aware → convert (no-op if already UTC)
    df_for_group1['transaction_datetime'] = df_for_group1['transaction_datetime'].dt.tz_convert('UTC')

# 2) Build UNIX-seconds column
df_for_group1['timeStamp'] = (
    df_for_group1['transaction_datetime'].astype('int64') // 10**9
).astype('Int64')

# 3) Normalize to midnight UTC for your “date” column
df_for_group1['date'] = df_for_group1['transaction_datetime'].dt.normalize()  # still tz-aware

# 4) Sort and inspect
df_for_group1.sort_values('timeStamp', inplace=True)
print(df_for_group1[['transaction_datetime','timeStamp']].dtypes)
print(df_for_group1[['transaction_datetime','date','timeStamp']].tail())

# Use tokenSymbol instead of tokenName
df_for_group1['tokenName'] = df_for_group1['tokenSymbol']

if "blockNumber" not in df_for_group1.columns:
    df_for_group1["blockNumber"] = BLOCK_END

hashes_to_update = [
    "0x39a7bbae22e0a6e97ca074851fa274b0eec04f9ef349ca13c7c55d151886de45",
    "0xda7b6b7daac89eedb7d605fe697a1f553aaa5cd6551a9fab19e121eeff3cc109"
]

df_for_group1.loc[df_for_group1["hash"].isin(hashes_to_update), "tokenDecimal"] = None
df_for_group1["tokenDecimal"] = pd.to_numeric(df_for_group1["tokenDecimal"], errors="coerce")

print(df_for_group1.loc[df_for_group1["tokenDecimal"] == "Unknown", "hash"].unique())

"""## Decimal Work"""

dec_map = {
    'ETH': 18,
    'WETH': 18,
    'DAI': 18,
    'USDC': 6,
    'USDT': 6,
    # add any others…
}

# 1) ensure tokenSymbol is normalized
df_for_group1['tokenSymbol'] = (
    df_for_group1['tokenSymbol']
        .fillna('ETH')   # default missing → ETH
        .str.upper()
)

# 2) preserve any on-chain decimal, then try your map for missing
df_for_group1['tokenDecimal'] = df_for_group1['tokenDecimal'].astype(float)  # ensure float to allow NaN
# 3) final fallback: fill remaining NaNs with 18
df_for_group1['tokenDecimal'] = df_for_group1['tokenDecimal'].fillna(18)

# mask for rows where decimal wasn’t set on-chain
mask_missing = df_for_group1['tokenDecimal'].isna()

# fill from your map wherever you have an entry
df_for_group1.loc[mask_missing, 'tokenDecimal'] = (
    df_for_group1.loc[mask_missing, 'tokenSymbol']
         .map(dec_map)
)
df_for_group1

columns_to_keep = [
    "hash", "from", "to", "value",
    "tokenName", "tokenSymbol", "TOKEN_SYMBOL", "transaction_datetime", "tokenDecimal"
]

hash_checker4 = df_for_group1.loc[
    df_for_group1["hash"] == "0xc158502b173b162fa4acc720cb7d20122a64b2687fde95a55d5cbbdc973dbc61",
    columns_to_keep
].copy()

hash_checker4

from decimal import Decimal, InvalidOperation

df_for_group2 = df_for_group1.copy()

# Force value as Decimal safely, default to 0 if invalid
def to_decimal_safe(x):
    try:
        return Decimal(str(x))
    except (InvalidOperation, ValueError, TypeError):
        return Decimal(0)

df_for_group2['value'] = df_for_group2['value'].map(to_decimal_safe)

def scale_to_raw(v, decimals):
    try:
        d = Decimal(str(v))
        scale = Decimal(10) ** int(decimals)
        result = d * scale
        return result.quantize(Decimal('1'))  # Round to integer, no fractional part
    except (InvalidOperation, ValueError, TypeError):
        return Decimal(0)

mask_human = (df_for_group2['value'] < Decimal('1e6')) & (df_for_group2['value'] != 0)

df_for_group2.loc[mask_human, 'value'] = df_for_group2.loc[mask_human].apply(
    lambda row: scale_to_raw(row['value'], row['tokenDecimal']),
    axis=1
)

print(df_for_group2.loc[mask_human, 'value'].map(type).unique())

df_for_group2.columns

from decimal import Decimal
stablecoins = ["USDC", "USDT", "VARIABLEDEBTETHUSDC", "VARIABLEDEBTETHUSDT"]

# Ensure tokenSymbol is uppercase and fill any missing values first
#df_for_group1['tokenSymbol'] = df_for_group1['tokenSymbol'].fillna("ETH").str.upper()

df_for_group2['tx_price_usd'] = df_for_group2.apply(
    lambda row: (
        Decimal("1.00") if row['tokenSymbol'] in stablecoins
        else Decimal(row['TX_ETH_USD_price'])
    ).quantize(SCALE_USD, rounding=ROUND_HALF_EVEN),
    axis=1
)

df_for_group2['eod_price_usd'] = df_for_group2.apply(
    lambda row: (
        Decimal("1.00") if row['tokenSymbol'] in stablecoins
        else Decimal(row['EOD_ETH_USD_price'])
    ).quantize(SCALE_USD, rounding=ROUND_HALF_EVEN),
    axis=1
)

df_for_group2.drop(columns=['date',], inplace=True)
df_for_group2.rename(columns={'transaction_datetime': 'date',}, inplace=True)
df_for_group2

"""## Fund Wallet ID"""

fund_wallet_ids_lower = [w.lower() for w in fund_wallet_ids]
fund_ids = set(w.lower() for w in fund_wallet_ids_lower)

# 1) pick the fund wallet into its own column
mask_from = df_for_group2['from'].str.lower().isin(fund_ids)
mask_to   = df_for_group2['to'].str.lower().isin(fund_ids)

df_for_group2.loc[mask_from, 'wallet_address'] = df_for_group2.loc[mask_from, 'from']
df_for_group2.loc[mask_to,   'wallet_address'] = df_for_group2.loc[mask_to,   'to']

# 2) checksum it so it lines up with your metadata keys
df_for_group2['wallet_cs'] = (
    df_for_group2['wallet_address']
      .dropna()
      .str.lower()
      .map(Web3.to_checksum_address)
)

# 3) now map in fund_id and platform_variable_name
df_for_group2['friendly_name']            = df_for_group2['wallet_cs'].map(lambda a: wallet_metadata.get(a, {}).get('friendly_name'))
df_for_group2['fund_id']                = df_for_group2['wallet_cs'].map(lambda a: wallet_metadata.get(a, {}).get('fund_id'))
df_for_group2['platform_variable_name'] = df_for_group2['wallet_cs'].map(lambda a: wallet_metadata.get(a, {}).get('platform_variable_name'))


# (optional) drop the helper column if you like
df_for_group2.drop(columns='wallet_cs', inplace=True)
df_for_group2

hash_checker6 = df_for_group2[df_for_group2["hash"] == "0xf20302e87dc24b5df630fa91a334b941f3dc912e5658887a0c8e6798ff448743"]
hash_checker6

"""## Create FIFO buy/sell"""

df = df_for_group2.copy()
print(df.columns.tolist())

df['wallet_address'] = df['wallet_address'].str.lower()
df['from'] = df['from'].str.lower()
df['to']   = df['to'].str.lower()
fund_ids   = [f.lower() for f in fund_ids]

# 1. Force positive qty always
df['qty'] = df.apply(
    lambda r: (
        abs(Decimal(str(r['value']))) / (Decimal(10) ** int(r['tokenDecimal']))
    ).quantize(SCALE_CRYPTO),
    axis=1
)




# 3) identify internal vs external
internal_mask = (
    df['from'].str.lower().isin(fund_ids) &
    df['to'].str.lower().isin(fund_ids)
)
external_df = df[~internal_mask].copy()
internal_df = df[ internal_mask].copy()

# 4) sign & side for externals just as before
external_df['wallet_address'] = external_df['wallet_address'].str.lower()
external_df['qty'] = external_df.apply(
    lambda r: (
        r['qty'] if str(r['to']).lower() == str(r['wallet_address']).lower()
        else -r['qty']
    ).quantize(SCALE_CRYPTO),
    axis=1
)


external_df['side'] = external_df['qty'].apply(lambda q: 'buy' if q>0 else 'sell')

# 5) split internals into two legs

# 5a) sell-leg out of the `from` wallet
sell_leg = internal_df.copy()
sell_leg['wallet_address'] = sell_leg['from'].str.lower()
sell_leg['qty'] = -sell_leg['qty']
sell_leg['side']           = 'sell'

# 5b) buy-leg into the `to` wallet
buy_leg = internal_df.copy()
buy_leg['wallet_address'] = buy_leg['to'].str.lower()
buy_leg['qty']  =  buy_leg['qty']
buy_leg['side']           = 'buy'

# 6) concatenate and carry forward the asset label
result = (
    pd.concat([external_df, sell_leg, buy_leg], ignore_index=True)
      .assign(asset=lambda d: d['tokenSymbol'])
)

result.sort_values(by=['timeStamp', 'side', 'qty'], ascending=[True, True, False], inplace=True)

result.columns

# Rename columns
result.rename(columns={
    "transaction_datetime": "date",
    "TX_ETH_USD_price": "eth_usd_price"
}, inplace=True)

# Fill missing asset names with "ETH"
result["asset"] = result["asset"].fillna("ETH")

# Sort by date, side, and descending qty
result.sort_values(by=["date", "side", "qty"], ascending=[True, True, False], inplace=True)

# Final DataFrame
result.tail(5)

# Always map wallet_original fresh from wallet_address → wallet_metadata key
result['wallet_address'] = result['wallet_address'].str.lower()

result['wallet_cs'] = result['wallet_address'].map(
    lambda x: Web3.to_checksum_address(x) if pd.notna(x) and x else None
)

result['wallet_original'] = result['wallet_cs']

result['fund_id'] = result['wallet_cs'].map(lambda a: wallet_metadata.get(a, {}).get('fund_id'))
result['platform_variable_name'] = result['wallet_cs'].map(lambda a: wallet_metadata.get(a, {}).get('platform_variable_name'))

result.drop(columns=['wallet_cs'], inplace=True)

hash_checker3 = result[result["hash"] == "0x671f49679b562c4f1261c9ad89ad9f70d4bd9fd9a26af4dbecea1904383cf889"].copy()#Wrapped Ether
hash_checker3

result.columns

"""## Add manual entries"""

import pandas as pd
from decimal import Decimal

# Load CSV
true_up_df = pd.read_csv("/content/drive/MyDrive/Drip_Capital/accounting_records/investments/cryptocurrency/dfs/manual_entries_for_fifo.csv")

# Convert all numeric-looking columns to Decimal
for col in true_up_df.columns:
    if pd.api.types.is_numeric_dtype(true_up_df[col]):
        true_up_df[col] = true_up_df[col].apply(lambda x: Decimal(str(x)) if pd.notnull(x) else None)

# Append to result DataFrame
result = pd.concat([result, true_up_df], ignore_index=True)

# Confirm
print(f"✅ Appended {len(true_up_df)} true-up rows. New total: {len(result)} rows.")

print(result.columns[result.columns.duplicated()])
print(true_up_df.columns[true_up_df.columns.duplicated()])

"""## Test FIFO Ending Balances"""

# 4c) Build the exact DataFrame your FIFO engine expects
fifo_input = result[[
    'hash',
    'date',
    'wallet_address',
    'fund_id',
    'asset',
    'side',
    'qty',
    'eth_usd_price'
]].copy()
fifo_input.sort_values(by=['date', 'side', 'qty'], ascending=[True, True, False], inplace=True)
fifo_input = fifo_input.loc[:, ~fifo_input.columns.duplicated()].copy()

fifo_input.tail(5)

"""### checka"""

# Ensure full column values are shown
pd.set_option('display.max_colwidth', None)
from datetime import datetime
fifo_input['date'] = pd.to_datetime(fifo_input['date'])

# Define your date range
start_date = pd.to_datetime("2024-12-01 00:00+00:00")
end_date = pd.to_datetime("2024-12-31 23:59:59+00:00")

# Apply filters including the date range
weth_check = fifo_input[
    (fifo_input['asset'] == 'WETH') &
    (fifo_input['wallet_address'] == '0xef732b402abcf15df684e0e9c5795022a8696d9d') &
    (fifo_input['date'] >= start_date) &
    (fifo_input['date'] <= end_date)
    & (fifo_input["side"] == "sell")
]
qty_sum = weth_check.groupby("side")["qty"].sum()
#net = qty_sum.loc["buy"] + qty_sum.loc["sell"]
print(qty_sum)
#print(f"net: {net}")

weth_check.tail(40)

from google.colab import files
weth_check.to_csv("fuwah.csv", index=False)
#files.download("fuwah.csv")

hash_checker5 = fifo_input[fifo_input["hash"] == "0xddf9057ae7174ba5800e7d53fd5a2a68cf5af1fe0e5ff3584681791de4024645"].copy()#Wrapped Ether
hash_checker5

# Step 1: Normalize asset column
fifo_input["asset"] = fifo_input["asset"].str.upper()

# Step 2: Find hashes that have BLUR POOL
blur_hashes = set(fifo_input[fifo_input["asset"] == "BLUR POOL"]["hash"])

# Step 3: Find hashes that have ETH
eth_hashes = set(fifo_input[fifo_input["asset"] == "ETH"]["hash"])

# Step 4: Find hashes that have BOTH
shared_hashes = blur_hashes & eth_hashes

# Step 5: Filter original df
blur_eth_pairs = fifo_input[fifo_input["hash"].isin(shared_hashes)]

# Step 6: Optional - Sort for clarity
blur_eth_pairs = blur_eth_pairs.sort_values(["hash", "asset", "qty"], ascending=[True, True, False])

# ✅ Show result
print(f"✅ Found {len(shared_hashes)} hashes with both BLUR POOL and ETH entries.")

# Only rows where `qty != 0` and `hash` is duplicated
weth_check_dupes = weth_check[
    (weth_check["qty"] != 0) &
    (weth_check.duplicated(subset=["hash"], keep=False))
]
weth_check_dupes.head(30)

df['wallet_address'] = df['wallet_address'].str.lower()
df['from'] = df['from'].str.lower()
df['to']   = df['to'].str.lower()
fund_ids   = [f.lower() for f in fund_ids]

# 1. Force positive qty always
df['qty'] = df.apply(
    lambda r: (
        abs(Decimal(str(r['value']))) / (Decimal(10) ** int(r['tokenDecimal']))
    ).quantize(SCALE_CRYPTO),
    axis=1
)




# 3) identify internal vs external
internal_mask = (
    df['from'].str.lower().isin(fund_ids) &
    df['to'].str.lower().isin(fund_ids)
)
external_df = df[~internal_mask].copy()
internal_df = df[ internal_mask].copy()

# 4) sign & side for externals just as before
external_df['wallet_address'] = external_df['wallet_address'].str.lower()
external_df['qty'] = external_df.apply(
    lambda r: (
        r['qty'] if str(r['to']).lower() == str(r['wallet_address']).lower()
        else -r['qty']
    ).quantize(SCALE_CRYPTO),
    axis=1
)


external_df['side'] = external_df['qty'].apply(lambda q: 'buy' if q>0 else 'sell')

# 5a) sell-leg out of the `from` wallet
sell_leg = internal_df.copy()
sell_leg['wallet_address'] = sell_leg['from'].str.lower()
sell_leg['qty'] = sell_leg['qty'].apply(lambda x: (-abs(x)).quantize(SCALE_CRYPTO))
sell_leg['side'] = 'sell'

# 5b) buy-leg into the `to` wallet
buy_leg = internal_df.copy()
buy_leg['wallet_address'] = buy_leg['to'].str.lower()
buy_leg['qty'] = buy_leg['qty'].apply(lambda x: abs(x).quantize(SCALE_CRYPTO))
buy_leg['side'] = 'buy'


# 6) concatenate and carry forward the asset label
result = (
    pd.concat([external_df, sell_leg, buy_leg], ignore_index=True)
      .assign(asset=lambda d: d['tokenSymbol'])
).copy()

"""# grab onchain balance"""

import json, pandas as pd, requests
from web3 import Web3
from datetime import timezone

# === Setup ===
INFURA_URL = "https://mainnet.infura.io/v3/16f12641c1db46beb60e95cf4c88cbe1"
w3 = Web3(Web3.HTTPProvider(INFURA_URL))
assert w3.is_connected() and w3.eth.chain_id == 1

ETHERSCAN_API_KEY = "P13CVTCP43NWU9GX5D9VBA2QMUTJDDS941"

def get_token_balance_etherscan(address, token_address, block, api_key):
    url = "https://api.etherscan.io/api"
    params = {
        "module": "account",
        "action": "tokenbalance",
        "contractaddress": token_address,
        "address": address,
        "tag": block,
        "apikey": api_key
    }
    response = requests.get(url, params=params).json()
    if response.get("status") == "1":
        return int(response["result"])
    return 0

# === Get target block at month end ===
target_dt = (end - pd.Timedelta(days=1)).replace(hour=23, minute=59, second=59)
ts = int(target_dt.timestamp())
res = requests.get(
    "https://api.etherscan.io/api",
    params={
        "module":    "block",
        "action":    "getblocknobytime",
        "timestamp": ts,
        "closest":   "before",
        "apikey":    ETHERSCAN_API_KEY
    }
)
blk = int(res.json()["result"])
print(f"Querying block {blk} ({target_dt.isoformat()})")

# === Tokens ===
TOKENS = [
    dict(symbol="ETH",       address=None,                                                            decimals=18),
    dict(symbol="WETH",      address="0xC02aaA39b223FE8D0A0e5C4F27eAD9083C756Cc2",                     decimals=18),
    dict(symbol="BLUR POOL",      address="0x0000000000A39bb272e79075ade125fd351887Ac",                     decimals=18),
    dict(symbol="USDC",      address="0xA0b86991c6218b36c1d19D4a2e9Eb0cE3606eB48",                     decimals=6 ),
    dict(symbol="AETHWETH",  address="0x4d5F47FA6A74757f35C14fD3a6Ef8E3C9BC514E8",                     decimals=18),
    dict(symbol="ARCD",      address="0x7De71BC6694ca827e043b18102CAf01518C0b54D",                     decimals=18),
]

ERC20_ABI = json.loads("""[
  { "constant":true, "inputs":[{"name":"_owner","type":"address"}],
    "name":"balanceOf", "outputs":[{"name":"balance","type":"uint256"}],
    "type":"function" }
]""")

wallets = [Web3.to_checksum_address(w) for w in fund_wallet_ids]
records = []

for w in wallets:
    # ETH
    wei = w3.eth.get_balance(w, block_identifier=blk)
    eth = float(w3.from_wei(wei, "ether"))
    records.append({
        "wallet_address": w.lower(),
        "asset": "ETH",
        "onchain": round(eth, 6)
    })

    # ERC-20s
    for t in TOKENS[1:]:
        symbol = t["symbol"]
        try:
            contract = w3.eth.contract(address=Web3.to_checksum_address(t["address"]), abi=ERC20_ABI)
            raw = contract.functions.balanceOf(w).call(block_identifier=blk)
        except Exception:
            print(f"⚠️ balanceOf() failed for {symbol}, falling back to Etherscan…")
            raw = get_token_balance_etherscan(w, t["address"], hex(blk), ETHERSCAN_API_KEY)

        try:
            bal = raw / 10**t["decimals"]
        except Exception:
            bal = 0

        records.append({
            "wallet_address": w.lower(),
            "asset": symbol,
            "onchain": round(bal, 6)
        })

# === DataFrame output ===
onchain_df = pd.DataFrame(records)
onchain_df = onchain_df.query("onchain != 0").reset_index(drop=True)

export_path = f"/content/drive/MyDrive/Drip_Capital/accounting_records/investments/cryptocurrency/generated/onchain_balances_{end}.csv"
# onchain_df.to_csv(export_path, index=False)

onchain_df

"""# target wallet"""

# ------------------------------------------------------------------
# Compare on-chain token balances with FIFO-derived balances
# ------------------------------------------------------------------

# 1. Filter FIFO to just fund_wallet_ids
df_target = fifo_input[
    fifo_input["wallet_address"].str.lower().isin([w.lower() for w in fund_wallet_ids])
]

# 2. Compute ending balances from FIFO
ending_balances = (
    df_target
      .groupby(["wallet_address", "asset"])["qty"]
      .sum()
      .reset_index()
      .rename(columns={"qty": "ending_balance"})
)

ending_balances["wallet_address"] = ending_balances["wallet_address"].str.lower()
ending_balances["ending_balance"] = ending_balances["ending_balance"].astype(float).round(6)

# 3. Merge with onchain balances
combined = pd.merge(
    ending_balances,
    onchain_df,
    on=["wallet_address", "asset"],
    how="outer"
).fillna(0)

# 4. Compute difference
combined["difference"] = (combined["onchain"] - combined["ending_balance"]).round(6)

# 5. Optional: filter to rows with any nonzero value
#combined = combined[
#    (combined["ending_balance"] != 0) |
#    (combined["onchain"] != 0)
#].reset_index(drop=True)

# 6. Display result
combined

print(fifo_input["eth_usd_price"].unique())

combined.to_csv("/content/drive/MyDrive/Drip_Capital/accounting_records/investments/cryptocurrency/generated/fifo_ending_balances.csv", index=False)

"""# FIFO GENERATOR"""

import os
import pandas as pd
from collections import deque
from decimal import Decimal, InvalidOperation, DivisionByZero
ts = datetime.now().strftime("%H%M%S")
# ────────────────────────────────────────────────────────────────────
# CONFIGURATION
# ────────────────────────────────────────────────────────────────────
MASTER_PATH_ETH = f"/content/drive/MyDrive/Drip_Capital/accounting_records/investments/cryptocurrency/generated/master_fifo_ledger_eth_{ts}.csv"
MASTER_PATH_USD = f"/content/drive/MyDrive/Drip_Capital/accounting_records/investments/cryptocurrency/generated/master_fifo_ledger_usd_{ts}.csv"

STABLECOINS = {"USDC", "USDT", "DAI"}
ETH_one_for_one    = {"ETH", "BLUR POOL", "BLUR", "WETH", "AETHWETH"}
WSTETH       = {"WSTETH"}
WSTETH_RATE_DEFAULT  = Decimal("1.18045433553113")
MEME_COIN    = {"MEME"}
MEME_RATE    = Decimal("0.0000006")


# ────────────────────────────────────────────────────────────────────
# FIFO LOT TRACKING CLASS
# ────────────────────────────────────────────────────────────────────
class FIFOTracker:
    def __init__(self):
        self.lots = {}
        self.logs = []

    def process(self, fund_id, wallet, asset, side, qty, price_usd, date, tx_hash, log=False, eth_usd_price=None):
        try:
            q = Decimal(str(qty))
            p = Decimal(str(price_usd))
        except (InvalidOperation, TypeError):
            return

        key = (fund_id, wallet, asset)
        dq = self.lots.setdefault(key, deque())

        proceeds = cost_basis = gain = Decimal("0")

        if side.lower() == "buy":
            dq.append([q, p])
        else:
            to_sell = abs(q)
            while to_sell > 0 and dq:
                lq, lp = dq[0]
                take = min(lq, to_sell)
                proceeds   += take * p
                cost_basis += take * lp
                gain       += take * (p - lp)
                lq -= take
                to_sell -= take
                if lq > 0:
                    dq[0][0] = lq
                else:
                    dq.popleft()

            if to_sell > 0:
                proceeds   += to_sell * p
                cost_basis += to_sell * p
                dq.appendleft([-to_sell, p])

        remaining_qty  = sum(l for l, _ in dq)
        remaining_cost = sum(l * pr for l, pr in dq)

        if log:
            self.logs.append({
                "fund_id": fund_id,
                "wallet_address": wallet,
                "asset": asset,
                "date": date,
                "hash": tx_hash,
                "side": side,
                "qty": q,
                "price_eth": eth_usd_price,
                "proceeds_usd": proceeds,
                "cost_basis_sold_usd": cost_basis,
                "realized_gain_usd": gain,
                "remaining_qty": remaining_qty,
                "remaining_cost_basis_usd": remaining_cost,
            })

    def to_dataframe(self):
        return pd.DataFrame(self.logs)

# ────────────────────────────────────────────────────────────────────
# WSTETH ↔ ETH rate per hash (carry-forward only)
# ────────────────────────────────────────────────────────────────────
def build_wsteth_rate_map(df):
    df = df.copy()
    df["asset_upper"] = df["asset"].str.upper()
    df["date"]        = pd.to_datetime(df["date"])

    # keep only rows that mention ETH or WSTETH
    conv = df[df["asset_upper"].isin({"ETH", "WSTETH"})]

    # visit hashes in chronological order (earliest timestamp in hash)
    hash_order = (
        conv.groupby("hash")["date"]
            .min()
            .sort_values()
            .index
    )

    rate_map  = {}
    last_rate = WSTETH_RATE_DEFAULT       # fallback for very first hashes

    for h in hash_order:
        grp     = conv[conv["hash"] == h]
        assets  = set(grp["asset_upper"])

        # if both sides present, update the running rate
        if {"ETH", "WSTETH"} <= assets:
            eth_qty = grp.loc[grp["asset_upper"] == "ETH",     "qty"] \
                        .apply(lambda x: abs(Decimal(str(x)))).sum()
            wst_qty = grp.loc[grp["asset_upper"] == "WSTETH", "qty"] \
                        .apply(lambda x: abs(Decimal(str(x)))).sum()
            if wst_qty:
                last_rate = eth_qty / wst_qty

        rate_map[h] = last_rate

    return rate_map

# ────────────────────────────────────────────────────────────────────
# UNIT PRICE COMPUTATION HELPERS
# ────────────────────────────────────────────────────────────────────

def compute_unit_price_eth(row):
    try:
        asset = row["asset"].upper()
        eth_usd_price = Decimal(row["eth_usd_price"])

        if asset in ETH_one_for_one:
            return Decimal("1")

        if asset in STABLECOINS:
            return Decimal("1") / eth_usd_price

        if asset in WSTETH:
            rate = row.get("wst_rate")
            rate = Decimal(str(rate)) if rate not in (None, "") else WSTETH_RATE_DEFAULT
            return rate

        if asset in MEME_COIN:
            return MEME_RATE * eth_usd_price

    except (InvalidOperation, TypeError, DivisionByZero):
        pass

    return Decimal("0")

def compute_unit_price_usd(row):
    try:
        asset = row["asset"].upper()
        eth_usd_price = Decimal(row["eth_usd_price"])

        if asset in STABLECOINS:
            return Decimal("1")

        if asset in WSTETH:
            rate = row.get("wst_rate")
            rate = Decimal(str(rate)) if rate not in (None, "") else WSTETH_RATE_DEFAULT
            return rate * eth_usd_price

        return eth_usd_price
    except (InvalidOperation, TypeError):
        return Decimal("0")

# ────────────────────────────────────────────────────────────────────
# LEDGER BUILDER
# ────────────────────────────────────────────────────────────────────

def build_fifo_ledger(df_input, price_column, output_suffix):
    tracker = FIFOTracker()
    for r in df_input.itertuples(index=False):
        tracker.process(
            fund_id     = r.fund_id,
            wallet      = r.wallet_address,
            asset       = r.asset,
            side        = r.side,
            price_usd   = getattr(r, price_column),
            qty         = r.qty,
            date        = r.date,
            tx_hash     = r.hash,
            log         = True,
            eth_usd_price = r.eth_usd_price,
        )

    df = tracker.to_dataframe()
    df = df[df["qty"] != 0].rename(columns={"eth_usd_price": "price_eth"})

    if output_suffix == "eth":
        df = df.rename(columns={
            "proceeds_usd": "proceeds_eth",
            "cost_basis_sold_usd": "cost_basis_sold_eth",
            "realized_gain_usd": "realized_gain_eth",
            "remaining_cost_basis_usd": "remaining_cost_basis_eth",
        })
    return df

# ────────────────────────────────────────────────────────────────────
# CSV OUTPUT
# ────────────────────────────────────────────────────────────────────

def save_master_ledger(path, df):
    df.to_csv(path, index=False)

def insert_after(df, col, after):
    cols = [c for c in df.columns if c != col]
    idx  = cols.index(after) + 1
    cols.insert(idx, col)
    return df[cols]

# ────────────────────────────────────────────────────────────────────
# MAIN SCRIPT
# ────────────────────────────────────────────────────────────────────

cols_to_keep = ["fund_id", "wallet_address", "asset", "date", "hash",
                "side", "qty", "eth_usd_price"]

for path in [MASTER_PATH_ETH, MASTER_PATH_USD]:
    if not os.path.exists(path):
        pd.DataFrame(columns=cols_to_keep).to_csv(path, index=False)

df_base = fifo_input[cols_to_keep].copy()

# Build dynamic WSTETH rate map and attach
wst_rate_map = build_wsteth_rate_map(df_base)
df_base["wst_rate"] = df_base["hash"].map(wst_rate_map)

# Timestamps & ordering
df_base["date"] = pd.to_datetime(df_base["date"])
df_base = df_base.sort_values("date")

# Unit prices (updated)
df_base["unit_price_eth"] = df_base.apply(compute_unit_price_eth, axis=1)
df_base["unit_price_usd"] = df_base.apply(compute_unit_price_usd, axis=1)

# ──────────────────────────────────────────────────────────────
# 1) Build ledgers
# ──────────────────────────────────────────────────────────────
df_ledger_eth = build_fifo_ledger(df_base, "unit_price_eth", "eth")
df_ledger_usd = build_fifo_ledger(df_base, "unit_price_usd", "usd")

# ──────────────────────────────────────────────────────────────
# 2) Bring unit_price_eth into ETH ledger + total_eth
# ──────────────────────────────────────────────────────────────
merge_cols = ["fund_id", "wallet_address", "asset", "date", "hash", "side"]

unit_price_patch = (
    df_base[merge_cols + ["unit_price_eth"]]
    .drop_duplicates(merge_cols)
)

df_ledger_eth = df_ledger_eth.merge(unit_price_patch, on=merge_cols, how="left")

# Fill any remaining unit_price_eth using price_eth for ETH-denominated tokens
df_ledger_eth["unit_price_eth"] = df_ledger_eth["unit_price_eth"].fillna(
    df_ledger_eth["price_eth"]
)

# Correct total_eth calculation
mask_one_for_one = df_ledger_eth["asset"].str.upper().isin(ETH_one_for_one)

# For ETH_one_for_one assets, total_eth = abs(qty)
df_ledger_eth["total_eth"] = Decimal("0")  # default init
df_ledger_eth.loc[mask_one_for_one, "total_eth"] = (
    df_ledger_eth.loc[mask_one_for_one, "qty"]
    .apply(lambda x: abs(Decimal(str(x))))
)

# For others, total_eth = abs(qty) * unit_price_eth
df_ledger_eth.loc[~mask_one_for_one, "total_eth"] = (
    df_ledger_eth.loc[~mask_one_for_one, "qty"]
    .apply(lambda x: abs(Decimal(str(x))))
    * df_ledger_eth.loc[~mask_one_for_one, "unit_price_eth"]
    .apply(lambda x: Decimal(str(x)))
)
# ──────────────────────────────────────────────────────────────
# 2b) Patch USD ledger with total_eth, total_usd
# ──────────────────────────────────────────────────────────────
# Add unit_price_usd
unit_price_usd_patch = (
    df_base[merge_cols + ["unit_price_usd"]]
    .drop_duplicates(merge_cols)
)
df_ledger_usd = df_ledger_usd.merge(unit_price_usd_patch, on=merge_cols, how="left")

# Add eth_usd_price
eth_price_patch = (
    df_base[merge_cols + ["eth_usd_price"]]
    .drop_duplicates(merge_cols)
)
df_ledger_usd = df_ledger_usd.merge(eth_price_patch, on=merge_cols, how="left")

# Add total_eth
total_eth_patch = (
    df_ledger_eth[merge_cols + ["total_eth"]]
    .drop_duplicates(merge_cols)
)
df_ledger_usd = df_ledger_usd.merge(total_eth_patch, on=merge_cols, how="left")

# total_usd = total_eth * eth_usd_price
df_ledger_usd["total_usd"] = (
    df_ledger_usd["total_eth"]
    .apply(lambda x: Decimal(str(x)))
    * df_ledger_usd["eth_usd_price"]
    .apply(lambda x: Decimal(str(x)))
)

# Order: qty → total_eth → eth_usd_price → total_usd → unit_price_usd
df_ledger_usd = insert_after(df_ledger_usd, "total_eth", "qty")
df_ledger_usd = insert_after(df_ledger_usd, "eth_usd_price", "total_eth")
df_ledger_usd = insert_after(df_ledger_usd, "total_usd", "eth_usd_price")
df_ledger_usd = insert_after(df_ledger_usd, "unit_price_usd", "total_usd")


# ──────────────────────────────────────────────────────────────
# 3) Patch one-for-one conversions (WETH ➜ MWETH-PPG, etc.)
# ──────────────────────────────────────────────────────────────
def patch_one_for_one_conversions(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    grp_cols = ["fund_id", "wallet_address", "hash", "date"]

    for _, g in df.groupby(grp_cols):
        if len(g) != 2:
            continue

        buy  = g[g["side"].str.lower() == "buy"]
        sell = g[g["side"].str.lower() == "sell"]
        if buy.empty or sell.empty:
            continue

        buy  = buy.iloc[0]
        sell = sell.iloc[0]

        # Skip if same asset (not a conversion)
        if buy["asset"].upper() == sell["asset"].upper():
            continue

        # Skip if the buy leg is ETH-one-for-one (preserve unit price = 1)
        if buy["asset"].upper() in ETH_one_for_one:
            continue

        # Skip if sell leg isn't ETH-based
        if sell["asset"].upper() not in ETH_one_for_one:
            continue

        idx = buy.name
        total_eth_used = abs(Decimal(str(sell["total_eth"])))
        qty_received   = abs(Decimal(str(buy["qty"])))
        new_unit_price = total_eth_used / qty_received

        df.loc[idx, "total_eth"]                = total_eth_used
        df.loc[idx, "unit_price_eth"]           = new_unit_price
        df.loc[idx, "remaining_cost_basis_eth"] = total_eth_used

    return df


# ──────────────────────────────────────────────────────────────
# 4) Set cost basis = qty for pure 1:1 ETH-based tokens
# ──────────────────────────────────────────────────────────────
mask_one_for_one = df_ledger_eth["asset"].str.upper().isin(ETH_one_for_one)
df_ledger_eth.loc[mask_one_for_one, "remaining_cost_basis_eth"] = (
    df_ledger_eth.loc[mask_one_for_one, "remaining_qty"]
)

# ──────────────────────────────────────────────────────────────
# 5) Reorder: qty → total_eth → price_eth → unit_price_eth
# ──────────────────────────────────────────────────────────────

df_ledger_eth = insert_after(df_ledger_eth, "total_eth", "qty")
df_ledger_eth = insert_after(df_ledger_eth, "unit_price_eth", "price_eth")

if "price_eth" in df_ledger_usd.columns:
    df_ledger_usd["eth_usd_price"] = df_ledger_usd["price_eth"]
    df_ledger_usd.drop(columns=["price_eth"], inplace=True)
    df_ledger_usd.rename(columns={"eth_usd_price": "price_eth"}, inplace=True)

# ──────────────────────────────────────────────────────────────
# 6) Save
# ──────────────────────────────────────────────────────────────
save_master_ledger(MASTER_PATH_ETH, df_ledger_eth)
save_master_ledger(MASTER_PATH_USD, df_ledger_usd)

print(f"✅ Built ETH ledger: {len(df_ledger_eth)} rows")
print(f"✅ Built USD ledger: {len(df_ledger_usd)} rows")

from google.colab import files
files.download(MASTER_PATH_ETH)
#files.download(MASTER_PATH_USD)

target_hash = "0x39a7bbae22e0a6e97ca074851fa274b0eec04f9ef349ca13c7c55d151886de45"

# Pull the relevant rows
df_tx = df_ledger_usd[df_ledger_usd["hash"] == target_hash].copy()
display(df_tx[["date", "asset", "side", "qty", "price_eth"]])

"""# Create JEs"""

#load fifo if you prefer a csv :)
df_ledger_usd = pd.read_csv("/content/drive/MyDrive/Drip_Capital/accounting_records/investments/cryptocurrency/master_fifo_ledger_usd.csv")

STABLECOINS = {"USDC", "USDT", "DAI"}
ETH_one_for_one    = {"ETH", "BLUR POOL", "BLUR", "WETH", "AETHWETH"}
WSTETH       = {"WSTETH"}
WSTETH_RATE_DEFAULT  = Decimal("1.18045433553113")
MEME_COIN    = {"MEME"}
MEME_RATE    = Decimal("0.0000006")
USE_TOTAL_ETH_NOT_QTY = {"MEME"}
from decimal import Decimal, InvalidOperation
import pandas as pd
import os
df_running = df_ledger_usd
# ─────────────────────────────────────────────────────────
# Helpers (add near the top of the file)
# ─────────────────────────────────────────────────────────
def normalise_asset(asset: str) -> str:
    return asset.strip().lower().replace(" ", "_")

def ensure_allowed(acct_name: str, allowed_set: set[str]) -> None:
    if acct_name not in allowed_set:
        allowed_set.add(acct_name)

def load_master_ledger(path):
    tracker = FIFOTracker()
    if os.path.exists(path):
        df_old = pd.read_csv(path, parse_dates=["date"])
        for _, r in df_old.iterrows():
            tracker.process(
                fund_id  = r["fund_id"],
                wallet   = r["wallet_address"],
                asset    = r["asset"],
                side     = r["side"],
                qty      = r["qty"],
                price_usd= r["price_eth"],
                date     = r["date"],
                tx_hash  = r["hash"],
                log      = True
            )
        return tracker, df_old
    else:
        return tracker, pd.DataFrame(columns=[
            "fund_id","wallet_address","asset","date","hash","side","qty",
            "price_eth",
            "proceeds_usd","cost_basis_sold_usd",
            "realized_gain_usd","remaining_qty","remaining_cost_basis_usd"
        ])

def save_master_ledger(path, df):
    df.to_csv(path, index=False)

def build_full_journal(
        df_running: pd.DataFrame,
        df_for_group1: pd.DataFrame,          # ← NEW: table that has from / to / contract_address
        wallet_metadata: dict
) -> pd.DataFrame:
    """
    Build the journal with correct USD calculations **and** enrich each row
    with `from`, `to`, and `contract_address` taken from `df_for_group1` (joined on `hash`).
    Duplicate hashes are NOT removed; a one-to-many merge is allowed.
    """
    # ─────────────────────────────────────────────────────────
    # 1.  PREP –  wallet groups & allowed accounts
    # ─────────────────────────────────────────────────────────
    fund_i_wallet_ids  = [m["wallet_address"].lower() for m in wallet_metadata.values() if m["group"] == "fund_i"]
    fund_ii_wallet_ids = [m["wallet_address"].lower() for m in wallet_metadata.values() if m["group"] == "fund_ii"]

    ALLOWED_ACCOUNTS = {
        "deemed_cash_usd", "digital_assets_usdc", "digital_assets_usdt",
        "digital_assets_eth", "digital_assets_weth", "digital_assets_blur_pool",
        "digital_assets_meme", "digital_assets_variabledebtethusdc",
        "digital_assets_metastreet_pool_staking_ppg_5", "digital_assets_aethweth",
        "digital_assets_aave_yield_token_weth", "digital_assets_wsteth",
        "realized_gain_loss"
    }

    # ─────────────────────────────────────────────────────────
    # 2.  MERGE – safely enrich with `from`, `to`, `contract_address`
    # ─────────────────────────────────────────────────────────
    df_group1_deduped = (
        df_for_group1
        .drop_duplicates(subset=["hash"])  # ← ensures 1 metadata row per hash
        .rename(columns={"contractAddress": "contract_address"})
    )

    merge_cols = ["hash", "from", "to", "contract_address"]

    df_enriched = df_running.merge(
        df_group1_deduped[merge_cols],
        on="hash",
        how="left",
        validate="many_to_one"  # ← enforces safety
    )

    # ─────────────────────────────────────────────────────────
    # 3.  JOURNAL BUILD
    # ─────────────────────────────────────────────────────────
    je = []
    for _, r in df_enriched.iterrows():
        try:
            # ────────────── raw fields ──────────────
            asset_raw  = str(r["asset"])
            asset      = asset_raw.upper()
            asset_key  = normalise_asset(asset_raw)
            side       = str(r["side"]).lower()
            qty        = Decimal(str(r["qty"]))
            qty_abs    = abs(qty)
            eth_usd_price = Decimal(str(r["price_eth"]))

            # Dynamic crypto account name
            acct_crypto = f"digital_assets_{asset_key}"
            ensure_allowed(acct_crypto, ALLOWED_ACCOUNTS)
            acct_cash   = "deemed_cash_usd"
            acct_gain   = "realized_gain_loss"

            is_stablecoin = asset in STABLECOINS


            #intercompany check
            from_addr = str(r.get("from", "")).lower()
            to_addr   = str(r.get("to", "")).lower()

            all_internal_wallets = set(fund_i_wallet_ids) | set(fund_ii_wallet_ids)

            intercompany = from_addr in all_internal_wallets and to_addr in all_internal_wallets

            # ────────────── common fields ──────────────
            base_fields = {
                "date": r["date"],
                "transaction_type": "cryptocurrency_trades",
                "fund_id": r.get("fund_id", ""),
                "counterparty_fund_id": "",
                "wallet_id": r.get("wallet_address", ""),
                "cryptocurrency": asset,
                "hash": r["hash"],
                "from": from_addr,
                "to": to_addr,
                "contract_address": r.get("contract_address", ""),
                "intercompany": intercompany,
                "eth_usd_price": eth_usd_price,             # convenience copy
            }

            # ────────────── BUY ──────────────
            if side == "buy":
                if asset in USE_TOTAL_ETH_NOT_QTY:
                    total_eth = Decimal(str(r["total_eth"])).quantize(SCALE_CRYPTO)
                    crypto_amt = total_eth
                    usd_value = (total_eth * eth_usd_price).quantize(SCALE_USD)
                elif is_stablecoin:
                    crypto_amt = (qty_abs / eth_usd_price).quantize(SCALE_CRYPTO)
                    usd_value = qty_abs
                else:
                    crypto_amt = qty_abs
                    usd_value = (qty_abs * eth_usd_price).quantize(SCALE_USD)

                # 1) Debit the token you just bought
                je.append({**base_fields, "account_name": acct_crypto,
                          "debit_crypto": crypto_amt,
                          "credit_crypto": Decimal("0"),
                          "debit_USD": usd_value,
                          "credit_USD": Decimal("0")})

                # 2) Credit the cash you paid (held in ETH units)
                je.append({**base_fields, "account_name": acct_cash,
                          "debit_crypto": Decimal("0"),
                          "credit_crypto": crypto_amt,
                          "debit_USD": Decimal("0"),
                          "credit_USD": usd_value})


            # ────────────── SELL ──────────────
            elif side == "sell":
                if asset in USE_TOTAL_ETH_NOT_QTY:
                    total_eth = Decimal(str(r["total_eth"])).quantize(SCALE_CRYPTO)
                    crypto_amt = total_eth
                    usd_value = (total_eth * eth_usd_price).quantize(SCALE_USD)
                elif is_stablecoin:
                    crypto_amt = (qty_abs / eth_usd_price).quantize(SCALE_CRYPTO)
                    usd_value = qty_abs
                else:
                    crypto_amt = qty_abs
                    usd_value = (qty_abs * eth_usd_price).quantize(SCALE_USD)

                proceeds   = Decimal(str(r["proceeds_usd"]))
                cost_basis = Decimal(str(r["cost_basis_sold_usd"]))
                gain       = Decimal(str(r["realized_gain_usd"]))

                # 1) Debit cash received
                je.append({**base_fields, "account_name": acct_cash,
                          "debit_crypto": crypto_amt,
                          "credit_crypto": Decimal("0"),
                          "debit_USD": proceeds,
                          "credit_USD": Decimal("0")})

                # 2) Credit the asset you sold (remove it)
                je.append({**base_fields, "account_name": acct_crypto,
                          "debit_crypto": Decimal("0"),
                          "credit_crypto": crypto_amt,
                          "debit_USD":  Decimal("0"),
                          "credit_USD": cost_basis})

                # 3) Realised gain / loss
                if gain != 0:
                    je.append({**base_fields, "account_name": acct_gain,
                              "debit_crypto":  Decimal("0"),
                              "credit_crypto": Decimal("0"),
                              "debit_USD":  -gain if gain < 0 else Decimal("0"),
                              "credit_USD":  gain  if gain > 0 else Decimal("0")})
        except (KeyError, InvalidOperation, TypeError, ValueError) as e:
          print(f"Skipping row due to error: {e}")
          continue

    # ─────────────────────────────────────────────────────────
    # 4.  POST-PROCESS  (imbalance check, final columns)
    # ─────────────────────────────────────────────────────────
    journal_df = pd.DataFrame(je)
    if not journal_df.empty:
        journal_df['debit_USD']  = journal_df['debit_USD'].apply(Decimal)
        journal_df['credit_USD'] = journal_df['credit_USD'].apply(Decimal)

        imbalance = (
            journal_df
            .groupby('hash')[["debit_USD", "credit_USD"]]
            .sum()
        )
        imbalance["net_USD"] = imbalance["debit_USD"] - imbalance["credit_USD"]

        bad = imbalance[imbalance["net_USD"].abs() > Decimal("0.000001")]
        if not bad.empty:
            print("⚠️ Imbalanced journal entries found:")
            print(bad)

    final_cols = [
        'date', 'transaction_type', 'fund_id', 'counterparty_fund_id', 'wallet_id',
        'cryptocurrency', 'account_name', 'debit_crypto', 'credit_crypto', 'eth_usd_price',
        'debit_USD', 'credit_USD', 'hash', 'from', 'to', 'contract_address', 'intercompany'
    ]
    return journal_df.reindex(columns=final_cols, fill_value=Decimal("0"))

journal_df = build_full_journal(df_ledger_usd, df_for_group1, wallet_metadata)

import os
import pandas as pd
from calendar import monthrange
from datetime import datetime
from google.colab import files

# Ensure 'date' column is datetime
journal_df["date"] = pd.to_datetime(journal_df["date"])

# Group by year and month
for (year, month), group in journal_df.groupby([journal_df["date"].dt.year,
                                                journal_df["date"].dt.month]):
    # Last calendar day of the month
    last_day = monthrange(year, month)[1]
    month_end_str = f"{year}{month:02d}{last_day:02d}"   # YYYYMMDD

    # Runtime timestamp (HHMMSS) – guarantees unique filenames
    ts = datetime.now().strftime("%H%M%S")               # e.g. 145932

    # Build directory and file path
    dir_path = (
        f"/content/drive/MyDrive/Drip_Capital/accounting_records/"
        f"{month_end_str}/general_journal_entries"
    )
    os.makedirs(dir_path, exist_ok=True)

    file_path = f"{dir_path}/{month_end_str}_crypto_trades_{ts}.csv"

    # Save and (optionally) download
    group.to_csv(file_path, index=False)
    # files.download(file_path)   # uncomment if you still want auto-download

#download last file to check

# Ensure 'date' column is datetime
journal_df["date"] = pd.to_datetime(journal_df["date"])

# Get the latest year and month
latest_date = journal_df["date"].max()
latest_year, latest_month = latest_date.year, latest_date.month

# Filter for that month
latest_month_df = journal_df[
    (journal_df["date"].dt.year == latest_year) &
    (journal_df["date"].dt.month == latest_month)
]

# Save and download
output_path = f"/content/journal_{latest_year}-{latest_month:02d}.csv"
latest_month_df.to_csv(output_path, index=False)
files.download(output_path)

"""# NFT JE CREATIONS"""

nft_usd_ledger = pd.read_csv(
    "/content/drive/MyDrive/Drip_Capital/accounting_records/investments/NFT/master_nft_ledger_USD.csv",
    parse_dates=["date"],
    date_parser=lambda x: pd.to_datetime(x, utc=True, errors="coerce")
)
nft_usd_ledger_clean = nft_usd_ledger.rename(columns={"eth_usd_price": "price_eth"}).copy()

nft_eth_ledger = pd.read_csv(
    "/content/drive/MyDrive/Drip_Capital/accounting_records/investments/NFT/master_nft_ledger_ETH.csv",
    parse_dates=["date"],
    date_parser=lambda x: pd.to_datetime(x, utc=True, errors="coerce")
)
nft_eth_ledger = nft_eth_ledger.rename(columns={"eth_usd_price": "price_eth"}).copy()

usd_hashes = set(nft_usd_ledger_clean['hash'].unique())
eth_hashes = set(nft_eth_ledger['hash'].unique())
missing_in_eth = usd_hashes - eth_hashes
print(f"{len(missing_in_eth)} hashes missing in nft_eth_ledger")
print(list(missing_in_eth)[:10])

nft_usd_ledger.head(3)

nft_eth_ledger.head(3)

start_nft = "20241101"
end_nft = "20241130"

from decimal import Decimal, InvalidOperation
import pandas as pd
import os

def normalise_asset(asset: str) -> str:
    return asset.strip().lower().replace(" ", "_")

def ensure_allowed(acct_name: str, allowed_set: set[str]) -> None:
    if acct_name not in allowed_set:
        allowed_set.add(acct_name)

def build_full_journal(df_running: pd.DataFrame, df_for_group1: pd.DataFrame, wallet_metadata: dict) -> pd.DataFrame:
    fund_i_wallet_ids = [m["wallet_address"].lower() for m in wallet_metadata.values() if m["group"] == "fund_i"]
    fund_ii_wallet_ids = [m["wallet_address"].lower() for m in wallet_metadata.values() if m["group"] == "fund_ii"]

    ALLOWED_ACCOUNTS = {"deemed_cash_usd", "realized_gain_loss"}

    df_group1_deduped = (
        df_for_group1.drop_duplicates(subset=["hash"])
        .rename(columns={"contractAddress": "contract_address"})
    )

    merge_cols = ["hash", "from", "to", "contract_address"]
    df_enriched = df_running.merge(df_group1_deduped[merge_cols], on="hash", how="left", validate="many_to_one")

    je = []
    for _, r in df_enriched.iterrows():
        try:
            asset = str(r["asset"]).upper()
            side = str(r["side"]).lower()
            qty = Decimal(str(r["qty"]))
            eth_usd_price = Decimal(str(r["price_eth"]))
            proceeds = Decimal(str(r.get("proceeds_usd", 0)))
            cost_basis = Decimal(str(r.get("cost_basis_sold_usd", 0)))
            gain_usd = Decimal(str(r.get("realized_gain_usd", 0)))
            sell_qty = Decimal(str(r.get("unit_price_eth", 0)))
            buy_qty = Decimal(str(r.get("cost_basis_sold_eth", 0)))
            crypto_gain_qty = sell_qty - buy_qty

            acct_crypto = f"digital_assets_{normalise_asset(asset)}"
            acct_cash = "deemed_cash_usd"
            acct_gain = "realized_gain_loss"
            ensure_allowed(acct_crypto, ALLOWED_ACCOUNTS)

            from_addr = str(r.get("from", "")).lower()
            to_addr = str(r.get("to", "")).lower()
            all_internal_wallets = set(fund_i_wallet_ids) | set(fund_ii_wallet_ids)
            intercompany = from_addr in all_internal_wallets and to_addr in all_internal_wallets

            base_fields = {
                "date": r["date"],
                "transaction_type": "cryptocurrency_trades",
                "fund_id": r.get("fund_id", ""),
                "counterparty_fund_id": "",
                "wallet_id": r.get("wallet_address", ""),
                "cryptocurrency": asset,
                "hash": r["hash"],
                "from": from_addr,
                "to": to_addr,
                "contract_address": r.get("contract_address", ""),
                "intercompany": intercompany,
                "eth_usd_price": eth_usd_price,
            }

            if side == "buy":
                je.append({**base_fields, "account_name": acct_crypto, "debit_crypto": qty, "credit_crypto": Decimal("0"), "debit_USD": qty * eth_usd_price, "credit_USD": Decimal("0")})
                je.append({**base_fields, "account_name": acct_cash, "debit_crypto": Decimal("0"), "credit_crypto": qty, "debit_USD": Decimal("0"), "credit_USD": qty * eth_usd_price})

            elif side == "sell":
                je.append({**base_fields, "account_name": acct_cash, "debit_crypto": sell_qty, "credit_crypto": Decimal("0"), "debit_USD": proceeds, "credit_USD": Decimal("0")})
                je.append({**base_fields, "account_name": acct_crypto, "debit_crypto": Decimal("0"), "credit_crypto": buy_qty, "debit_USD": Decimal("0"), "credit_USD": cost_basis})

                if gain_usd != 0:
                    je.append({**base_fields, "account_name": acct_gain, "debit_crypto": Decimal("0"), "credit_crypto": Decimal("0"), "debit_USD": -gain_usd if gain_usd < 0 else Decimal("0"), "credit_USD": gain_usd if gain_usd > 0 else Decimal("0")})

                if crypto_gain_qty != 0:
                    je.append({**base_fields, "account_name": acct_gain, "debit_crypto": Decimal("0"), "credit_crypto": crypto_gain_qty, "debit_USD": Decimal("0"), "credit_USD": Decimal("0")})

        except (KeyError, InvalidOperation, TypeError, ValueError) as e:
            print(f"Skipping row due to error: {e}")
            continue

    journal_df = pd.DataFrame(je)
    if not journal_df.empty:
        journal_df['debit_USD'] = journal_df['debit_USD'].apply(Decimal)
        journal_df['credit_USD'] = journal_df['credit_USD'].apply(Decimal)

    final_cols = [
        'date', 'transaction_type', 'fund_id', 'counterparty_fund_id', 'wallet_id',
        'cryptocurrency', 'account_name', 'debit_crypto', 'credit_crypto', 'eth_usd_price',
        'debit_USD', 'credit_USD', 'hash', 'from', 'to', 'contract_address', 'intercompany'
    ]
    return journal_df.reindex(columns=final_cols, fill_value=Decimal("0"))

nft_je_df = build_full_journal(nft_usd_ledger_clean, nft_eth_ledger, wallet_metadata, start_nft, end_nft)

from google.colab import files
nft_je_df.to_csv("fuwah.csv", index=False)
files.download("fuwah.csv")

"""# NFT FIFO GENERATOR - Not in use"""

from decimal import Decimal, InvalidOperation
import pandas as pd
import os
df_running = pd.read_csv("/content/drive/MyDrive/Drip_Capital/accounting_records/investments/NFT/master_nft_ledger_USD.csv")
# ─────────────────────────────────────────────────────────
# Helpers (add near the top of the file)
# ─────────────────────────────────────────────────────────
def normalise_asset(asset: str) -> str:
    return asset.strip().lower().replace(" ", "_")

def ensure_allowed(acct_name: str, allowed_set: set[str]) -> None:
    if acct_name not in allowed_set:
        allowed_set.add(acct_name)

def load_master_ledger(path):
    tracker = FIFOTracker()
    if os.path.exists(path):
        df_old = pd.read_csv(path, parse_dates=["date"])
        for _, r in df_old.iterrows():
            tracker.process(
                fund_id  = r["fund_id"],
                wallet   = r["wallet_address"],
                asset    = r["asset"],
                side     = r["side"],
                qty      = r["qty"],
                price_usd= r["price_eth"],
                date     = r["date"],
                tx_hash  = r["hash"],
                log      = True
            )
        return tracker, df_old
    else:
        return tracker, pd.DataFrame(columns=[
            "fund_id","wallet_address","asset","date","hash","side","qty",
            "price_eth",
            "proceeds_usd","cost_basis_sold_usd",
            "realized_gain_usd","remaining_qty","remaining_cost_basis_usd"
        ])

def save_master_ledger(path, df):
    df.to_csv(path, index=False)

def build_full_journal(
        df_running: pd.DataFrame,
        df_for_group1: pd.DataFrame,          # ← NEW: table that has from / to / contract_address
        wallet_metadata: dict
) -> pd.DataFrame:
    """
    Build the journal with correct USD calculations **and** enrich each row
    with `from`, `to`, and `contract_address` taken from `df_for_group1` (joined on `hash`).
    Duplicate hashes are NOT removed; a one-to-many merge is allowed.
    """
    # ─────────────────────────────────────────────────────────
    # 1.  PREP –  wallet groups & allowed accounts
    # ─────────────────────────────────────────────────────────
    fund_i_wallet_ids  = [m["wallet_address"].lower() for m in wallet_metadata.values() if m["group"] == "fund_i"]
    fund_ii_wallet_ids = [m["wallet_address"].lower() for m in wallet_metadata.values() if m["group"] == "fund_ii"]

    ALLOWED_ACCOUNTS = {
        "deemed_cash_usd", "digital_assets_usdc", "digital_assets_usdt",
        "digital_assets_eth", "digital_assets_weth", "digital_assets_blur_pool",
        "digital_assets_meme", "digital_assets_variabledebtethusdc",
        "digital_assets_metastreet_pool_staking_ppg_5", "digital_assets_aethweth",
        "digital_assets_aave_yield_token_weth", "digital_assets_wsteth",
        "realized_gain_loss"
    }

    # ─────────────────────────────────────────────────────────
    # 2.  MERGE – safely enrich with `from`, `to`, `contract_address`
    # ─────────────────────────────────────────────────────────
    df_group1_deduped = (
        df_for_group1
        .drop_duplicates(subset=["hash"])  # ← ensures 1 metadata row per hash
        .rename(columns={"contractAddress": "contract_address"})
    )

    merge_cols = ["hash", "from", "to", "contract_address"]

    df_enriched = df_running.merge(
        df_group1_deduped[merge_cols],
        on="hash",
        how="left",
        validate="many_to_one"  # ← enforces safety
    )

    # ─────────────────────────────────────────────────────────
    # 3.  JOURNAL BUILD
    # ─────────────────────────────────────────────────────────
    je = []
    for _, r in df_enriched.iterrows():
        try:
            # ────────────── raw fields ──────────────
            asset_raw  = str(r["asset"])
            asset      = asset_raw.upper()
            asset_key  = normalise_asset(asset_raw)
            side       = str(r["side"]).lower()
            qty        = Decimal(str(r["qty"]))
            qty_abs    = abs(qty)
            eth_usd_price = Decimal(str(r["price_eth"]))

            # Dynamic crypto account name
            acct_crypto = f"digital_assets_{asset_key}"
            ensure_allowed(acct_crypto, ALLOWED_ACCOUNTS)
            acct_cash   = "deemed_cash_usd"
            acct_gain   = "realized_gain_loss"

            is_stablecoin = asset in STABLECOINS


            #intercompany check
            from_addr = str(r.get("from", "")).lower()
            to_addr   = str(r.get("to", "")).lower()

            all_internal_wallets = set(fund_i_wallet_ids) | set(fund_ii_wallet_ids)

            intercompany = from_addr in all_internal_wallets and to_addr in all_internal_wallets

            # ────────────── common fields ──────────────
            base_fields = {
                "date": r["date"],
                "transaction_type": "cryptocurrency_trades",
                "fund_id": r.get("fund_id", ""),
                "counterparty_fund_id": "",
                "wallet_id": r.get("wallet_address", ""),
                "cryptocurrency": asset,
                "hash": r["hash"],
                "from": from_addr,
                "to": to_addr,
                "contract_address": r.get("contract_address", ""),
                "intercompany": intercompany,
                "eth_usd_price": eth_usd_price,             # convenience copy
            }

            # ────────────── BUY ──────────────
            if side == "buy":
                # How many “ETH-units” does this cost?
                crypto_amt = (qty_abs / eth_usd_price).quantize(SCALE_CRYPTO) if is_stablecoin else qty_abs
                usd_value  = qty_abs if is_stablecoin else (qty_abs * eth_usd_price).quantize(SCALE_USD)
                # 1) Debit the token you just bought
                je.append({**base_fields, "account_name": acct_crypto,
                          "debit_crypto": crypto_amt,
                          "credit_crypto": Decimal("0"),
                          "debit_USD": usd_value,
                          "credit_USD": Decimal("0")})

                # 2) Credit the cash you paid (held in ETH units)
                je.append({**base_fields, "account_name": acct_cash,
                          "debit_crypto": Decimal("0"),
                          "credit_crypto": crypto_amt,
                          "debit_USD": Decimal("0"),
                          "credit_USD": usd_value})


            # ────────────── SELL ──────────────
            elif side == "sell":
                crypto_amt = (qty_abs / eth_usd_price).quantize(SCALE_CRYPTO) if is_stablecoin else qty_abs
                usd_value  = qty_abs if is_stablecoin else (qty_abs * eth_usd_price).quantize(SCALE_USD)  # == proceeds
                proceeds   = Decimal(str(r["proceeds_usd"]))
                cost_basis = Decimal(str(r["cost_basis_sold_usd"]))
                gain       = Decimal(str(r["realized_gain_usd"]))

                # 1) Debit cash received
                je.append({**base_fields, "account_name": acct_cash,
                          "debit_crypto": crypto_amt,
                          "credit_crypto": Decimal("0"),
                          "debit_USD": proceeds,
                          "credit_USD": Decimal("0")})

                # 2) Credit the asset you sold (remove it)
                je.append({**base_fields, "account_name": acct_crypto,
                          "debit_crypto": Decimal("0"),
                          "credit_crypto": crypto_amt,
                          "debit_USD":  Decimal("0"),
                          "credit_USD": cost_basis})

                # 3) Realised gain / loss
                if gain != 0:
                    je.append({**base_fields, "account_name": acct_gain,
                              "debit_crypto":  Decimal("0"),
                              "credit_crypto": Decimal("0"),
                              "debit_USD":  -gain if gain < 0 else Decimal("0"),
                              "credit_USD":  gain  if gain > 0 else Decimal("0")})
        except (KeyError, InvalidOperation, TypeError, ValueError) as e:
          print(f"Skipping row due to error: {e}")
          continue

    # ─────────────────────────────────────────────────────────
    # 4.  POST-PROCESS  (imbalance check, final columns)
    # ─────────────────────────────────────────────────────────
    journal_df = pd.DataFrame(je)
    if not journal_df.empty:
        journal_df['debit_USD']  = journal_df['debit_USD'].apply(Decimal)
        journal_df['credit_USD'] = journal_df['credit_USD'].apply(Decimal)

        imbalance = (
            journal_df
            .groupby('hash')[["debit_USD", "credit_USD"]]
            .sum()
        )
        imbalance["net_USD"] = imbalance["debit_USD"] - imbalance["credit_USD"]

        bad = imbalance[imbalance["net_USD"].abs() > Decimal("0.000001")]
        if not bad.empty:
            print("⚠️ Imbalanced journal entries found:")
            print(bad)

    final_cols = [
        'date', 'transaction_type', 'fund_id', 'counterparty_fund_id', 'wallet_id',
        'cryptocurrency', 'account_name', 'debit_crypto', 'credit_crypto', 'eth_usd_price',
        'debit_USD', 'credit_USD', 'hash', 'from', 'to', 'contract_address', 'intercompany'
    ]
    return journal_df.reindex(columns=final_cols, fill_value=Decimal("0"))

nft_jes = build_full_journal(df_running, df_for_group1, wallet_metadata)

nft_jes.to_csv("fuwah.csv")
files.download("fuwah.csv")

"""# Finalize FIFO and Create JEs - Old

"""

def load_master_ledger(path):
    tracker = FIFOTracker()
    if os.path.exists(path):
        df_old = pd.read_csv(path, parse_dates=["date"])
        # replay old rows to rebuild lots, but don't log again
        for _, r in df_old.iterrows():
            tracker.process(
                fund_id  = r["fund_id"],
                wallet   = r["wallet_address"],
                asset    = r["asset"],
                side     = r["side"],
                qty      = r["qty"],
                price_usd= r["eth_usd_price"], #eth_usd_price
                date     = r["date"],
                tx_hash  = r["hash"],
                log      = True     # ← replay only
            )
        return tracker, df_old
    else:
        return tracker, pd.DataFrame(columns=[
            "fund_id","wallet_address","asset","date","hash","side","qty",
            "eth_usd_price",
            "proceeds_usd","cost_basis_sold_usd",
            "realized_gain_usd","remaining_qty","remaining_cost_basis_usd"
        ])

def save_master_ledger(path, df):
    df.to_csv(path, index=False)

def build_full_journal(df_running: pd.DataFrame) -> pd.DataFrame:
    from decimal import Decimal, InvalidOperation
    import pandas as pd

    je = []
    for _, r in df_running.iterrows():
        try:
            common = {
                'date':                  r['date'],
                'hash':                  r['hash'],
                'fund_id':               r.get('fund_id', ''),
                'counterparty_fund_id':  '',  # placeholder
                'Transaction Type':      'Cryptocurrency purchase or sell',
                'cryptocurrency':        r['asset'],
                'units':                 r['qty'],
                'wallet_address':        r.get('wallet_address', '')
            }

            qty = Decimal(str(r['qty']))
            tx_price_usd = Decimal(str(r['eth_usd_price']))
            #tx_price_usd = Decimal(str(r['tx_price_usd']))
            proceeds = Decimal(str(r['proceeds_usd']))
            cost_basis = Decimal(str(r['cost_basis_sold_usd']))
            gain = Decimal(str(r['realized_gain_usd']))

            if r['side'] == 'buy':
                value = qty * tx_price_usd
                je.append({
                    **common,
                    'account':   f"Cryptocurrency - {r['asset']}",
                    'debit_USD':  value,
                    'credit_USD': Decimal("0"),
                })
                je.append({
                    **common,
                    'account':   "Deemed cash - USD",
                    'debit_USD':  Decimal("0"),
                    'credit_USD': value,
                })

            elif r['side'] == 'sell':
                je.append({
                    **common,
                    'account':   "Deemed cash - USD",
                    'debit_USD':  proceeds,
                    'credit_USD': Decimal("0"),
                })
                je.append({
                    **common,
                    'account':   f"Cryptocurrency - {r['asset']}",
                    'debit_USD':  Decimal("0"),
                    'credit_USD': cost_basis,
                })
                je.append({
                    **common,
                    'account':   "Realized gain/loss - Cryptocurrency",
                    'debit_USD':  -gain if gain < 0 else Decimal("0"),
                    'credit_USD': gain if gain > 0 else Decimal("0"),
                })
        except (KeyError, InvalidOperation, TypeError, ValueError) as e:
            print(f"Skipping row due to error: {e}")
            continue

    journal_df = pd.DataFrame(je)

    if not journal_df.empty:
        journal_df['debit_USD'] = journal_df['debit_USD'].apply(Decimal)
        journal_df['credit_USD'] = journal_df['credit_USD'].apply(Decimal)
        journal_df['net_debit_credit'] = journal_df['debit_USD'] - journal_df['credit_USD']

        balance_check = journal_df.groupby(['hash'])['net_debit_credit'].sum().reset_index()
        imbalances = balance_check[balance_check['net_debit_credit'].abs() > Decimal('0.000001')]

        if not imbalances.empty:
            print("⚠️ Imbalanced journal entries found for the following transaction hashes:")
            print(imbalances)

        cols = [
            'date', 'wallet_address', 'fund_id', 'counterparty_fund_id', 'cryptocurrency',
            'account', 'debit_USD', 'credit_USD', 'Transaction Type', 'units', 'hash',
        ]
        journal_df = journal_df[cols]

    else:
        journal_df = pd.DataFrame(columns=[
            'date', 'wallet_address', 'fund_id', 'counterparty_fund_id', 'cryptocurrency',
            'account', 'debit_USD', 'credit_USD', 'Transaction Type', 'units', 'hash'
        ])

    return journal_df



import os
import pandas as pd
from collections import deque
from decimal import Decimal, InvalidOperation

# ─── FIFOTracker with replay support ────────────────────────────────
class FIFOTracker:
    def __init__(self):
        # key → deque of [qty, price]
        self.lots = {}
        self.logs = []

    def process(self, fund_id, wallet, asset, side, qty, price_usd, date, tx_hash, log=False):
        """ Process one leg. If log=False, rebuild lots w/o recording. """
        # normalize
        try:
            q = Decimal(str(qty))
            p = Decimal(str(price_usd))
        except (InvalidOperation, TypeError):
            return

        key = (fund_id, wallet, asset)
        dq = self.lots.setdefault(key, deque())

        proceeds = cost_basis = gain = Decimal("0")
        if side == "buy":
            dq.append([q, p])
        else:  # sell
            to_sell = abs(q)
            while to_sell > 0 and dq:
                lq, lp = dq[0]
                take = min(lq, to_sell)
                proceeds   += take * p
                cost_basis += take * lp
                gain       += (take * p - take * lp)
                lq -= take; to_sell -= take
                if lq > 0:
                    dq[0][0] = lq
                else:
                    dq.popleft()
            if to_sell > 0:
                # short
                proceeds   += to_sell * p
                cost_basis += to_sell * p
                dq.appendleft([-to_sell, p])

        remaining_qty  = sum(l for l,_ in dq)
        remaining_cost = sum(l * pr for l,pr in dq)

        if log:
            self.logs.append({
                "fund_id":                  fund_id,
                "wallet_address":           wallet,
                "asset":                    asset,
                "date":                     date,
                "hash":                     tx_hash,
                "side":                     side,
                "qty":                      q,
                "eth_usd_price":           p,
                "proceeds_usd":             proceeds,
                "cost_basis_sold_usd":      cost_basis,
                "realized_gain_usd":        gain,
                "remaining_qty":            remaining_qty,
                "remaining_cost_basis_usd": remaining_cost
            })

    def to_dataframe(self):
        return pd.DataFrame(self.logs)

"""## Config"""

# ─── CONFIG ─────────────────────────────────────────────────
MASTER_PATH = f"/content/drive/MyDrive/Drip_Capital/accounting_records/investments/cryptocurrency/Test/master_fifo_ledger_test6.csv"

fifo_input.columns

"""## Recalculate **master_fifo**"""

import os
import pandas as pd

STABLECOINS = {"USDC", "USDT", "DAI", "VARIABLEDEBTETHUSDC","VARIABLEDEBTETHUSDT"}

ETH_one_for_one = {"ETH", "BLUR POOL", "BLUR","WETH", "MWETH-PPG:5", "AETHWETH"}

WSTETH = {"WSTETH"}
WSTETH_RATE = Decimal("1.18045433553113")

MEME_COIN = {"MEME"}
MEME_RATE = Decimal("0.000001")

def compute_unit_price_usd(row):
    try:
        if row["asset"].upper() in STABLECOINS:
            return 1
        if row["asset"].upper() in WSTETH:
            return Decimal(row["eth_usd_price"]) * Decimal(row["unit_price_eth"])
        else:
            return Decimal(row["eth_usd_price"])
    except (InvalidOperation, TypeError):
        return Decimal("0")

def compute_unit_price_eth(row):
    try:
        if row["asset"].upper() in STABLECOINS and Decimal(row["qty"]) != 0:
            return Decimal(row["qty"]) / Decimal(row["eth_usd_price"])
        if row["asset"].upper() in ETH_one_for_one and Decimal(row["qty"]) != 0:
          return Decimal(row["eth_usd_price"])
        if row["asset"].upper() in WSTETH and Decimal(row["qty"]) != 0:
          return WSTETH_RATE * Decimal(row["qty"])
        if row["asset"].upper() in MEME_COIN and Decimal(row["qty"]) != 0:
          return MEME_RATE * Decimal(row["eth_usd_price"])
    except (InvalidOperation, ZeroDivisionError, TypeError):
        return Decimal("0")
# Columns to track in the ledger
cols_to_keep = [
    "fund_id", "wallet_address", "asset", "date", "hash",
    "side", "qty", "eth_usd_price"
]

# 1. Create empty file with headers if it doesn't exist
if not os.path.exists(MASTER_PATH):
    print(f"🚨 No existing master ledger found at {MASTER_PATH}. Creating a fresh one.")
    df_empty = pd.DataFrame(columns=cols_to_keep)
    df_empty.to_csv(MASTER_PATH, index=False)

# 2. Normalize fifo_input to required format and sort by date
df_base = fifo_input[cols_to_keep].copy()
df_base["date"] = pd.to_datetime(df_base["date"])
df_base = df_base.sort_values("date")

df_base["unit_price_eth"] = df_base.apply(compute_unit_price_eth, axis=1)
df_base["unit_price_usd"] = df_base.apply(compute_unit_price_usd, axis=1)


# 3. Rebuild the tracker from fifo_input rows
tracker = FIFOTracker()
for r in df_base.itertuples(index=False):
    tracker.process(
        fund_id   = r.fund_id,
        wallet    = r.wallet_address,
        asset     = r.asset,
        side      = r.side,
        price_usd = r.unit_price_usd,
        qty       = r.qty,
        date      = r.date,
        tx_hash   = r.hash,
        log       = True
    )
df_master_rebuilt = tracker.to_dataframe()

df_master_rebuilt["unit_price_eth"] = df_master_rebuilt.apply(compute_unit_price_eth, axis=1)
df_master_rebuilt["unit_price_usd"] = df_master_rebuilt.apply(compute_unit_price_usd, axis=1)

# 4. Extract and save full ledger
desired_order = [
    "fund_id", "wallet_address", "asset", "date", "hash", "side", "eth_usd_price", "qty",
    "unit_price_eth", "unit_price_usd", "proceeds_usd", "cost_basis_sold_usd",
    "realized_gain_usd", "remaining_qty", "remaining_cost_basis_usd"
]

# Only include columns that actually exist
final_columns = [col for col in desired_order if col in df_master_rebuilt.columns]

# Add any other columns that may have been added
other_columns = [col for col in df_master_rebuilt.columns if col not in final_columns]

# Reorder
df_master_rebuilt = df_master_rebuilt[final_columns + other_columns]

#Drop rows where qty is zero
df_master_rebuilt = df_master_rebuilt[df_master_rebuilt["qty"] != 0]

save_master_ledger(MASTER_PATH, df_master_rebuilt)

print(f"✅ Rebuilt master ledger from fifo_input with {len(df_master_rebuilt)} recalculated rows.")

from google.colab import files
files.download(MASTER_PATH)

"""## Use correct master_fifo and then append"""

# 1. Load current master ledger (with accurate derived values)
tracker, df_existing = load_master_ledger(MASTER_PATH)

# 2. Get existing keys to prevent duplicates
existing_keys = set(
    (r.hash, r.wallet_address, r.asset, r.side)
    for r in df_existing.itertuples(index=False)
)

# 3. Process only new transaction legs
for r in fifo_input.itertuples(index=False):
    key = (r.hash, r.wallet_address, r.asset, r.side)
    if key in existing_keys:
        continue
    tracker.process(
        fund_id   = r.fund_id,
        wallet    = r.wallet_address,
        asset     = r.asset,
        side      = r.side,
        qty       = r.qty,
        price_usd = r.eth_usd_price,
        date      = r.date,
        tx_hash   = r.hash,
        log       = True
    )

# 4. Extract only the new rows just logged
df_new_fifo = tracker.to_dataframe()
df_new_fifo = df_new_fifo[~df_new_fifo['hash'].isin(df_existing['hash'])]

# 5. Append to master and save
if not df_new_fifo.empty:
    df_master = pd.concat([df_existing, df_new_fifo], ignore_index=True)
    save_master_ledger(MASTER_PATH, df_master)
    print(f"✅ Appended {len(df_new_fifo)} new rows to master ledger.")
else:
    print("No new FIFO rows to append.")

MASTER_PATH_LOCAL = "master_fifo_ledger_download.csv"
df_master.to_csv(MASTER_PATH_LOCAL, index=False)